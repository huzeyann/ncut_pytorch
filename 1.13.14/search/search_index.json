{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Install & Quick Start","text":"GitHub         \ud83e\udd17 Demo         \ud83d\udcdd Paper (Coming)    <pre><code>pip install ncut-pytorch</code></pre>"},{"location":"#ncut-nystrom-normalized-cut","title":"NCUT: Nystr\u00f6m Normalized Cut","text":"<p>Normalized Cut, aka. spectral clustering, is a graphical method to analyze data grouping in the affinity eigenvector space. It has been widely used for unsupervised segmentation in the 2000s.</p> <p>Nystr\u00f6m Normalized Cut, is a new approximation algorithm developed for large-scale graph cuts,  a large-graph of million nodes can be processed in under 10s (cpu) or 2s (gpu).  </p> <p>Video: NCUT applied to image encoder features from Segment Anything Model.  RGB color is 3D spectral-tSNE embedding of NCUT eigenvectors. code </p> <p>NCUT (DiNO features as input) produce segmentation at various granularity.  NCUT segments coloring is aligned across images, SAM color is arbitrary. </p> <p> PROCEDURE How NCUT Works</p> <p> 1. Feature Extraction: extract feature from pre-trained model. 2. NCUT: compute 100 NCUT eigenvectors, input feature is from deep models.  </p> <p>Demo Application: Point-Prompting Segmentation tool for pseudo-labeling across multiple images. Try it in \ud83e\udd17HuggingFace (Switch to the Tab \"Application\"). More Examples in Gallery.</p> <p>Video: Heatmap is cosine distance of eigenvectors, w.r.t the mouse pointer. details </p> <p></p>"},{"location":"#gallery","title":"Gallery","text":"<p>Just plugin features extracted from any pre-trained model and ready to go. NCUT works for any input -- image, text, video, 3D, .... Planty examples code and plots in the Gallery</p>"},{"location":"#installation","title":"Installation","text":""},{"location":"#1-install-pytorch","title":"1. Install PyTorch","text":"<pre><code>conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n</code></pre>"},{"location":"#2-install-ncut-pytorch","title":"2. Install <code>ncut-pytorch</code>","text":"<pre><code>pip install ncut-pytorch</code></pre>"},{"location":"#installation-trouble-shooting","title":"Installation Trouble Shooting","text":"<p>In case of <code>pip</code> install error, please try install the build dependencies.</p> <p>Option A:</p> <pre><code>sudo apt-get update &amp;&amp; sudo apt-get install build-essential cargo rustc -y</code></pre> <p>Option B:</p> <pre><code>conda install rust -c conda-forge</code></pre> <p>Option C:</p> <pre><code>curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh &amp;&amp; . \"$HOME/.cargo/env\"</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Minimal example on how to run NCUT, more examples in Tutorial and Gallery.</p> <pre><code>\nimport torch\nfrom ncut_pytorch import NCUT, rgb_from_tsne_3d\n\nmodel_features = torch.rand(20, 64, 64, 768)  # (B, H, W, C)\n\ninp = model_features.reshape(-1, 768)  # flatten\neigvectors, eigvalues = NCUT(num_eig=100, device='cuda:0').fit_transform(inp)\ntsne_x3d, tsne_rgb = rgb_from_tsne_3d(eigvectors, device='cuda:0')\n\neigvectors = eigvectors.reshape(20, 64, 64, 100)  # (B, H, W, num_eig)\ntsne_rgb = tsne_rgb.reshape(20, 64, 64, 3)  # (B, H, W, 3)\n    </code></pre>"},{"location":"#load-feature-extractor-model","title":"Load Feature Extractor Model","text":"<p>Any backbone model works as plug-in feature extractor.  We have implemented some backbone models, here is a list of available models:</p> <pre><code> \nfrom ncut_pytorch.backbone import list_models \nprint(list_models()) \n[\n  'SAM2(sam2_hiera_t)', 'SAM2(sam2_hiera_s)', 'SAM2(sam2_hiera_b+)', 'SAM2(sam2_hiera_l)', \n  'SAM(sam_vit_b)', 'SAM(sam_vit_l)', 'SAM(sam_vit_h)', 'MobileSAM(TinyViT)', \n  'DiNOv2reg(dinov2_vits14_reg)', 'DiNOv2reg(dinov2_vitb14_reg)', 'DiNOv2reg(dinov2_vitl14_reg)', 'DiNOv2reg(dinov2_vitg14_reg)', \n  'DiNOv2(dinov2_vits14)', 'DiNOv2(dinov2_vitb14)', 'DiNOv2(dinov2_vitl14)', 'DiNOv2(dinov2_vitg14)', \n  'DiNO(dino_vits8_896)', 'DiNO(dino_vitb8_896)', 'DiNO(dino_vits8_672)', 'DiNO(dino_vitb8_672)', 'DiNO(dino_vits8_448)', 'DiNO(dino_vitb8_448)', 'DiNO(dino_vits16_448)', 'DiNO(dino_vitb16_448)',\n  'Diffusion(stabilityai/stable-diffusion-2)', 'Diffusion(CompVis/stable-diffusion-v1-4)', 'Diffusion(stabilityai/stable-diffusion-3-medium-diffusers)',\n  'CLIP(ViT-B-16/openai)', 'CLIP(ViT-L-14/openai)', 'CLIP(ViT-H-14/openai)', 'CLIP(ViT-B-16/laion2b_s34b_b88k)', \n  'CLIP(convnext_base_w_320/laion_aesthetic_s13b_b82k)', 'CLIP(convnext_large_d_320/laion2b_s29b_b131k_ft_soup)', 'CLIP(convnext_xxlarge/laion2b_s34b_b82k_augreg_soup)', \n  'CLIP(eva02_base_patch14_448/mim_in22k_ft_in1k)', \"CLIP(eva02_large_patch14_448/mim_m38m_ft_in22k_in1k)\",\n  'MAE(vit_base)', 'MAE(vit_large)', 'MAE(vit_huge)', \n  'ImageNet(vit_base)'\n]\nfrom ncut_pytorch.backbone_text import list_models \nprint(list_models()) \n[\"meta-llama/Meta-Llama-3.1-8B\", \"meta-llama/Meta-Llama-3-8B\", \"gpt2\"]\n </code></pre>"},{"location":"#image-model-example","title":"Image model example:","text":"<pre><code> \nimport torch from ncut_pytorch import NCUT, rgb_from_tsne_3d \nfrom ncut_pytorch.backbone import load_model, extract_features\n\nmodel = load_model(model_name=\"SAM(sam_vit_b)\") \nimages = torch.rand(20, 3, 1024, 1024) \nmodel_features = extract_features(images, model, node_type='attn', layer=6) \n# model_features = model(images)['attn'][6]  # this also works\n\ninp = model_features.reshape(-1, 768) # flatten\neigvectors, eigvalues = NCUT(num_eig=100, device='cuda:0').fit_transform(inp) \ntsne_x3d, tsne_rgb = rgb_from_tsne_3d(eigvectors, device='cuda:0')\n\neigvectors = eigvectors.reshape(20, 64, 64, 100) # (B, H, W, num_eig) \ntsne_rgb = tsne_rgb.reshape(20, 64, 64, 3) # (B, H, W, 3) </code></pre>"},{"location":"#text-model-example","title":"Text model example:","text":"This example use your access token and download Llama from HuggingFace. How to set up Llama access token from HuggingFace (click to expand):   <p>Step 1: Request access for Llama from https://huggingface.co/meta-llama/Meta-Llama-3.1-8B <p>Step 2: Find your access token at https://huggingface.co/settings/tokens </p> <pre><code>\nimport os\nfrom ncut_pytorch import NCUT, rgb_from_tsne_3d\nfrom ncut_pytorch.backbone_text import load_text_model\n\nos.environ['HF_ACCESS_TOKEN'] = \"your_huggingface_token\"\nllama = load_text_model(\"meta-llama/Meta-Llama-3.1-8B\").cuda()\noutput_dict = llama(\"The quick white fox jumps over the lazy cat.\")\n\nmodel_features = output_dict['block'][31].squeeze(0)  # 32nd block output\ntoken_texts = output_dict['token_texts']\neigvectors, eigvalues = NCUT(num_eig=5, device='cuda:0').fit_transform(model_features)\ntsne_x3d, tsne_rgb = rgb_from_tsne_3d(eigvectors, device='cuda:0')\n# eigvectors.shape[0] == tsne_rgb.shape[0] == len(token_texts)\n    </code></pre>"},{"location":"#why-ncut","title":"Why NCUT","text":"<p>Normalized cut offers two advantages:</p> <ol> <li> <p>soft-cluster assignments as eigenvectors</p> </li> <li> <p>hierarchical clustering by varying the number of eigenvectors</p> </li> </ol> <p>Video: Heatmap is cosine distance of eigenvectors, w.r.t the mouse pixel (blue point). Reduce `n_eig` hierarchical grow the object heatmap try it at \ud83e\udd17HuggingFace Demo (switch to tab \"PlayGround\")  <p>Please see NCUT and t-SNE/UMAP for a comparison over common PCA, t-SNE, UMAP.</p> <p>paper in prep, Yang 2024</p> <p>AlignedCut: Visual Concepts Discovery on Brain-Guided Universal Feature Space, Huzheng Yang, James Gee*, Jianbo Shi*,2024</p> <p>Normalized Cuts and Image Segmentation, Jianbo Shi and Jitendra Malik, 2000</p>"},{"location":"add_nodes/","title":"Tutorial: Adding New Nodes to Existing Graph","text":"<p>Once NCUT is computed, it is possible to add nodes to the existing graph. New nodes are assigned with eigenvectors/color by KNN Propagation, in other words, the new nodes are treated as they were not sampled in the nystrom approximation, they only join the propagation step. This is a good approach when the original sampled nodes have good cover of the newly added nodes.</p>"},{"location":"add_nodes/#feature-extraction","title":"Feature Extraction","text":"Click to expand full code  <pre><code>dataset = torchvision.datasets.VOCSegmentation(...)\nfeat = feature_extractor(images, layer=9)\n# Feature shape for 100 images: torch.Size([100, 32, 32, 768])\n</code></pre> <pre><code>import torchvision\nimport torch\nfrom torchvision import transforms\nfrom PIL import Image\nimport numpy as np\n\ndataset_voc = torchvision.datasets.VOCSegmentation(\n    \"/data/pascal_voc/\",\n    year=\"2012\",\n    download=True,\n    image_set=\"val\",\n)\nprint(\"number of images in the dataset:\", len(dataset_voc))\n\n# create a large-scale feature matrix\nimages = [dataset_voc[i][0] for i in range(100)]\nfeats = feature_extractor(images, resolution=(336, 336), layer=9)\nprint(\"Feature shape for 100 images:\", feats.shape)\nnum_nodes = np.prod(feats.shape[:3])\nprint(\"Number of nodes for 100 images:\", num_nodes)\n\n# Feature shape for 100 images: torch.Size([100, 32, 32, 768])\n# Number of nodes for 100 images: 102400\n</code></pre>"},{"location":"add_nodes/#ncut-on-original-images","title":"NCUT on Original Images","text":"<pre><code>from ncut_pytorch import NCUT\n\ninput_feats = feats.flatten(0, 2)\neigenvectors, eigenvalues = NCUT(\n    num_eig=20, num_sample=30000, knn=10, affinity_focal_gamma=0.5, device='cpu'\n).fit_transform(input_feats)\n</code></pre>  Click to expand full code <pre><code>def plot_images(images, rgb, title):\n    ...\n</code></pre> <pre><code>import matplotlib.pyplot as plt\n\ndef plot_images(images, rgb, title):\n    fig, axs = plt.subplots(4, 8, figsize=(10, 4))\n    for i_row in range(0, 4, 2):\n        for i_col in range(8):\n            ax = axs[i_row, i_col]\n            image = images[i_row * 4 + i_col]\n            image = image.resize((224, 224), Image.BILINEAR)\n            ax.imshow(image)\n            ax.axis(\"off\")\n        for i_col in range(8):\n            ax = axs[i_row + 1, i_col]\n            ax.imshow(rgb[i_row * 4 + i_col])\n            ax.axis(\"off\")\n    plt.suptitle(title, fontsize=16)\n    plt.show()\n</code></pre> <pre><code># apply t-SNE for visualization of the eigenvectors\nfrom ncut_pytorch import rgb_from_tsne_3d\n\nX_3d, rgb = rgb_from_tsne_3d(\n    eigenvectors[:, :20], num_sample=30000, perplexity=100, device=\"cuda:0\", knn=10\n)\n\nimage_rgb = rgb.reshape(feats.shape[:3] + (3,))\nplot_images(images, image_rgb, \"NCUT, original images, spectral-tSNE 20 eigenvectors\")\n</code></pre>"},{"location":"add_nodes/#feature-extraction-for-new-images","title":"Feature Extraction for New Images","text":"<pre><code>new_images = [dataset_voc[i][0] for i in range(1000, 1100)]\nnew_feats = feature_extractor(new_images, resolution=(336, 336), layer=9)\nprint(\"Feature shape for new images:\", new_feats.shape)\n</code></pre>"},{"location":"add_nodes/#propagate-eigenvectors-to-new-images","title":"Propagate Eigenvectors to New Images","text":"<pre><code>from ncut_pytorch import propagate_eigenvectors, propagate_rgb_color\n\nnew_eigenvectors = propagate_eigenvectors(eigenvectors[:, :20], feats.reshape(-1, feats.shape[-1]), new_feats.reshape(-1, new_feats.shape[-1]), knn=10)\nnew_rgb = propagate_rgb_color(rgb, eigenvectors[:, :20], new_eigenvectors[:, :20], knn=10)\n\nplot_images(\n    new_images,\n    new_rgb.reshape(new_feats.shape[:3] + (3,)).cpu(),\n    \"NCUT, added images, propagated, spectral-tSNE 20 eigenvectors\",\n)\n</code></pre>"},{"location":"add_nodes/#comparison-recompute-eigenvectors-vs-propagate-eigenvectors","title":"Comparison: Recompute Eigenvectors vs. Propagate Eigenvectors","text":"<p>Recomputing the eigenvectors results in better segmentation, but the color is not consistent with previous images.</p> <pre><code>recomputed_eigenvectors, _ = NCUT(\n    num_eig=50, num_sample=30000, knn=10, affinity_focal_gamma=0.3, device=\"cuda:0\"\n).fit_transform(new_feats.reshape(-1, new_feats.shape[-1]))\n\nrecomputed_rgb = rgb_from_tsne_3d(\n    recomputed_eigenvectors[:, :20], num_sample=30000, perplexity=100, device=\"cuda:0\", knn=10\n)[1].reshape(new_feats.shape[:3] + (3,))\n\nplot_images(\n    new_images,\n    recomputed_rgb,\n    \"NCUT, added images, recomputed, spectral-tSNE 20 eigenvectors\",\n)\n</code></pre> The complete code for this tutorial <p>huzeyann/ncut_pytorch</p> <p></p>"},{"location":"alignedcut_vs_ncut/","title":"Large-Scale Aligned NCUT across Images","text":"<ul> <li>NCut processes each image independently. There's many small affinity matrix, each affinity matrix only contain pixels from one image.</li> </ul> <ul> <li>AlignedCut process all images as one large-scale graph cut. There's only one big affinity matrix that connects all images.</li> </ul> <p>AlignedCut can discover better pattern that span multiple images. However, the size and complexity of affinity matrix is order of magnitude higher. The new developed Nystrom approximation solved the scalability and speed bottle-neck (see How NCUT Works).</p>"},{"location":"alignedcut_vs_ncut/#pattern-discovered-by-alignedcut","title":"Pattern Discovered by AlignedCut","text":"<p>AlignedCut: Color is aligned across images. Try on HuggingFace </p> <p>NCut: Color is not aligned across images. Try on HuggingFace </p>"},{"location":"alignedcut_vs_ncut/#correspondence-from-alignedcut","title":"Correspondence from AlignedCut","text":"<p>Since the color is aligned across images, we build a simple software: it checks the distance (in eigenvector color) from one mouse pointer pixel to all the other pixels.</p> <p>: A demo software for one-point prompting segmentation and pseudo-labeling.</p> <p>Video: Heatmap is cosine distance of eigenvectors, w.r.t the mouse pointer. code </p>"},{"location":"api_reference/","title":"NCUT APIs","text":""},{"location":"api_reference/#ncut_pytorch.NCUT","title":"<code>ncut_pytorch.NCUT</code>","text":"<p>Nystrom Normalized Cut.</p> Source code in <code>ncut_pytorch/ncut_pytorch.py</code> <pre><code>class NCUT:\n    \"\"\"Nystrom Normalized Cut.\"\"\"\n\n    def __init__(\n        self,\n        num_eig: int = 100,\n        knn: int = 10,\n        affinity_focal_gamma: float = None,\n        degree: float = 0.05,\n        num_sample: int = 10240,\n        num_sample2: int = 1024,\n        sample_method: Literal[\"farthest\", \"random\"] = \"farthest\",\n        distance: Literal[\"cosine\", \"euclidean\", \"rbf\"] = \"rbf\",\n        device: str = None,\n        move_output_to_cpu: bool = False,\n        make_orthogonal: bool = False,\n        **kwargs,\n    ):\n        \"\"\"\n        Args:\n            num_eig (int): default 100, number of top eigenvectors to return\n            num_sample (int): default 10240, number of samples for Nystrom-like approximation\n            num_sample2 (int): default 1024, number of samples for eigenvector propagation\n            knn (int): default 10, number of KNN for propagating eigenvectors from subgraph to full graph,\n                smaller knn will result in more sharp eigenvectors,\n            sample_method (str): sample method, 'farthest' (default) or 'random'\n                'farthest' is recommended for better approximation\n            distance (str): distance metric, 'cosine' (default) or 'euclidean', 'rbf'\n            affinity_focal_gamma (float): affinity matrix parameter, lower t reduce the weak edge weights,\n                resulting in more sharp eigenvectors, default None (auto search)\n            degree (float): target degree to search for optimal gamma, default 0.05. \n                lower degree will result in more sharp eigenvectors\n            device (str): device to use for computation, if None, will not change device\n                a good practice is to pass features by CPU since it's usually large,\n                and move subgraph affinity to GPU to speed up eigenvector computation\n            make_orthogonal (bool): make eigenvectors orthogonal after propagation, default True\n            move_output_to_cpu (bool): move output to CPU, set to True if you have memory issue\n\n        Examples:\n            &gt;&gt;&gt; from ncut_pytorch import NCUT\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; features = torch.rand(10000, 100)\n            &gt;&gt;&gt; ncut = NCUT(num_eig=20)\n            &gt;&gt;&gt; ncut.fit(features)\n            &gt;&gt;&gt; eigenvectors, eigenvalues = ncut.transform(features)\n            &gt;&gt;&gt; print(eigenvectors.shape, eigenvalues.shape)\n            &gt;&gt;&gt; # (10000, 20) (20,)\n\n            &gt;&gt;&gt; # transform new features\n            &gt;&gt;&gt; new_features = torch.rand(500, 100)\n            &gt;&gt;&gt; new_eigenvectors, _ = ncut.transform(new_features)\n            &gt;&gt;&gt; print(new_eigenvectors.shape)\n            &gt;&gt;&gt; # (500, 20)\n        \"\"\"\n        self.num_eig = num_eig\n        self.num_sample = num_sample\n        self.num_sample2 = num_sample2\n        self.knn = knn\n        self.sample_method = sample_method\n        self.distance = distance\n        self.affinity_focal_gamma = affinity_focal_gamma\n        self.degree = degree\n        self.device = device\n        self.move_output_to_cpu = move_output_to_cpu\n        self.make_orthogonal = make_orthogonal\n\n        self.subgraph_eigen_vector = None\n        self.eigen_value = None\n        self.subgraph_sample_indices = None\n        self.subgraph_features = None\n\n    def fit(self,\n            features: torch.Tensor,\n            precomputed_sampled_indices: torch.Tensor = None\n            ):\n        \"\"\"Fit Nystrom Normalized Cut on the input features.\n        Args:\n            features (torch.Tensor): input features, shape (n_samples, n_features)\n            precomputed_sampled_indices (torch.Tensor): precomputed sampled indices, shape (num_sample,)\n                override the sample_method, if not None\n        Returns:\n            (NCUT): self\n        \"\"\"\n        _n = features.shape[0]\n        if self.num_sample &gt;= _n:\n            logging.info(\n                f\"NCUT nystrom num_sample is larger than number of input samples, nystr\u00f6m approximation is not needed, setting num_sample={_n} and knn=1\"\n            )\n            self.num_sample = _n\n            self.knn = 1\n\n        # save the eigenvectors solution on the sub-sampled graph, do not propagate to full graph yet\n        (self.subgraph_eigen_vector, self.eigen_value,\n        self.subgraph_sample_indices, self.affinity_focal_gamma) = nystrom_ncut(\n                                features,\n                                num_eig=self.num_eig,\n                                num_sample=self.num_sample,\n                                num_sample2=self.num_sample2,\n                                sample_method=self.sample_method,\n                                precomputed_sampled_indices=precomputed_sampled_indices,\n                                distance=self.distance,\n                                affinity_focal_gamma=self.affinity_focal_gamma,\n                                degree=self.degree,\n                                device=self.device,\n                                move_output_to_cpu=self.move_output_to_cpu,\n                                no_propagation=True,\n                            )\n        self.subgraph_features = features[self.subgraph_sample_indices]\n        return self\n\n    def transform(self, features: torch.Tensor):\n        \"\"\"Transform new features using the fitted Nystrom Normalized Cut.\n        Args:\n            features (torch.Tensor): new features, shape (n_samples, n_features)\n        Returns:\n            (torch.Tensor): eigen_vectors, shape (n_samples, num_eig)\n            (torch.Tensor): eigen_values, sorted in descending order, shape (num_eig,)\n        \"\"\"\n\n        # propagate eigenvectors from subgraph to full graph\n        eigen_vector = propagate_knn(\n            self.subgraph_eigen_vector,\n            features,\n            self.subgraph_features,\n            self.knn,\n            num_sample=self.num_sample2,\n            distance=self.distance,\n            affinity_focal_gamma=self.affinity_focal_gamma,\n            device=self.device,\n            move_output_to_cpu=self.move_output_to_cpu,\n        )\n        if self.make_orthogonal:\n            eigen_vector = gram_schmidt(eigen_vector)\n        return eigen_vector, self.eigen_value\n\n    def fit_transform(self,\n                      features: torch.Tensor,\n                      precomputed_sampled_indices: torch.Tensor = None\n                      ):\n        \"\"\"\n        Args:\n            features (torch.Tensor): input features, shape (n_samples, n_features)\n            precomputed_sampled_indices (torch.Tensor): precomputed sampled indices, shape (num_sample,)\n                override the sample_method, if not None\n\n        Returns:\n            (torch.Tensor): eigen_vectors, shape (n_samples, num_eig)\n            (torch.Tensor): eigen_values, sorted in descending order, shape (num_eig,)\n        \"\"\"\n        return self.fit(features, precomputed_sampled_indices=precomputed_sampled_indices).transform(features)\n</code></pre>"},{"location":"api_reference/#ncut_pytorch.NCUT.__init__","title":"<code>__init__(num_eig=100, knn=10, affinity_focal_gamma=None, degree=0.05, num_sample=10240, num_sample2=1024, sample_method='farthest', distance='rbf', device=None, move_output_to_cpu=False, make_orthogonal=False, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>num_eig</code> <code>int</code> <p>default 100, number of top eigenvectors to return</p> <code>100</code> <code>num_sample</code> <code>int</code> <p>default 10240, number of samples for Nystrom-like approximation</p> <code>10240</code> <code>num_sample2</code> <code>int</code> <p>default 1024, number of samples for eigenvector propagation</p> <code>1024</code> <code>knn</code> <code>int</code> <p>default 10, number of KNN for propagating eigenvectors from subgraph to full graph, smaller knn will result in more sharp eigenvectors,</p> <code>10</code> <code>sample_method</code> <code>str</code> <p>sample method, 'farthest' (default) or 'random' 'farthest' is recommended for better approximation</p> <code>'farthest'</code> <code>distance</code> <code>str</code> <p>distance metric, 'cosine' (default) or 'euclidean', 'rbf'</p> <code>'rbf'</code> <code>affinity_focal_gamma</code> <code>float</code> <p>affinity matrix parameter, lower t reduce the weak edge weights, resulting in more sharp eigenvectors, default None (auto search)</p> <code>None</code> <code>degree</code> <code>float</code> <p>target degree to search for optimal gamma, default 0.05.  lower degree will result in more sharp eigenvectors</p> <code>0.05</code> <code>device</code> <code>str</code> <p>device to use for computation, if None, will not change device a good practice is to pass features by CPU since it's usually large, and move subgraph affinity to GPU to speed up eigenvector computation</p> <code>None</code> <code>make_orthogonal</code> <code>bool</code> <p>make eigenvectors orthogonal after propagation, default True</p> <code>False</code> <code>move_output_to_cpu</code> <code>bool</code> <p>move output to CPU, set to True if you have memory issue</p> <code>False</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from ncut_pytorch import NCUT\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; features = torch.rand(10000, 100)\n&gt;&gt;&gt; ncut = NCUT(num_eig=20)\n&gt;&gt;&gt; ncut.fit(features)\n&gt;&gt;&gt; eigenvectors, eigenvalues = ncut.transform(features)\n&gt;&gt;&gt; print(eigenvectors.shape, eigenvalues.shape)\n&gt;&gt;&gt; # (10000, 20) (20,)\n</code></pre> <pre><code>&gt;&gt;&gt; # transform new features\n&gt;&gt;&gt; new_features = torch.rand(500, 100)\n&gt;&gt;&gt; new_eigenvectors, _ = ncut.transform(new_features)\n&gt;&gt;&gt; print(new_eigenvectors.shape)\n&gt;&gt;&gt; # (500, 20)\n</code></pre> Source code in <code>ncut_pytorch/ncut_pytorch.py</code> <pre><code>def __init__(\n    self,\n    num_eig: int = 100,\n    knn: int = 10,\n    affinity_focal_gamma: float = None,\n    degree: float = 0.05,\n    num_sample: int = 10240,\n    num_sample2: int = 1024,\n    sample_method: Literal[\"farthest\", \"random\"] = \"farthest\",\n    distance: Literal[\"cosine\", \"euclidean\", \"rbf\"] = \"rbf\",\n    device: str = None,\n    move_output_to_cpu: bool = False,\n    make_orthogonal: bool = False,\n    **kwargs,\n):\n    \"\"\"\n    Args:\n        num_eig (int): default 100, number of top eigenvectors to return\n        num_sample (int): default 10240, number of samples for Nystrom-like approximation\n        num_sample2 (int): default 1024, number of samples for eigenvector propagation\n        knn (int): default 10, number of KNN for propagating eigenvectors from subgraph to full graph,\n            smaller knn will result in more sharp eigenvectors,\n        sample_method (str): sample method, 'farthest' (default) or 'random'\n            'farthest' is recommended for better approximation\n        distance (str): distance metric, 'cosine' (default) or 'euclidean', 'rbf'\n        affinity_focal_gamma (float): affinity matrix parameter, lower t reduce the weak edge weights,\n            resulting in more sharp eigenvectors, default None (auto search)\n        degree (float): target degree to search for optimal gamma, default 0.05. \n            lower degree will result in more sharp eigenvectors\n        device (str): device to use for computation, if None, will not change device\n            a good practice is to pass features by CPU since it's usually large,\n            and move subgraph affinity to GPU to speed up eigenvector computation\n        make_orthogonal (bool): make eigenvectors orthogonal after propagation, default True\n        move_output_to_cpu (bool): move output to CPU, set to True if you have memory issue\n\n    Examples:\n        &gt;&gt;&gt; from ncut_pytorch import NCUT\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; features = torch.rand(10000, 100)\n        &gt;&gt;&gt; ncut = NCUT(num_eig=20)\n        &gt;&gt;&gt; ncut.fit(features)\n        &gt;&gt;&gt; eigenvectors, eigenvalues = ncut.transform(features)\n        &gt;&gt;&gt; print(eigenvectors.shape, eigenvalues.shape)\n        &gt;&gt;&gt; # (10000, 20) (20,)\n\n        &gt;&gt;&gt; # transform new features\n        &gt;&gt;&gt; new_features = torch.rand(500, 100)\n        &gt;&gt;&gt; new_eigenvectors, _ = ncut.transform(new_features)\n        &gt;&gt;&gt; print(new_eigenvectors.shape)\n        &gt;&gt;&gt; # (500, 20)\n    \"\"\"\n    self.num_eig = num_eig\n    self.num_sample = num_sample\n    self.num_sample2 = num_sample2\n    self.knn = knn\n    self.sample_method = sample_method\n    self.distance = distance\n    self.affinity_focal_gamma = affinity_focal_gamma\n    self.degree = degree\n    self.device = device\n    self.move_output_to_cpu = move_output_to_cpu\n    self.make_orthogonal = make_orthogonal\n\n    self.subgraph_eigen_vector = None\n    self.eigen_value = None\n    self.subgraph_sample_indices = None\n    self.subgraph_features = None\n</code></pre>"},{"location":"api_reference/#ncut_pytorch.NCUT.fit","title":"<code>fit(features, precomputed_sampled_indices=None)</code>","text":"<p>Fit Nystrom Normalized Cut on the input features. Args:     features (torch.Tensor): input features, shape (n_samples, n_features)     precomputed_sampled_indices (torch.Tensor): precomputed sampled indices, shape (num_sample,)         override the sample_method, if not None Returns:     (NCUT): self</p> Source code in <code>ncut_pytorch/ncut_pytorch.py</code> <pre><code>def fit(self,\n        features: torch.Tensor,\n        precomputed_sampled_indices: torch.Tensor = None\n        ):\n    \"\"\"Fit Nystrom Normalized Cut on the input features.\n    Args:\n        features (torch.Tensor): input features, shape (n_samples, n_features)\n        precomputed_sampled_indices (torch.Tensor): precomputed sampled indices, shape (num_sample,)\n            override the sample_method, if not None\n    Returns:\n        (NCUT): self\n    \"\"\"\n    _n = features.shape[0]\n    if self.num_sample &gt;= _n:\n        logging.info(\n            f\"NCUT nystrom num_sample is larger than number of input samples, nystr\u00f6m approximation is not needed, setting num_sample={_n} and knn=1\"\n        )\n        self.num_sample = _n\n        self.knn = 1\n\n    # save the eigenvectors solution on the sub-sampled graph, do not propagate to full graph yet\n    (self.subgraph_eigen_vector, self.eigen_value,\n    self.subgraph_sample_indices, self.affinity_focal_gamma) = nystrom_ncut(\n                            features,\n                            num_eig=self.num_eig,\n                            num_sample=self.num_sample,\n                            num_sample2=self.num_sample2,\n                            sample_method=self.sample_method,\n                            precomputed_sampled_indices=precomputed_sampled_indices,\n                            distance=self.distance,\n                            affinity_focal_gamma=self.affinity_focal_gamma,\n                            degree=self.degree,\n                            device=self.device,\n                            move_output_to_cpu=self.move_output_to_cpu,\n                            no_propagation=True,\n                        )\n    self.subgraph_features = features[self.subgraph_sample_indices]\n    return self\n</code></pre>"},{"location":"api_reference/#ncut_pytorch.NCUT.fit_transform","title":"<code>fit_transform(features, precomputed_sampled_indices=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>features</code> <code>Tensor</code> <p>input features, shape (n_samples, n_features)</p> required <code>precomputed_sampled_indices</code> <code>Tensor</code> <p>precomputed sampled indices, shape (num_sample,) override the sample_method, if not None</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>eigen_vectors, shape (n_samples, num_eig)</p> <code>Tensor</code> <p>eigen_values, sorted in descending order, shape (num_eig,)</p> Source code in <code>ncut_pytorch/ncut_pytorch.py</code> <pre><code>def fit_transform(self,\n                  features: torch.Tensor,\n                  precomputed_sampled_indices: torch.Tensor = None\n                  ):\n    \"\"\"\n    Args:\n        features (torch.Tensor): input features, shape (n_samples, n_features)\n        precomputed_sampled_indices (torch.Tensor): precomputed sampled indices, shape (num_sample,)\n            override the sample_method, if not None\n\n    Returns:\n        (torch.Tensor): eigen_vectors, shape (n_samples, num_eig)\n        (torch.Tensor): eigen_values, sorted in descending order, shape (num_eig,)\n    \"\"\"\n    return self.fit(features, precomputed_sampled_indices=precomputed_sampled_indices).transform(features)\n</code></pre>"},{"location":"api_reference/#ncut_pytorch.NCUT.transform","title":"<code>transform(features)</code>","text":"<p>Transform new features using the fitted Nystrom Normalized Cut. Args:     features (torch.Tensor): new features, shape (n_samples, n_features) Returns:     (torch.Tensor): eigen_vectors, shape (n_samples, num_eig)     (torch.Tensor): eigen_values, sorted in descending order, shape (num_eig,)</p> Source code in <code>ncut_pytorch/ncut_pytorch.py</code> <pre><code>def transform(self, features: torch.Tensor):\n    \"\"\"Transform new features using the fitted Nystrom Normalized Cut.\n    Args:\n        features (torch.Tensor): new features, shape (n_samples, n_features)\n    Returns:\n        (torch.Tensor): eigen_vectors, shape (n_samples, num_eig)\n        (torch.Tensor): eigen_values, sorted in descending order, shape (num_eig,)\n    \"\"\"\n\n    # propagate eigenvectors from subgraph to full graph\n    eigen_vector = propagate_knn(\n        self.subgraph_eigen_vector,\n        features,\n        self.subgraph_features,\n        self.knn,\n        num_sample=self.num_sample2,\n        distance=self.distance,\n        affinity_focal_gamma=self.affinity_focal_gamma,\n        device=self.device,\n        move_output_to_cpu=self.move_output_to_cpu,\n    )\n    if self.make_orthogonal:\n        eigen_vector = gram_schmidt(eigen_vector)\n    return eigen_vector, self.eigen_value\n</code></pre>"},{"location":"api_reference/#convert-eigenvectors-to-rgb-color","title":"Convert Eigenvectors to RGB Color","text":""},{"location":"api_reference/#ncut_pytorch.rgb_from_tsne_3d","title":"<code>ncut_pytorch.rgb_from_tsne_3d(features, num_sample=1000, perplexity=150, metric='cosine', device=None, seed=0, q=0.95, knn=10, **kwargs)</code>","text":"<p>Returns:</p> Type Description <code>Tensor</code> <p>Embedding in 3D, shape (n_samples, 3)</p> <code>Tensor</code> <p>RGB color for each data sample, shape (n_samples, 3)</p> Source code in <code>ncut_pytorch/visualize_utils.py</code> <pre><code>def rgb_from_tsne_3d(\n    features: torch.Tensor,\n    num_sample: int = 1000,\n    perplexity: int = 150,\n    metric: Literal[\"cosine\", \"euclidean\"] = \"cosine\",\n    device: str = None,\n    seed: int = 0,\n    q: float = 0.95,\n    knn: int = 10,\n    **kwargs: Any,\n):\n    \"\"\"\n    Returns:\n        (torch.Tensor): Embedding in 3D, shape (n_samples, 3)\n        (torch.Tensor): RGB color for each data sample, shape (n_samples, 3)\n    \"\"\"\n    try:\n        from sklearn.manifold import TSNE\n    except ImportError:\n        raise ImportError(\n            \"sklearn import failed, please install `pip install scikit-learn`\"\n        )\n    num_sample = min(num_sample, features.shape[0])\n    if perplexity &gt; num_sample // 2:\n        logging.warning(\n            f\"perplexity is larger than num_sample, set perplexity to {num_sample // 2}\"\n        )\n        perplexity = num_sample // 2\n\n    x3d, rgb = _rgb_with_dimensionality_reduction(\n        features=features,\n        num_sample=num_sample,\n        metric=metric,\n        rgb_func=rgb_from_3d_rgb_cube,\n        q=q, knn=knn,\n        seed=seed, device=device,\n        reduction=TSNE, reduction_dim=3, reduction_kwargs={\n            \"perplexity\": perplexity,\n        },\n    )\n\n    return x3d, rgb\n</code></pre>"},{"location":"api_reference/#ncut_pytorch.rgb_from_umap_sphere","title":"<code>ncut_pytorch.rgb_from_umap_sphere(features, num_sample=1000, n_neighbors=150, min_dist=0.1, metric='cosine', device=None, seed=0, q=0.95, knn=10, **kwargs)</code>","text":"<p>Returns:</p> Type Description <code>Tensor</code> <p>Embedding in 2D, shape (n_samples, 2)</p> <code>Tensor</code> <p>RGB color for each data sample, shape (n_samples, 3)</p> Source code in <code>ncut_pytorch/visualize_utils.py</code> <pre><code>def rgb_from_umap_sphere(\n    features: torch.Tensor,\n    num_sample: int = 1000,\n    n_neighbors: int = 150,\n    min_dist: float = 0.1,\n    metric: Literal[\"cosine\", \"euclidean\"] = \"cosine\",\n    device: str = None,\n    seed: int = 0,\n    q: float = 0.95,\n    knn: int = 10,\n    **kwargs: Any,\n):\n    \"\"\"\n    Returns:\n        (torch.Tensor): Embedding in 2D, shape (n_samples, 2)\n        (torch.Tensor): RGB color for each data sample, shape (n_samples, 3)\n    \"\"\"\n    try:\n        from umap import UMAP\n    except ImportError:\n        raise ImportError(\"umap import failed, please install `pip install umap-learn`\")\n\n    def transform_func(X: torch.Tensor) -&gt; torch.Tensor:\n        return torch.stack((\n            torch.sin(X[:, 0]) * torch.cos(X[:, 1]),\n            torch.sin(X[:, 0]) * torch.sin(X[:, 1]),\n            torch.cos(X[:, 0]),\n        ), dim=1)\n\n    x3d, rgb = _rgb_with_dimensionality_reduction(\n        features=features,\n        num_sample=num_sample,\n        metric=metric,\n        rgb_func=rgb_from_3d_rgb_cube,\n        q=q, knn=knn,\n        seed=seed, device=device,\n        reduction=UMAP, reduction_dim=2, reduction_kwargs={\n            \"n_neighbors\": n_neighbors,\n            \"min_dist\": min_dist,\n            \"output_metric\": \"haversine\",\n        },\n        transform_func=transform_func\n    )\n\n    return x3d, rgb\n</code></pre>"},{"location":"api_reference/#ncut_pytorch.rgb_from_umap_3d","title":"<code>ncut_pytorch.rgb_from_umap_3d(features, num_sample=1000, n_neighbors=150, min_dist=0.1, metric='cosine', device=None, seed=0, q=0.95, knn=10, **kwargs)</code>","text":"<p>Returns:</p> Type Description <code>Tensor</code> <p>Embedding in 2D, shape (n_samples, 2)</p> <code>Tensor</code> <p>RGB color for each data sample, shape (n_samples, 3)</p> Source code in <code>ncut_pytorch/visualize_utils.py</code> <pre><code>def rgb_from_umap_3d(\n    features: torch.Tensor,\n    num_sample: int = 1000,\n    n_neighbors: int = 150,\n    min_dist: float = 0.1,\n    metric: Literal[\"cosine\", \"euclidean\"] = \"cosine\",\n    device: str = None,\n    seed: int = 0,\n    q: float = 0.95,\n    knn: int = 10,\n    **kwargs: Any,\n):\n    \"\"\"\n    Returns:\n        (torch.Tensor): Embedding in 2D, shape (n_samples, 2)\n        (torch.Tensor): RGB color for each data sample, shape (n_samples, 3)\n    \"\"\"\n    try:\n        from umap import UMAP\n    except ImportError:\n        raise ImportError(\"umap import failed, please install `pip install umap-learn`\")\n\n    x3d, rgb = _rgb_with_dimensionality_reduction(\n        features=features,\n        num_sample=num_sample,\n        metric=metric,\n        rgb_func=rgb_from_3d_rgb_cube,\n        q=q, knn=knn,\n        seed=seed, device=device,\n        reduction=UMAP, reduction_dim=3, reduction_kwargs={\n            \"n_neighbors\": n_neighbors,\n            \"min_dist\": min_dist,\n        },\n    )\n\n    return x3d, rgb\n</code></pre>"},{"location":"api_reference/#ncut_pytorch.rgb_from_tsne_2d","title":"<code>ncut_pytorch.rgb_from_tsne_2d(features, num_sample=1000, perplexity=150, metric='cosine', device=None, seed=0, q=0.95, knn=10, **kwargs)</code>","text":"<p>Returns:</p> Type Description <code>Tensor</code> <p>Embedding in 2D, shape (n_samples, 2)</p> <code>Tensor</code> <p>RGB color for each data sample, shape (n_samples, 3)</p> Source code in <code>ncut_pytorch/visualize_utils.py</code> <pre><code>def rgb_from_tsne_2d(\n    features: torch.Tensor,\n    num_sample: int = 1000,\n    perplexity: int = 150,\n    metric: Literal[\"cosine\", \"euclidean\"] = \"cosine\",\n    device: str = None,\n    seed: int = 0,\n    q: float = 0.95,\n    knn: int = 10,\n    **kwargs: Any,\n):\n    \"\"\"\n    Returns:\n        (torch.Tensor): Embedding in 2D, shape (n_samples, 2)\n        (torch.Tensor): RGB color for each data sample, shape (n_samples, 3)\n    \"\"\"\n    try:\n        from sklearn.manifold import TSNE\n    except ImportError:\n        raise ImportError(\n            \"sklearn import failed, please install `pip install scikit-learn`\"\n        )\n    num_sample = min(num_sample, features.shape[0])\n    if perplexity &gt; num_sample // 2:\n        logging.warning(\n            f\"perplexity is larger than num_sample, set perplexity to {num_sample // 2}\"\n        )\n        perplexity = num_sample // 2\n\n    x2d, rgb = _rgb_with_dimensionality_reduction(\n        features=features,\n        num_sample=num_sample,\n        metric=metric,\n        rgb_func=rgb_from_2d_colormap,\n        q=q, knn=knn,\n        seed=seed, device=device,\n        reduction=TSNE, reduction_dim=2, reduction_kwargs={\n            \"perplexity\": perplexity,\n        },\n    )\n\n    return x2d, rgb\n</code></pre>"},{"location":"api_reference/#ncut_pytorch.rgb_from_umap_2d","title":"<code>ncut_pytorch.rgb_from_umap_2d(features, num_sample=1000, n_neighbors=150, min_dist=0.1, metric='cosine', device=None, seed=0, q=0.95, knn=10, **kwargs)</code>","text":"<p>Returns:</p> Type Description <code>Tensor</code> <p>Embedding in 2D, shape (n_samples, 2)</p> <code>Tensor</code> <p>RGB color for each data sample, shape (n_samples, 3)</p> Source code in <code>ncut_pytorch/visualize_utils.py</code> <pre><code>def rgb_from_umap_2d(\n    features: torch.Tensor,\n    num_sample: int = 1000,\n    n_neighbors: int = 150,\n    min_dist: float = 0.1,\n    metric: Literal[\"cosine\", \"euclidean\"] = \"cosine\",\n    device: str = None,\n    seed: int = 0,\n    q: float = 0.95,\n    knn: int = 10,\n    **kwargs: Any,\n):\n    \"\"\"\n    Returns:\n        (torch.Tensor): Embedding in 2D, shape (n_samples, 2)\n        (torch.Tensor): RGB color for each data sample, shape (n_samples, 3)\n    \"\"\"\n    try:\n        from umap import UMAP\n    except ImportError:\n        raise ImportError(\"umap import failed, please install `pip install umap-learn`\")\n\n    x2d, rgb = _rgb_with_dimensionality_reduction(\n        features=features,\n        num_sample=num_sample,\n        metric=metric,\n        rgb_func=rgb_from_2d_colormap,\n        q=q, knn=knn,\n        seed=seed, device=device,\n        reduction=UMAP, reduction_dim=2, reduction_kwargs={\n            \"n_neighbors\": n_neighbors,\n            \"min_dist\": min_dist,\n        },\n    )\n\n    return x2d, rgb\n</code></pre>"},{"location":"api_reference/#ncut_pytorch.quantile_normalize","title":"<code>ncut_pytorch.quantile_normalize(x, q=0.95)</code>","text":"<p>normalize each dimension of x to [0, 1], take 95-th percentage, this robust to outliers      1. sort x      2. take q-th quantile          min_value -&gt; (1-q)-th quantile          max_value -&gt; q-th quantile      3. normalize      x = (x - min_value) / (max_value - min_value)</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>input tensor, shape (n_samples, n_features) normalize each feature to 0-1 range</p> required <code>q</code> <code>float</code> <p>quantile, default 0.95</p> <code>0.95</code> <p>Returns:</p> Type Description <p>torch.Tensor: quantile normalized tensor</p> Source code in <code>ncut_pytorch/math_utils.py</code> <pre><code>def quantile_normalize(x, q=0.95):\n    \"\"\"normalize each dimension of x to [0, 1], take 95-th percentage, this robust to outliers\n        &lt;/br&gt; 1. sort x\n        &lt;/br&gt; 2. take q-th quantile\n        &lt;/br&gt;     min_value -&gt; (1-q)-th quantile\n        &lt;/br&gt;     max_value -&gt; q-th quantile\n        &lt;/br&gt; 3. normalize\n        &lt;/br&gt; x = (x - min_value) / (max_value - min_value)\n\n    Args:\n        x (torch.Tensor): input tensor, shape (n_samples, n_features)\n            normalize each feature to 0-1 range\n        q (float): quantile, default 0.95\n\n    Returns:\n        torch.Tensor: quantile normalized tensor\n    \"\"\"\n    # normalize x to 0-1 range, max value is q-th quantile\n    # quantile makes the normalization robust to outliers\n    if isinstance(x, np.ndarray):\n        x = torch.tensor(x)\n    vmax, vmin = quantile_min_max(x, q, 1 - q)\n    x = (x - vmin) / (vmax - vmin)\n    x = x.clamp(0, 1)\n    return x\n</code></pre>"},{"location":"api_reference/#ncut-functional-api","title":"NCUT Functional API","text":""},{"location":"api_reference/#ncut_pytorch.nystrom_ncut","title":"<code>ncut_pytorch.nystrom_ncut(features, num_eig=100, degree=0.05, distance='rbf', num_sample=10240, num_sample2=1024, knn=10, matmul_chunk_size=16384, sample_method='farthest', precomputed_sampled_indices=None, affinity_focal_gamma=None, device=None, make_orthogonal=False, no_propagation=False, move_output_to_cpu=None, **kwargs)</code>","text":"<p>PyTorch implementation of Faster Nystrom Normalized cut. Args:     features (torch.Tensor): feature matrix, shape (n_samples, n_features)     num_eig (int): default 100, number of top eigenvectors to return     degree (float): target degree to search for optimal gamma, default 0.05.          lower degree will result in more sharp eigenvectors     distance (str): distance metric, 'cosine' (default) or 'euclidean', 'rbf'     num_sample (int): default 10240, number of samples for Nystrom-like approximation     num_sample2 (int): default 1024, number of samples for eigenvector propagation     knn (int): default 10, number of KNN for propagating eigenvectors from subgraph to full graph,         smaller knn will result in more sharp eigenvectors,     sample_method (str): sample method, 'farthest' (default) or 'random'         'farthest' is recommended for better approximation     precomputed_sampled_indices (torch.Tensor): precomputed sampled indices, shape (num_sample,)         override the sample_method, if not None     affinity_focal_gamma (float): affinity matrix parameter, lower t reduce the weak edge weights,         resulting in more sharp eigenvectors, default None (auto search)     device (str): device to use for computation, if None, will not change device         a good practice is to pass features by CPU since it's usually large,         and move subgraph affinity to GPU to speed up eigenvector computation     make_orthogonal (bool): make eigenvectors orthogonal after propagation, default True     no_propagation (bool): if True, skip the eigenvector propagation step, only return the subgraph eigenvectors     move_output_to_cpu (bool): move output to CPU, set to True if output is too large to fit in GPU memory Returns:     (torch.Tensor): eigenvectors, shape (n_samples, num_eig)     (torch.Tensor): eigenvalues, sorted in descending order, shape (num_eig,)     (torch.Tensor): sampled_indices used by Nystrom-like approximation subgraph, shape (num_sample,) Examples:     &gt;&gt;&gt; from ncut_pytorch import nystrom_ncut     &gt;&gt;&gt; import torch     &gt;&gt;&gt; features = torch.rand(10000, 100)     &gt;&gt;&gt; eigenvectors, eigenvalues = nystrom_ncut(features, num_eig=20)     &gt;&gt;&gt; print(eigenvectors.shape, eigenvalues.shape)     &gt;&gt;&gt; # (10000, 20) (20,)</p> Source code in <code>ncut_pytorch/ncut_pytorch.py</code> <pre><code>def nystrom_ncut(\n    features: torch.Tensor,\n    num_eig: int = 100,\n    degree: float = 0.05,\n    distance: Literal[\"cosine\", \"euclidean\", \"rbf\"] = \"rbf\",\n    num_sample: int = 10240,\n    num_sample2: int = 1024,\n    knn: int = 10,\n    matmul_chunk_size: int = 16384,\n    sample_method: Literal[\"farthest\", \"random\"] = \"farthest\",\n    precomputed_sampled_indices: torch.Tensor = None,\n    affinity_focal_gamma: float = None,\n    device: str = None,\n    make_orthogonal: bool = False,\n    no_propagation: bool = False,\n    move_output_to_cpu: bool = None,\n    **kwargs,\n):\n    \"\"\"PyTorch implementation of Faster Nystrom Normalized cut.\n    Args:\n        features (torch.Tensor): feature matrix, shape (n_samples, n_features)\n        num_eig (int): default 100, number of top eigenvectors to return\n        degree (float): target degree to search for optimal gamma, default 0.05. \n            lower degree will result in more sharp eigenvectors\n        distance (str): distance metric, 'cosine' (default) or 'euclidean', 'rbf'\n        num_sample (int): default 10240, number of samples for Nystrom-like approximation\n        num_sample2 (int): default 1024, number of samples for eigenvector propagation\n        knn (int): default 10, number of KNN for propagating eigenvectors from subgraph to full graph,\n            smaller knn will result in more sharp eigenvectors,\n        sample_method (str): sample method, 'farthest' (default) or 'random'\n            'farthest' is recommended for better approximation\n        precomputed_sampled_indices (torch.Tensor): precomputed sampled indices, shape (num_sample,)\n            override the sample_method, if not None\n        affinity_focal_gamma (float): affinity matrix parameter, lower t reduce the weak edge weights,\n            resulting in more sharp eigenvectors, default None (auto search)\n        device (str): device to use for computation, if None, will not change device\n            a good practice is to pass features by CPU since it's usually large,\n            and move subgraph affinity to GPU to speed up eigenvector computation\n        make_orthogonal (bool): make eigenvectors orthogonal after propagation, default True\n        no_propagation (bool): if True, skip the eigenvector propagation step, only return the subgraph eigenvectors\n        move_output_to_cpu (bool): move output to CPU, set to True if output is too large to fit in GPU memory\n    Returns:\n        (torch.Tensor): eigenvectors, shape (n_samples, num_eig)\n        (torch.Tensor): eigenvalues, sorted in descending order, shape (num_eig,)\n        (torch.Tensor): sampled_indices used by Nystrom-like approximation subgraph, shape (num_sample,)\n    Examples:\n        &gt;&gt;&gt; from ncut_pytorch import nystrom_ncut\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; features = torch.rand(10000, 100)\n        &gt;&gt;&gt; eigenvectors, eigenvalues = nystrom_ncut(features, num_eig=20)\n        &gt;&gt;&gt; print(eigenvectors.shape, eigenvalues.shape)\n        &gt;&gt;&gt; # (10000, 20) (20,)\n    \"\"\"\n\n    if precomputed_sampled_indices is not None:\n        sampled_indices = precomputed_sampled_indices\n    else:\n        sampled_indices = run_subgraph_sampling(\n            features,\n            num_sample=num_sample,\n            sample_method=sample_method,\n        )\n\n    if move_output_to_cpu is None:\n        move_output_to_cpu = True if features.device.type == \"cpu\" else False\n\n    sampled_features = features[sampled_indices]\n    device = which_device(sampled_features.device, device)\n    sampled_features = sampled_features.to(device)\n\n    # compute affinity matrix on subgraph\n    if affinity_focal_gamma is None:\n        with torch.no_grad():\n            affinity_focal_gamma = find_gamma_by_degree_after_fps(sampled_features, degree, distance=distance)\n\n    A = affinity_from_features(sampled_features, affinity_focal_gamma=affinity_focal_gamma, distance=distance)\n\n    # compute normalized cut on the subgraph\n    eigen_vector, eigen_value = ncut(A, num_eig)\n\n    if no_propagation:\n        return eigen_vector, eigen_value, sampled_indices, affinity_focal_gamma\n\n    # propagate eigenvectors from subgraph to full graph\n    eigen_vector = propagate_knn(\n        eigen_vector,\n        features,\n        features[sampled_indices],\n        knn,\n        num_sample=num_sample2,\n        distance=distance,\n        affinity_focal_gamma=affinity_focal_gamma,\n        chunk_size=matmul_chunk_size,\n        device=device,\n        move_output_to_cpu=move_output_to_cpu,\n    )\n\n    # post-hoc orthogonalization\n    if make_orthogonal:\n        eigen_vector = gram_schmidt(eigen_vector)\n\n    return eigen_vector, eigen_value\n</code></pre>"},{"location":"api_reference/#ncut_pytorch.affinity_from_features","title":"<code>ncut_pytorch.affinity_from_features(features, features_B=None, affinity_focal_gamma=1.0, distance='rbf', **kwargs)</code>","text":"<p>Compute affinity matrix from input features.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>Tensor</code> <p>input features, shape (n_samples, n_features)</p> required <code>feature_B</code> <code>Tensor</code> <p>optional, if not None, compute affinity between two features</p> required <code>affinity_focal_gamma</code> <code>float</code> <p>affinity matrix parameter, lower t reduce the edge weights on weak connections, default 1.0</p> <code>1.0</code> <code>distance</code> <code>str</code> <p>distance metric, 'cosine' (default) or 'euclidean', 'rbf'.</p> <code>'rbf'</code> <code>normalize_features</code> <code>bool</code> <p>normalize input features before computing affinity matrix</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>affinity matrix, shape (n_samples, n_samples)</p> Source code in <code>ncut_pytorch/math_utils.py</code> <pre><code>def affinity_from_features(\n    features: torch.Tensor,\n    features_B: torch.Tensor = None,\n    affinity_focal_gamma: float = 1.0,\n    distance: Literal[\"cosine\", \"euclidean\", \"rbf\"] = \"rbf\",\n    **kwargs,\n):\n    \"\"\"Compute affinity matrix from input features.\n\n    Args:\n        features (torch.Tensor): input features, shape (n_samples, n_features)\n        feature_B (torch.Tensor, optional): optional, if not None, compute affinity between two features\n        affinity_focal_gamma (float): affinity matrix parameter, lower t reduce the edge weights\n            on weak connections, default 1.0\n        distance (str): distance metric, 'cosine' (default) or 'euclidean', 'rbf'.\n        normalize_features (bool): normalize input features before computing affinity matrix\n\n    Returns:\n        (torch.Tensor): affinity matrix, shape (n_samples, n_samples)\n    \"\"\"\n    # compute affinity matrix from input features\n\n    features_B = features if features_B is None else features_B\n\n    if distance == \"cosine\":\n        features = F.normalize(features, dim=-1)\n        features_B = F.normalize(features_B, dim=-1)\n        A = 1 - features @ features_B.T\n    elif distance == \"euclidean\":\n        A = torch.cdist(features, features_B, p=2)\n    elif distance == \"rbf\":\n        d = torch.cdist(features, features_B, p=2)\n        A = torch.pow(d, 2)\n    else:\n        raise ValueError(\"distance should be 'cosine' or 'euclidean', 'rbf'\")\n\n    if distance == \"rbf\":\n        sigma = 2 * affinity_focal_gamma * features.var(dim=0).sum()\n    else:\n        sigma = affinity_focal_gamma\n\n    A = torch.exp(-A / sigma)\n\n    return A\n</code></pre>"},{"location":"api_reference/#ncut_pytorch.ncut","title":"<code>ncut_pytorch.ncut(A, num_eig=100)</code>","text":"<p>PyTorch implementation of Normalized cut without Nystrom-like approximation.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>Tensor</code> <p>affinity matrix, shape (n_samples, n_samples)</p> required <code>num_eig</code> <code>int</code> <p>number of eigenvectors to return</p> <code>100</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>eigenvectors corresponding to the eigenvalues, shape (n_samples, num_eig)</p> <code>Tensor</code> <p>eigenvalues of the eigenvectors, sorted in descending order</p> Source code in <code>ncut_pytorch/ncut_pytorch.py</code> <pre><code>def ncut(\n    A: torch.Tensor,\n    num_eig: int = 100,\n):\n    \"\"\"PyTorch implementation of Normalized cut without Nystrom-like approximation.\n\n    Args:\n        A (torch.Tensor): affinity matrix, shape (n_samples, n_samples)\n        num_eig (int): number of eigenvectors to return\n\n    Returns:\n        (torch.Tensor): eigenvectors corresponding to the eigenvalues, shape (n_samples, num_eig)\n        (torch.Tensor): eigenvalues of the eigenvectors, sorted in descending order\n    \"\"\"\n\n    # normalization; A = D^(-1/2) A D^(-1/2)\n    A = normalize_affinity(A)\n\n    eigen_vector, eigen_value, _ = svd_lowrank(A, num_eig)\n\n    # correct the random rotation (flipping sign) of eigenvectors\n    eigen_vector = correct_rotation(eigen_vector)\n\n    return eigen_vector, eigen_value\n</code></pre>"},{"location":"application_segmentation/","title":"NCUT Tutorial: Segmentation Application","text":"<p>NCUT embedding (eigenvectors) can be discretized to create segmentation mask.</p> <p>This application output segmentation mask from input 1) all eigenvectors, and input 2) one prompt eigenvector (at a clicked latent pixel). The mask is computed by measuring the cosine similarity between the clicked eigenvector and all the eigenvectors in the latent space, cosine similarity is then normalized, scaled, then threshold to create mask.</p> <ol> <li> <p>Compute NCUT eigenvectors.</p> </li> <li> <p>Compute the cosine similarity between the clicked eigenvector and all the eigenvectors in the latent space.</p> </li> <li> <p>Transform the heatmap, normalize and apply scaling (gamma).</p> </li> <li> <p>Threshold the heatmap to get the mask.</p> </li> <li> <p>Optionally de-noise the mask by removing small connected components.</p> </li> </ol>"},{"location":"application_segmentation/#load-images","title":"Load Images","text":"Click to expand full code  <pre><code># pip install datasets\nfrom datasets import load_dataset\n</code></pre> <pre><code># pip install datasets\nfrom datasets import load_dataset\n\ndataset = load_dataset('EgoThink/EgoThink', 'Activity', trust_remote_code=True)\ndataset = dataset['test']\nimages = [dataset[i]['image'] for i in range(100)]\n\nimport numpy as np\nfrom PIL import Image\nimport torch\n\ndef transform_image(image, resolution=(1024, 1024)):\n    image = image.convert('RGB').resize(resolution, Image.LANCZOS)\n    # Convert to torch tensor\n    image = torch.tensor(np.array(image).transpose(2, 0, 1)).float()\n    image = image / 255\n    # Normalize\n    mean = [0.485, 0.456, 0.406]\n    std = [0.229, 0.224, 0.225]\n    image = (image - torch.tensor(mean).view(3, 1, 1)) / torch.tensor(std).view(3, 1, 1)\n    return image\n\nimages_tsf = torch.stack([transform_image(img) for img in images])\n</code></pre>"},{"location":"application_segmentation/#compute-ncut","title":"Compute NCUT","text":"<pre><code>from ncut_pytorch import NCUT, rgb_from_tsne_3d\nfrom ncut_pytorch.backbone import load_model, extract_features\n\nmodel = load_model(model_name=\"SAM(sam_vit_b)\")\nmodel_features = extract_features(images_tsf, model, node_type='attn', layer=11, batch_size=4)\n# model_features = model(images_tsf)['attn'][11]  # this also works\n\nnum_eig = 30\ninp = model_features.reshape(-1, 768) # flatten\neigvectors, eigvalues = NCUT(num_eig=num_eig, device='cuda:0').fit_transform(inp)\ntsne_x3d, tsne_rgb = rgb_from_tsne_3d(eigvectors, device='cuda:0')\n\neigvectors = eigvectors.reshape(-1, 64, 64, num_eig) # (B, H, W, num_eig)\ntsne_rgb = tsne_rgb.reshape(-1, 64, 64, 3) # (B, H, W, 3)\n</code></pre>"},{"location":"application_segmentation/#one-point-prompt","title":"One Point Prompt","text":"<pre><code>x1, x2 = 34, 46\nclicked_eigvec = eigvectors[0, x1, x2]  # hand pixel\nimport matplotlib.pyplot as plt\n# display the clicked pixel on the tsne_rgb image\nplt.imshow(tsne_rgb[0])\nplt.scatter(x2, x1, c='red', s=100, label='clicked pixel', edgecolors='black')\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"application_segmentation/#get-segmentation-mask-correspondence-to-the-clicked-point","title":"Get Segmentation Mask (correspondence to the clicked point)","text":"<pre><code>from ncut_pytorch import get_mask\nmasks = get_mask(eigvectors, clicked_eigvec, threshold=0.5, gamma=1.0, denoise=True, denoise_area_th=3)\n</code></pre>"},{"location":"application_segmentation/#results-one-point","title":"Results: One point","text":"<pre><code>import matplotlib.pyplot as plt\n\nfig, axs = plt.subplots(2, 4, figsize=(16, 8))\nfor i in range(4):\n    axs[0, i].imshow(masks[i])\n    axs[0, i].axis('off')\n    axs[1, i].imshow(tsne_rgb[i])\n    axs[1, i].axis('off')\n</code></pre>"},{"location":"application_segmentation/#multiple-points-prompt","title":"Multiple Points Prompt","text":"<pre><code>clicks = []  # (i_img, x1, x2)\nclicks.append((0, 34, 46))\nclicks.append((0, 50, 43))\nclicks.append((0, 60, 20))\nclicks.append((0, 61, 29))\nclicks.append((0, 45, 20))\nclicks.append((1, 59, 32))\nclicks.append((1, 60, 55))\nclicks.append((1, 52, 30))\nclicks.append((1, 50, 15))\nclicks.append((1, 45, 55))\n\nimport matplotlib.pyplot as plt\n# display 2 images\nfig, axs = plt.subplots(1, 2, figsize=(16, 8))\naxs[0].imshow(tsne_rgb[0])\naxs[1].imshow(tsne_rgb[1])\nfor i_img, x1, x2 in clicks:\n    axs[i_img].scatter(x2, x1, c='red', s=100, label='clicked pixel', edgecolors='black')\n\naxs[0].legend()\naxs[1].legend()\nplt.show()\n</code></pre>"},{"location":"application_segmentation/#how-to-combine-multiple-points","title":"How to combine multiple points","text":"<p>Run segmentation separately for every point, then do a | (or) operation to merge the masks.</p> <pre><code>masks = []\nfor i_img, x1, x2 in clicks:    \n    masks.append(get_mask(eigvectors, eigvectors[i_img, x1, x2], threshold=0.7, gamma=1.0, denoise=True, denoise_area_th=8))\nmask = np.stack(masks)\nmask = mask.sum(0) &gt; 0\n</code></pre>"},{"location":"application_segmentation/#results-multiple-point","title":"Results: Multiple Point","text":"<pre><code>fig, axs = plt.subplots(2, 4, figsize=(16, 8))\nfor i in range(4):\n    axs[0, i].imshow(mask[i])\n    axs[0, i].axis('off')\n    axs[1, i].imshow(tsne_rgb[i])\n    axs[1, i].axis('off')\n</code></pre>"},{"location":"application_segmentation/#negative-point-prompt","title":"Negative Point Prompt","text":"<pre><code>negative_clicks = []\nnegative_clicks.append((0, 35, 15))\nnegative_clicks.append((0, 25, 22))\nnegative_clicks.append((1, 26, 10))\n\nfig, axs = plt.subplots(1, 2, figsize=(16, 8))\naxs[0].imshow(tsne_rgb[0])\naxs[1].imshow(tsne_rgb[1])\nfor i_img, x1, x2 in clicks:\n    axs[i_img].scatter(x2, x1, c='red', s=100, label='clicked pixel', edgecolors='black')\nfor i_img, x1, x2 in negative_clicks:\n    axs[i_img].scatter(x2, x1, c='blue', s=100, label='negative clicked pixel', edgecolors='black')\naxs[0].legend()\naxs[1].legend()\nplt.show()\n</code></pre>"},{"location":"application_segmentation/#how-to-combine-negative-points","title":"How to combine negative points","text":"<p>Run segmentation separately for every point, then do a 'not' operation to merge the positive and negative mask.</p> <pre><code>positive_masks, negative_masks = [], []\nfor i_img, x1, x2 in clicks:    \n    mask = get_mask(eigvectors, eigvectors[i_img, x1, x2], threshold=0.7, gamma=1.0, denoise=True, denoise_area_th=10)\n    positive_masks.append(mask)\nfor i_img, x1, x2 in negative_clicks:\n    mask = get_mask(eigvectors, eigvectors[i_img, x1, x2], threshold=0.99, gamma=1.0, denoise=False)\n    negative_masks.append(mask)\npositive_mask = np.stack(positive_masks).sum(0) &gt; 0\nnegative_mask = np.stack(negative_masks).sum(0) &gt; 0\nfinal_mask = positive_mask &amp; ~negative_mask\n</code></pre>"},{"location":"application_segmentation/#results-negative-point","title":"Results: Negative Point","text":"<pre><code>fig, axs = plt.subplots(2, 4, figsize=(16, 8))\nfor i in range(4):\n    axs[0, i].imshow(final_mask[i])\n    axs[0, i].axis('off')\n    axs[1, i].imshow(tsne_rgb[i])\n    axs[1, i].axis('off')\n</code></pre> The complete code for this tutorial <p>huzeyann/ncut_pytorch</p> <p></p>"},{"location":"backbones/","title":"Feature Extraction from Backbone Models","text":"<p>Any backbone model works as plug-in feature extractor.  We have implemented some backbone models, here is a list of available models:</p> <pre><code> \nfrom ncut_pytorch.backbone import list_models \nprint(list_models()) \n[\n  'SAM2(sam2_hiera_t)', 'SAM2(sam2_hiera_s)', 'SAM2(sam2_hiera_b+)', 'SAM2(sam2_hiera_l)', \n  'SAM(sam_vit_b)', 'SAM(sam_vit_l)', 'SAM(sam_vit_h)', 'MobileSAM(TinyViT)', \n  'DiNOv2reg(dinov2_vits14_reg)', 'DiNOv2reg(dinov2_vitb14_reg)', 'DiNOv2reg(dinov2_vitl14_reg)', 'DiNOv2reg(dinov2_vitg14_reg)', \n  'DiNOv2(dinov2_vits14)', 'DiNOv2(dinov2_vitb14)', 'DiNOv2(dinov2_vitl14)', 'DiNOv2(dinov2_vitg14)', \n  'DiNO(dino_vits8_896)', 'DiNO(dino_vitb8_896)', 'DiNO(dino_vits8_672)', 'DiNO(dino_vitb8_672)', 'DiNO(dino_vits8_448)', 'DiNO(dino_vitb8_448)', 'DiNO(dino_vits16_448)', 'DiNO(dino_vitb16_448)',\n  'Diffusion(stabilityai/stable-diffusion-2)', 'Diffusion(CompVis/stable-diffusion-v1-4)', 'Diffusion(stabilityai/stable-diffusion-3-medium-diffusers)',\n  'CLIP(ViT-B-16/openai)', 'CLIP(ViT-L-14/openai)', 'CLIP(ViT-H-14/openai)', 'CLIP(ViT-B-16/laion2b_s34b_b88k)', \n  'CLIP(convnext_base_w_320/laion_aesthetic_s13b_b82k)', 'CLIP(convnext_large_d_320/laion2b_s29b_b131k_ft_soup)', 'CLIP(convnext_xxlarge/laion2b_s34b_b82k_augreg_soup)', \n  'CLIP(eva02_base_patch14_448/mim_in22k_ft_in1k)', \"CLIP(eva02_large_patch14_448/mim_m38m_ft_in22k_in1k)\",\n  'MAE(vit_base)', 'MAE(vit_large)', 'MAE(vit_huge)', \n  'ImageNet(vit_base)'\n]\nfrom ncut_pytorch.backbone_text import list_models \nprint(list_models()) \n[\"meta-llama/Meta-Llama-3.1-8B\", \"meta-llama/Meta-Llama-3-8B\", \"gpt2\"]\n </code></pre>"},{"location":"backbones/#loading-models","title":"Loading models","text":"<pre><code>from ncut_pytorch.backbone import load_model  # image models\nmodel = load_model(model_name='Diffusion(CompVis/stable-diffusion-v1-4)')\n\nfrom ncut_pytorch.backbone_text import load_text_model  # text models\ngpt2 = load_text_model(\"gpt2\")\n</code></pre>"},{"location":"backbones/#install-extra-dependency","title":"Install Extra Dependency","text":"<p>Some models require installing extra dependency, an error message will be printed if the dependency is not installed, please follow the error message to install the dependency.</p> <pre><code>&gt;&gt;&gt; model = load_model(\"SAM(sam_vit_b)\")\nError: Please install segment_anything from https://github.com/facebookresearch/segment-anything.git\npip install git+ttps://github.com/facebookresearch/segment-anything.git\n\n&gt;&gt;&gt; model = load_model(\"Diffusion(stabilityai/stable-diffusion-2)\")\nError: Please install diffusers to use this class.\npip install diffusers\n</code></pre>"},{"location":"backbones/#hf_access_token-huggingface-access-token","title":"<code>HF_ACCESS_TOKEN</code> HuggingFace Access Token","text":"<p>Some models requires setup HuggingFace access token to access. You will need to </p> <ol> <li> <p>request access at their HuggingFace repository (listed below). </p> </li> <li> <p>put your access token to environment variable <code>HF_ACCESS_TOKEN</code></p> </li> </ol> <pre><code>os.environ['HF_ACCESS_TOKEN'] = \"your_token_here\"\nllama = load_model(\"meta-llama/Meta-Llama-3.1-8B\")\n</code></pre> model_name HuggingFace repository stabilityai/stable-diffusion-3-medium https://huggingface.co/stabilityai/stable-diffusion-3-medium meta-llama/Meta-Llama-3.1-8B https://huggingface.co/meta-llama/Meta-Llama-3.1-8B meta-llama/Meta-Llama-3-8B https://huggingface.co/meta-llama/Meta-Llama-3-8B"},{"location":"backbones/#feature-extraction-output","title":"Feature Extraction Output","text":"<p>Feature extraction output is a dictionary:</p> <ol> <li> <p>dict keys are <code>node_name</code> (attention layer, mlp layer, etc.)</p> </li> <li> <p>dict values are a list of features, each element in the list is for each layer of the model.</p> </li> </ol> <pre><code># out_dict = {node_name: List[layer_0, layer_1, ...]}\n</code></pre>"},{"location":"backbones/#image-model-output","title":"Image Model Output","text":"<pre><code>from ncut_pytorch.backbone import load_model\nimport torch\nmodel = load_model(model_name=\"SAM(sam_vit_b)\").cuda()\nimages = torch.rand(1, 3, 1024, 1024).cuda()\nout_dict = model(images)\n\nfor node_name in out_dict.keys():\n    print(f\"node_name: `{node_name}`, num_layers: {len(out_dict[node_name])}\")\n    print(f\"layer_0 shape: {out_dict[node_name][0].shape}\")\n</code></pre> <pre><code>node_name: `attn`, num_layers: 12\nlayer_0 shape: torch.Size([1, 64, 64, 768])\nnode_name: `mlp`, num_layers: 12\nlayer_0 shape: torch.Size([1, 64, 64, 768])\nnode_name: `block`, num_layers: 12\nlayer_0 shape: torch.Size([1, 64, 64, 768])\n</code></pre>"},{"location":"backbones/#text-model-output","title":"Text Model Output","text":"<pre><code>from ncut_pytorch.backbone_text import load_text_model\nimport torch\nmodel = load_text_model(model_name=\"gpt2\").cuda()\nout_dict = model(\"I know this sky loves you\")\n\nfor node_name in out_dict.keys():\n    if isinstance(out_dict[node_name][0], torch.Tensor):\n        print(f\"node_name: `{node_name}`, num_layers: {len(out_dict[node_name])}\")\n        print(f\"layer_0 shape: {out_dict[node_name][0].shape}\")\n    else:\n        print(f\"token_texts: {out_dict[node_name]}\")\n</code></pre> <pre><code>node_name: `attn`, num_layers: 12\nlayer_0 shape: torch.Size([1, 6, 768])\nnode_name: `mlp`, num_layers: 12\nlayer_0 shape: torch.Size([1, 6, 768])\nnode_name: `block`, num_layers: 12\nlayer_0 shape: torch.Size([1, 6, 768])\ntoken_texts: ['I', ' know', ' this', ' sky', ' loves', ' you']\n</code></pre>"},{"location":"coloring/","title":"Tutorial 5 - Coloring","text":""},{"location":"coloring/#rgb-cube-from-t-sne-and-umap","title":"RGB cube from t-SNE and UMAP","text":"<p>NCUT eigenvectors are high dimensional, we need a bigger screen to see them all. Or we could use t-SNE/UMAP to reduce the dimension of eigenvectors to 3D, and use a 3D colormap (RGB cube) to show eigenvectors as an RGB image. </p> <p> PROCEDURE - Coloring</p> <p> 1. extract features from DiNOv2 layer9, 20 images, feature shape [20, h, w, 768]  2. compute NCUT eigenvectors, 30 eigenvectors, eigenvector shape [20, h, w, 30]  3. use t-SNE or UMAP to reduce 30 eigenvectors to 3D, shape [20, h, w, 3]  4. color each pixel by 3D colormap (RGB cube) </p> Pros Cons t-SNE(3D) make fuller use of the color space slow UMAP(3D) 2x faster than t-SNE holes in the color space UMAP(sphere) can be plotted in 2D do not use the full color space t-SNE(2D) can be plotted in 2D do not use the full color space UMAP(2D) can be plotted in 2D do not use the full color space"},{"location":"coloring/#rgb-cube-rotation","title":"RGB cube rotationThe complete code for this tutorial","text":"<p>Human perception is not uniform on the RGB color space -- green vs. yellow is less perceptually different than red vs. blue. Therefore, it's a good idea to rotate the RGB cube and try a different color. In the following example, all images has the same euclidean distance matrix, but perceptually they could tell different story.</p> <pre><code># 9-way rotation of the rgb\nimport matplotlib.pyplot as plt\nfrom ncut_pytorch import rotate_rgb_cube\n\nfig, axs = plt.subplots(2, 3, figsize=(6, 4))\naxs = axs.flatten()\nfor i in range(6):\n    _rgb = rotate_rgb_cube(rgb[4], position=i+1)\n    ax[i].imshow(_rgb)\n    ax[i].axis(\"off\")\nplt.suptitle(\"Rotation of the RGB cube\")\nplt.tight_layout()\nplt.show()\n</code></pre> <p>huzeyann/ncut_pytorch</p> <p></p>"},{"location":"compare/","title":"NCUT and t-SNE/UMAP","text":""},{"location":"compare/#comparison-of-ncut-vs-pca","title":"Comparison of NCUT vs. PCA","text":"<p>NCUT and PCA could both be used as dimension reduction before feeding into t-SNE or UMAP, it's a standard practice and recommended by the original t-SME/UMAP authors to use PCA as pre-processing. However, PCA by definition does not give good segmentation, NCUT gives better segmentation because:</p> <ol> <li> <p>the use of nonlinear pair-wise similarity space</p> </li> <li> <p>symmetrical normalization and graph cut objective</p> </li> <li> <p>clean up of the affinity matrix by focal gamma</p> </li> </ol> <p>We use the following procedure to compare NCUT and PCA, t-SNE/UMAP is applied after NCUT or PCA.</p>"},{"location":"compare/#ncutt-sne-vs-pcat-sne","title":"NCUT+t-SNE vs. PCA+t-SNE","text":"<p> PROCEDURE - Comparison</p> <p> For NCUT+t-SNE:  1. extract features from DiNOv2 layer9, 20 images, feature shape [20, h, w, 768]  2. compute NCUT eigenvectors, 20 eigenvectors, eigenvector shape [20, h, w, 20]  3. use t-SNE or UMAP to reduce 20 eigenvectors to 3D, shape [20, h, w, 3]  4. color each pixel by 3D colormap (RGB cube)  For PCA+t-SNE:  1. extract features from DiNOv2 layer9, 20 images, feature shape [20, h, w, 768]  2. compute PCA embeddings, 20 PCs, PC shape [20, h, w, 20]  3. use t-SNE or UMAP to reduce 20 PCs to 3D, shape [20, h, w, 3]  4. color each pixel by 3D colormap (RGB cube)  </p>"},{"location":"compare/#ncutumap-vs-pcaumap","title":"NCUT+UMAP vs. PCA+UMAP","text":""},{"location":"compare/#raw-ncut-vs-raw-pca","title":"raw NCUT vs. raw PCA","text":""},{"location":"demo/","title":"Demo","text":"<p>This demo is hosted at UPenn </p> <p>Link1 </p> <p>Link2 password is:      158.130.50.41     Copied! </p> <p>Link3 (for UPenn internal network)</p> <p> HuggingFace </p> <p> </p>"},{"location":"demo/#how-to-host-this-demo-yourself","title":"How to host this demo yourself","text":"<p>Step 1. Install Docker and Nvidia-docker plugin.</p> <pre><code>curl -fsSL https://get.docker.com -o get-docker.sh | sudo sh\ndistribution=$(. /etc/os-release;echo $ID$VERSION_ID) &amp;&amp; curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg &amp;&amp; curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list | sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list\nsudo apt-get update &amp;&amp; sudo apt-get install -y nvidia-container-toolkit\n</code></pre> <p>Step 2. Run this docker container locally. </p> <pre><code>docker run -it -p 7860:7860 --platform=linux/amd64 --gpus all \\\n    -e HF_ACCESS_TOKEN=\"YOUR_VALUE_HERE\" \\\n    -e USE_HUGGINGFACE_ZEROGPU=\"false\" \\\n    -e DOWNLOAD_ALL_MODELS_DATASETS=\"false\" \\\n    registry.hf.space/huzey-ncut-pytorch:latest python app.py\n</code></pre> <p><code>HF_ACCESS_TOKEN</code> can be left blank, only fill it if you need access to restricted models (Llama, SDv3), please see Backbones.</p> <p>Step 3. Use the printed out link to access your local demo.</p> <pre><code>...\nRunning on local URL:  http://0.0.0.0:7860\nRunning on public URL: https://some_link_here.gradio.live\n</code></pre>"},{"location":"gallery/","title":"Model Gallery","text":""},{"location":"gallery/#image","title":"Image","text":""},{"location":"gallery/#video","title":"Video","text":""},{"location":"gallery/#text","title":"Text","text":"<p>All the models, including those not listed in this gallery, can be accessed at \ud83e\udd17HuggingFace Demo.</p> <p> </p> <p></p>"},{"location":"gallery_application/","title":"Application Gallery","text":"<p>Video: Point-Prompting Segmentation, application of NCUT embedding.  Mask is produced by thresholding distance on NCUT eigenvectors. code </p> <p>This application output segmentation mask given prompt points, it work by computing and thresholding L2 distance on the NCUT embedding:</p> <ol> <li> <p>Compute NCUT embedding, spectral-tSNE. Color is aligned across all images Aligned NCUT</p> </li> <li> <p>Take rgb value (NCUT embedding) at prompt pixel, compute L2 distance to all other pixels</p> </li> <li> <p>Threshold L2 distance to get segmentation mask</p> </li> <li> <p>De-noise the mask, combine multiple prompt points.</p> </li> </ol> <p>You can try this demo segmentation app at \ud83e\udd17HuggingFace (Switch to the Tab \"Application\").</p>"},{"location":"gallery_application/#example-1","title":"Example 1","text":""},{"location":"gallery_application/#example-2","title":"Example 2","text":""},{"location":"gallery_application/#example-3","title":"Example 3","text":""},{"location":"gallery_application/#example-4","title":"Example 4","text":""},{"location":"gallery_clip/","title":"Gallery clip","text":"Click to expand full code  <pre><code>class CLIP(torch.nn.Module):\n</code></pre> <pre><code># %%\nfrom typing import Optional, Tuple\nfrom einops import rearrange\nimport torch\nfrom PIL import Image\nimport torchvision.transforms as transforms\nfrom torch import nn\nimport numpy as np\n\n# %%\nclass CLIP(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        from transformers import CLIPProcessor, CLIPModel\n\n        model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n        # processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n        self.model = model.eval().cuda()\n\n        def new_forward(\n            self,\n            hidden_states: torch.Tensor,\n            attention_mask: torch.Tensor,\n            causal_attention_mask: torch.Tensor,\n            output_attentions: Optional[bool] = False,\n        ) -&gt; Tuple[torch.FloatTensor]:\n\n            residual = hidden_states\n\n            hidden_states = self.layer_norm1(hidden_states)\n            hidden_states, attn_weights = self.self_attn(\n                hidden_states=hidden_states,\n                attention_mask=attention_mask,\n                causal_attention_mask=causal_attention_mask,\n                output_attentions=output_attentions,\n            )\n            self.attn_output = hidden_states.clone()\n            hidden_states = residual + hidden_states\n\n            residual = hidden_states\n            hidden_states = self.layer_norm2(hidden_states)\n            hidden_states = self.mlp(hidden_states)\n            self.mlp_output = hidden_states.clone()\n\n            hidden_states = residual + hidden_states\n\n            outputs = (hidden_states,)\n\n            if output_attentions:\n                outputs += (attn_weights,)\n\n            self.block_output = hidden_states.clone()\n            return outputs\n\n        setattr(self.model.vision_model.encoder.layers[0].__class__, \"forward\", new_forward)\n\n    @torch.no_grad()\n    def forward(self, x): \n\n        out = self.model.vision_model(x)\n\n        attn_outputs, mlp_outputs, block_outputs = [], [], []\n        for i, blk in enumerate(self.model.vision_model.encoder.layers):\n            attn_outputs.append(blk.attn_output)\n            mlp_outputs.append(blk.mlp_output)\n            block_outputs.append(blk.block_output)\n\n        attn_outputs = torch.stack(attn_outputs)\n        mlp_outputs = torch.stack(mlp_outputs)\n        block_outputs = torch.stack(block_outputs)\n        return attn_outputs, mlp_outputs, block_outputs\n\n\ndef image_dino_feature(\n    images, resolution=(224, 224),\n):\n    if isinstance(images, list):\n        assert isinstance(images[0], Image.Image), \"Input must be a list of PIL images.\"\n    else:\n        assert isinstance(images, Image.Image), \"Input must be a PIL image.\"\n        images = [images]\n\n    transform = transforms.Compose(\n        [\n            transforms.Resize(resolution),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n        ]\n    )\n\n    feat_extractor = CLIP()\n\n    attn_outputs, mlp_outputs, block_outputs = [], [], []\n    for i, image in enumerate(images):\n        torch_image = transform(image)\n        # feat = feat_extractor(torch_image.unsqueeze(0).cuda()).cpu()\n        attn_output, mlp_output, block_output = feat_extractor(\n            torch_image.unsqueeze(0).cuda()\n        )\n        # feats.append(feat)\n        attn_outputs.append(attn_output.cpu())\n        mlp_outputs.append(mlp_output.cpu())\n        block_outputs.append(block_output.cpu())\n    attn_outputs = torch.cat(attn_outputs, dim=1)\n    mlp_outputs = torch.cat(mlp_outputs, dim=1)\n    block_outputs = torch.cat(block_outputs, dim=1)\n\n    # feats = torch.cat(feats, dim=1)\n    # feats = rearrange(feats, \"l b c h w -&gt; l b h w c\")\n    return attn_outputs, mlp_outputs, block_outputs\n\n\n# %%\nfrom torchvision.datasets import ImageFolder\n\ndataset = ImageFolder(\"/data/coco/\")\nprint(\"number of images in the dataset:\", len(dataset))\n# %%\nimages = [dataset[i][0] for i in range(20)]\nattn_outputs, mlp_outputs, block_outputs = image_dino_feature(images)\n# %%\nprint(attn_outputs.shape, mlp_outputs.shape, block_outputs.shape)\n# %%\n# remove 1 cls token\ndef reshape_output(outputs):\n    from einops import rearrange\n    outputs = rearrange(outputs[:, :, 1:, :], \"l b (h w) c -&gt; l b h w c\", h=14, w=14)\n    return outputs\nattn_outputs = reshape_output(attn_outputs)\nmlp_outputs = reshape_output(mlp_outputs)\nblock_outputs = reshape_output(block_outputs)\n# %%\nnum_nodes = np.prod(attn_outputs.shape[1:4])\n\n\n# %%\nfrom ncut_pytorch import NCUT, rgb_from_tsne_3d\n\nfor i_layer in range(12):\n\n    attn_eig, _ = NCUT(num_eig=100, device=\"cuda:0\").fit_transform(\n        attn_outputs[i_layer].reshape(-1, attn_outputs[i_layer].shape[-1])\n    )\n    _, attn_rgb = rgb_from_tsne_3d(attn_eig, device=\"cuda:0\")\n    attn_rgb = attn_rgb.reshape(attn_outputs[i_layer].shape[:3] + (3,))\n    mlp_eig, _ = NCUT(num_eig=100, device=\"cuda:0\").fit_transform(\n        mlp_outputs[i_layer].reshape(-1, mlp_outputs[i_layer].shape[-1])\n    )\n    _, mlp_rgb = rgb_from_tsne_3d(mlp_eig, device=\"cuda:0\")\n    mlp_rgb = mlp_rgb.reshape(mlp_outputs[i_layer].shape[:3] + (3,))\n    block_eig, _ = NCUT(num_eig=100, device=\"cuda:0\").fit_transform(\n        block_outputs[i_layer].reshape(-1, block_outputs[i_layer].shape[-1])\n    )\n    _, block_rgb = rgb_from_tsne_3d(block_eig, device=\"cuda:0\")\n    block_rgb = block_rgb.reshape(block_outputs[i_layer].shape[:3] + (3,))\n\n    from matplotlib import pyplot as plt\n\n    fig, axs = plt.subplots(4, 10, figsize=(10, 5))\n    for ax in axs.flatten():\n        ax.axis(\"off\")\n    for i_col in range(10):\n        axs[0, i_col].imshow(images[i_col])\n        axs[1, i_col].imshow(attn_rgb[i_col])\n        axs[2, i_col].imshow(mlp_rgb[i_col])\n        axs[3, i_col].imshow(block_rgb[i_col])\n\n    axs[1, 0].set_title(\"attention layer output\", ha=\"left\")\n    axs[2, 0].set_title(\"MLP layer output\", ha=\"left\")\n    axs[3, 0].set_title(\"sum of residual stream\", ha=\"left\")\n\n    plt.suptitle(f\"CLIP layer {i_layer} NCUT spectral-tSNE\", fontsize=16)\n    # plt.show()\n    save_dir = \"/workspace/output/gallery/clip\"\n    import os\n    os.makedirs(save_dir, exist_ok=True)\n    plt.savefig(f\"{save_dir}/clip_layer_{i_layer}.jpg\", bbox_inches=\"tight\")\n    plt.close()\n# %%\n</code></pre>"},{"location":"gallery_dataset/","title":"Dataset Gallery","text":"<p>NCUT works good for out-of-distribution datasets (see SET 4 and SET 5).</p>"},{"location":"gallery_dataset/#ncut-is-training-free-run-time-adaptation","title":"NCUT is Training-free Run-time Adaptation","text":"<p>No re-training, no fine-tune, the backbone is freezed. (Backbone model is DiNO in the blow examples)</p> <p>The graph affinity adapt to images, even it's out-of-distribution: Nodes in the input set of images are contrasting against each other, thus makes the affinity matrix (in some sense it's a kernel matrix) adapts to out-of-distribution images.</p>"},{"location":"gallery_dataset/#example-1","title":"Example 1","text":""},{"location":"gallery_dataset/#example-2","title":"Example 2","text":""},{"location":"gallery_dataset/#example-3","title":"Example 3","text":""},{"location":"gallery_dataset/#example-4","title":"Example 4","text":""},{"location":"gallery_dataset/#example-5","title":"Example 5","text":""},{"location":"gallery_dinov2/","title":"Gallery dinov2","text":"Click to expand full code  <pre><code>class DiNOv2(torch.nn.Module):\n</code></pre> <pre><code># %%\nfrom einops import rearrange\nimport torch\nfrom PIL import Image\nimport torchvision.transforms as transforms\nfrom torch import nn\nimport numpy as np\n\n# %%\nclass DiNOv2(torch.nn.Module):\n    def __init__(self, ver=\"dinov2_vitb14_reg\"):\n        super().__init__()\n        self.dinov2 = torch.hub.load(\"facebookresearch/dinov2\", ver)\n        self.dinov2.requires_grad_(False)\n        self.dinov2.eval()\n        self.dinov2 = self.dinov2.cuda()\n\n        def new_block_forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n            def attn_residual_func(x):\n                return self.ls1(self.attn(self.norm1(x)))\n\n            def ffn_residual_func(x):\n                return self.ls2(self.mlp(self.norm2(x)))\n\n            attn_output = attn_residual_func(x)\n            self.attn_output = attn_output.clone()\n            x = x + attn_output\n            mlp_output = ffn_residual_func(x)\n            self.mlp_output = mlp_output.clone()\n            x = x + mlp_output\n            block_output = x\n            self.block_output = block_output.clone()\n            return x\n\n        setattr(self.dinov2.blocks[0].__class__, \"forward\", new_block_forward)\n\n    @torch.no_grad()\n    def forward(self, x): \n\n        out = self.dinov2(x)\n\n        attn_outputs, mlp_outputs, block_outputs = [], [], []\n        for i, blk in enumerate(self.dinov2.blocks):\n            attn_outputs.append(blk.attn_output)\n            mlp_outputs.append(blk.mlp_output)\n            block_outputs.append(blk.block_output)\n\n        attn_outputs = torch.stack(attn_outputs)\n        mlp_outputs = torch.stack(mlp_outputs)\n        block_outputs = torch.stack(block_outputs)\n        return attn_outputs, mlp_outputs, block_outputs\n\n\ndef image_dino_feature(\n    images, resolution=(448, 448),\n):\n    if isinstance(images, list):\n        assert isinstance(images[0], Image.Image), \"Input must be a list of PIL images.\"\n    else:\n        assert isinstance(images, Image.Image), \"Input must be a PIL image.\"\n        images = [images]\n\n    transform = transforms.Compose(\n        [\n            transforms.Resize(resolution),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n        ]\n    )\n\n    feat_extractor = DiNOv2()\n\n    attn_outputs, mlp_outputs, block_outputs = [], [], []\n    for i, image in enumerate(images):\n        torch_image = transform(image)\n        # feat = feat_extractor(torch_image.unsqueeze(0).cuda()).cpu()\n        attn_output, mlp_output, block_output = feat_extractor(\n            torch_image.unsqueeze(0).cuda()\n        )\n        # feats.append(feat)\n        attn_outputs.append(attn_output.cpu())\n        mlp_outputs.append(mlp_output.cpu())\n        block_outputs.append(block_output.cpu())\n    attn_outputs = torch.cat(attn_outputs, dim=1)\n    mlp_outputs = torch.cat(mlp_outputs, dim=1)\n    block_outputs = torch.cat(block_outputs, dim=1)\n\n    # feats = torch.cat(feats, dim=1)\n    # feats = rearrange(feats, \"l b c h w -&gt; l b h w c\")\n    return attn_outputs, mlp_outputs, block_outputs\n\n\n# %%\nfrom torchvision.datasets import ImageFolder\n\ndataset = ImageFolder(\"/data/coco/\")\nprint(\"number of images in the dataset:\", len(dataset))\n# %%\nimages = [dataset[i][0] for i in range(20)]\nattn_outputs, mlp_outputs, block_outputs = image_dino_feature(images)\n# %%\nprint(attn_outputs.shape, mlp_outputs.shape, block_outputs.shape)\n# remove 1 cls token 4 reg tokens\ndef reshape_output(outputs):\n    from einops import rearrange\n    outputs = rearrange(outputs[:, :, 5:, :], \"l b (h w) c -&gt; l b h w c\", h=32, w=32)\n    return outputs\nattn_outputs = reshape_output(attn_outputs)\nmlp_outputs = reshape_output(mlp_outputs)\nblock_outputs = reshape_output(block_outputs)\n# %%\nnum_nodes = np.prod(attn_outputs.shape[1:4])\n\n\n# %%\nfrom ncut_pytorch import NCUT, rgb_from_tsne_3d\n\nfor i_layer in range(12):\n\n    attn_eig, _ = NCUT(num_eig=100, device=\"cuda:0\").fit_transform(\n        attn_outputs[i_layer].reshape(-1, attn_outputs[i_layer].shape[-1])\n    )\n    _, attn_rgb = rgb_from_tsne_3d(attn_eig, device=\"cuda:0\")\n    attn_rgb = attn_rgb.reshape(attn_outputs[i_layer].shape[:3] + (3,))\n    mlp_eig, _ = NCUT(num_eig=100, device=\"cuda:0\").fit_transform(\n        mlp_outputs[i_layer].reshape(-1, mlp_outputs[i_layer].shape[-1])\n    )\n    _, mlp_rgb = rgb_from_tsne_3d(mlp_eig, device=\"cuda:0\")\n    mlp_rgb = mlp_rgb.reshape(mlp_outputs[i_layer].shape[:3] + (3,))\n    block_eig, _ = NCUT(num_eig=100, device=\"cuda:0\").fit_transform(\n        block_outputs[i_layer].reshape(-1, block_outputs[i_layer].shape[-1])\n    )\n    _, block_rgb = rgb_from_tsne_3d(block_eig, device=\"cuda:0\")\n    block_rgb = block_rgb.reshape(block_outputs[i_layer].shape[:3] + (3,))\n\n    from matplotlib import pyplot as plt\n\n    fig, axs = plt.subplots(4, 10, figsize=(10, 5))\n    for ax in axs.flatten():\n        ax.axis(\"off\")\n    for i_col in range(10):\n        axs[0, i_col].imshow(images[i_col])\n        axs[1, i_col].imshow(attn_rgb[i_col])\n        axs[2, i_col].imshow(mlp_rgb[i_col])\n        axs[3, i_col].imshow(block_rgb[i_col])\n\n    axs[1, 0].set_title(\"attention layer output\", ha=\"left\")\n    axs[2, 0].set_title(\"MLP layer output\", ha=\"left\")\n    axs[3, 0].set_title(\"sum of residual stream\", ha=\"left\")\n\n    plt.suptitle(f\"DiNOv2reg layer {i_layer} NCUT spectral-tSNE\", fontsize=16)\n    # plt.show()\n    save_dir = \"/workspace/output/gallery/dinov2reg\"\n    import os\n    os.makedirs(save_dir, exist_ok=True)\n    plt.savefig(f\"{save_dir}/dinov2reg_layer_{i_layer}.jpg\", bbox_inches=\"tight\")\n    plt.close()\n\nexit(0)\n</code></pre>"},{"location":"gallery_dinov2_video/","title":"Gallery dinov2 video","text":"Click to expand full code  <pre><code>class DiNOv2(torch.nn.Module):\n</code></pre> <pre><code># %%\nfrom einops import rearrange\nimport torch\nfrom PIL import Image\nimport torchvision.transforms as transforms\nfrom torch import nn\nimport numpy as np\n\nN_FRAMES = 600\n\n# %%\nclass DiNOv2(torch.nn.Module):\n    def __init__(self, ver=\"dinov2_vitb14_reg\"):\n        super().__init__()\n        self.dinov2 = torch.hub.load(\"facebookresearch/dinov2\", ver)\n        self.dinov2.requires_grad_(False)\n        self.dinov2.eval()\n        self.dinov2 = self.dinov2.cuda()\n\n        def new_block_forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n            def attn_residual_func(x):\n                return self.ls1(self.attn(self.norm1(x)))\n\n            def ffn_residual_func(x):\n                return self.ls2(self.mlp(self.norm2(x)))\n\n            attn_output = attn_residual_func(x)\n            self.attn_output = attn_output.clone()\n            x = x + attn_output\n            mlp_output = ffn_residual_func(x)\n            self.mlp_output = mlp_output.clone()\n            x = x + mlp_output\n            block_output = x\n            self.block_output = block_output.clone()\n            return x\n\n        setattr(self.dinov2.blocks[0].__class__, \"forward\", new_block_forward)\n\n    @torch.no_grad()\n    def forward(self, x): \n\n        out = self.dinov2(x)\n\n        attn_outputs, mlp_outputs, block_outputs = [], [], []\n        for i, blk in enumerate(self.dinov2.blocks):\n            attn_outputs.append(blk.attn_output)\n            mlp_outputs.append(blk.mlp_output)\n            block_outputs.append(blk.block_output)\n\n        attn_outputs = torch.stack(attn_outputs)\n        mlp_outputs = torch.stack(mlp_outputs)\n        block_outputs = torch.stack(block_outputs)\n        # return attn_outputs, mlp_outputs, block_outputs\n        return block_outputs\n\n\n\ndef transform_images(frames, size=(896, 896)):\n    resized = []\n    length = len(frames)\n    for i in range(length):\n        frame = frames[i]\n        # image = Image.fromarray((frame * 255).astype(np.uint8))\n        image = Image.fromarray(frame)\n        image = image.resize(size, Image.ANTIALIAS)\n        image = np.array(image) / 255.0\n        resized.append(np.array(image))\n    frames = np.stack(resized, axis=0)\n    frames = frames.transpose(0, 3, 1, 2)  # (N, H, W, C) -&gt; (N, C, H, W)\n    frames = torch.tensor(frames, dtype=torch.float32)\n    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\n    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n    frames = (frames - mean) / std\n    return frames\n\n\ndef read_video(video_path: str) -&gt; torch.Tensor:\n    try:\n        from decord import VideoReader\n    except ImportError:\n        raise ImportError(\"Please install the decord library: pip install decord\")\n\n    vr = VideoReader(video_path)\n    print(f\"Total frames: {len(vr)}\")\n    # frames = vr.get_batch(range(len(vr))).asnumpy()\n    lenth = len(vr)\n    lenth = N_FRAMES if lenth &gt; N_FRAMES else lenth\n    frames = vr.get_batch(np.arange(lenth)).asnumpy()\n    # if less than N_FRAMES frames, repeat the last frame\n    if lenth &lt; N_FRAMES:\n        last_frame = frames[-1]\n        for i in range(N_FRAMES - lenth):\n            frames = np.append(frames, last_frame.reshape(1, *last_frame.shape), axis=0)\n    return frames\n\n\n@torch.no_grad()\ndef video_sam_feature(video_path, checkpoint=\"/data/sam_model/sam_vit_b_01ec64.pth\"):\n    frames = read_video(video_path)\n\n    feat_extractor = DiNOv2()\n\n    attn_outputs, mlp_outputs, block_outputs = [], [], []\n    for i in range(frames.shape[0]):\n        frame = frames[i]\n        frame = transform_images([frame])\n        block_output = feat_extractor(frame.cuda())\n        block_outputs.append(block_output[-1].cpu())\n    block_outputs = torch.stack(block_outputs)\n    return block_outputs\n\n\n# %%\nvideo_paths = [\n    \"/workspace/ego4d_dog_264.mp4\",\n    \"/workspace/ego4d_tractor_264.mp4\",\n    \"/workspace/ego4d_drum_264.mp4\",\n]\n# %%\nfeatures = []\nfor video_path in video_paths:\n    features.append(video_sam_feature(video_path))\nfeatures = torch.cat(features, dim=0)\n# %%\nprint(features.shape)\n# remove reg tokens\nfeatures = features[:, :, 5:].reshape(-1, 64, 64, 768)\n# %%\nnum_nodes = np.prod(features.shape[:-1])\nprint(\"Number of nodes:\", num_nodes)\n\n# %%\nfrom ncut_pytorch import NCUT, rgb_from_tsne_3d\n\neigvectors, eigenvalues = NCUT(\n    num_eig=100, num_sample=30000, device=\"cuda:0\"\n).fit_transform(features.reshape(-1, features.shape[-1]))\n# %%\n_, rgb = rgb_from_tsne_3d(eigvectors, num_sample=50000, perplexity=500, knn=10, device=\"cuda:0\")\n\n\n# %%\nimage_rgb = rgb.reshape(*features.shape[:-1], 3)\n\nimport matplotlib.pyplot as plt\n\n\nframes1 = read_video(video_paths[0])\nframes2 = read_video(video_paths[1])\nframes3 = read_video(video_paths[2])\n\nsave_dir = \"/tmp/ncut_video_dinov2/\"\nimport shutil\nshutil.rmtree(save_dir, ignore_errors=True)\nimport os\nos.makedirs(save_dir, exist_ok=True)\n\ndef resize_image(image, size=(540, 540)):\n    image = Image.fromarray(image)\n    image = image.resize(size, Image.ANTIALIAS)\n    image = np.array(image)\n    return image\n\nfor i_frame in range(0, N_FRAMES):\n    fig, axes = plt.subplots(2, 3, figsize=(10, 7))\n    for ax in axes.flatten():\n        ax.axis(\"off\")\n\n    offsets = [0, N_FRAMES, 2 * N_FRAMES]\n    for i, frames in enumerate([frames1, frames2, frames3]):\n        axes[0, i].imshow(resize_image(frames[i_frame]))\n        offset = offsets[i]\n        np_image = image_rgb[i_frame+offset].cpu().numpy()\n        np_image = (np_image * 255).astype(np.uint8)\n        axes[1, i].imshow(resize_image(np_image))\n\n    plt.tight_layout()\n    plt.savefig(f\"{save_dir}/frame_{i_frame:04d}.png\")\n    # plt.show()\n    plt.close()\n\n# %%\n# make video\nsave_dir = \"/tmp/ncut_video_dinov2/\"\n\ndef make_video_from_images(image_dir, video_path):\n    import cv2\n    import os\n\n    images = sorted(os.listdir(image_dir))\n    frame = cv2.imread(os.path.join(image_dir, images[0]))\n    height, width, layers = frame.shape\n\n    video = cv2.VideoWriter(video_path, cv2.VideoWriter_fourcc(*\"mp4v\"), 30, (width, height))\n\n    for image in images:\n        video.write(cv2.imread(os.path.join(image_dir, image)))\n\n    cv2.destroyAllWindows()\n    video.release()\n\nmake_video_from_images(save_dir, \"/workspace/output/ncut_video_dinov2.mp4\")\n# %%\n</code></pre> <p>This video example use image model without temporal dimension</p> <ol> <li> <p>extract image feature for every frame, independently</p> </li> <li> <p>concatenate all the image features and compute NCUT</p> </li> </ol>"},{"location":"gallery_gpt2/","title":"Gallery gpt2","text":"Click to expand full code  <pre><code>class GPT2(torch.nn.Module):\n</code></pre> <pre><code># %%\nfrom transformers import GPT2Tokenizer, GPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = GPT2Model.from_pretrained('gpt2')\n# %%\nfrom typing import Optional, Tuple, Union\nimport torch\n\ndef new_forward(\n    self,\n    hidden_states: Optional[Tuple[torch.FloatTensor]],\n    layer_past: Optional[Tuple[torch.Tensor]] = None,\n    attention_mask: Optional[torch.FloatTensor] = None,\n    head_mask: Optional[torch.FloatTensor] = None,\n    encoder_hidden_states: Optional[torch.Tensor] = None,\n    encoder_attention_mask: Optional[torch.FloatTensor] = None,\n    use_cache: Optional[bool] = False,\n    output_attentions: Optional[bool] = False,\n) -&gt; Union[Tuple[torch.Tensor], Optional[Tuple[torch.Tensor, Tuple[torch.FloatTensor, ...]]]]:\n    residual = hidden_states\n    hidden_states = self.ln_1(hidden_states)\n    attn_outputs = self.attn(\n        hidden_states,\n        layer_past=layer_past,\n        attention_mask=attention_mask,\n        head_mask=head_mask,\n        use_cache=use_cache,\n        output_attentions=output_attentions,\n    )\n    attn_output = attn_outputs[0]  # output_attn: a, present, (attentions)\n    outputs = attn_outputs[1:]\n    # residual connection\n    self.attn_output = attn_output.clone()\n    hidden_states = attn_output + residual\n\n    if encoder_hidden_states is not None:\n        # add one self-attention block for cross-attention\n        if not hasattr(self, \"crossattention\"):\n            raise ValueError(\n                f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with \"\n                \"cross-attention layers by setting `config.add_cross_attention=True`\"\n            )\n        residual = hidden_states\n        hidden_states = self.ln_cross_attn(hidden_states)\n        cross_attn_outputs = self.crossattention(\n            hidden_states,\n            attention_mask=attention_mask,\n            head_mask=head_mask,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_attention_mask,\n            output_attentions=output_attentions,\n        )\n        attn_output = cross_attn_outputs[0]\n        # residual connection\n        hidden_states = residual + attn_output\n        outputs = outputs + cross_attn_outputs[2:]  # add cross attentions if we output attention weights\n\n    residual = hidden_states\n    hidden_states = self.ln_2(hidden_states)\n    feed_forward_hidden_states = self.mlp(hidden_states)\n    # residual connection\n    self.mlp_output = feed_forward_hidden_states.clone()\n    hidden_states = residual + feed_forward_hidden_states\n\n    if use_cache:\n        outputs = (hidden_states,) + outputs\n    else:\n        outputs = (hidden_states,) + outputs[1:]\n\n    self.block_output = hidden_states.clone()\n    return outputs  # hidden_states, present, (attentions, cross_attentions)\n# %%\nsetattr(model.h[0].__class__, \"forward\", new_forward)\n# %%\ntext = \"\"\"\n1. The majestic giraffe, with its towering height and distinctive long neck, roams the savannas of Africa. These gentle giants use their elongated tongues to pluck leaves from the tallest trees, making them well-adapted to their environment. Their unique coat patterns, much like human fingerprints, are unique to each individual.\n2. Penguins, the tuxedoed birds of the Antarctic, are expert swimmers and divers. These flightless seabirds rely on their dense, waterproof feathers and streamlined bodies to propel through icy waters in search of fish, krill, and other marine life. Their huddled colonies and amusing waddles make them a favorite among wildlife enthusiasts.\n3. The mighty African elephant, the largest land mammal, is revered for its intelligence and strong family bonds. These gentle giants use their versatile trunks for various tasks, from drinking and feeding to communicating and greeting one another. Their massive ears and wrinkled skin make them an iconic symbol of the African wilderness.\n4. The colorful and flamboyant peacock, native to Asia, is known for its stunning iridescent plumage. During mating season, the males fan out their magnificent train of feathers, adorned with intricate eye-like patterns, in an elaborate courtship display to attract potential mates, making them a true spectacle of nature.\n5. The sleek and powerful cheetah, the fastest land animal, is built for speed and agility. With its distinctive black tear-like markings and slender body, this feline predator can reach top speeds of up to 70 mph during short bursts, allowing it to chase down its prey with remarkable precision.\n6. The playful and intelligent dolphin, a highly social marine mammal, is known for its friendly demeanor and impressive acrobatic abilities. These aquatic creatures use echolocation to navigate and hunt, and their complex communication systems have long fascinated researchers studying their intricate social structures and cognitive abilities.\n7. The majestic bald eagle, the national emblem of the United States, soars high above with its distinctive white head and tail feathers. These powerful raptors are skilled hunters, swooping down from great heights to catch fish and other prey with their sharp talons, making them an iconic symbol of strength and freedom.\n8. The industrious beaver, nature's skilled engineers, are known for their remarkable ability to construct dams and lodges using their sharp incisors and webbed feet. These semiaquatic rodents play a crucial role in shaping their aquatic ecosystems, creating habitats for numerous other species while demonstrating their ingenuity and perseverance.\n9. The vibrant and enchanting hummingbird, one of the smallest bird species, is a true marvel of nature. With their rapidly flapping wings and ability to hover in mid-air, these tiny feathered creatures are expert pollinators, flitting from flower to flower in search of nectar and playing a vital role in plant reproduction.\n10. The majestic polar bear, the apex predator of the Arctic, is perfectly adapted to its icy environment. With its thick insulating fur and specialized paws for gripping the ice, this powerful carnivore relies on its exceptional hunting skills and keen senses to locate and capture seals, its primary prey, in the harsh Arctic landscape.\n\"\"\"\nencoded_input = tokenizer(text, return_tensors='pt')\nwith torch.no_grad():\n    output = model(**encoded_input, output_hidden_states=True)\n# %%\nattn_outputs, mlp_outputs, block_outputs = [], [], []\nfor i, blk in enumerate(model.h):\n    attn_outputs.append(blk.attn_output)\n    mlp_outputs.append(blk.mlp_output)\n    block_outputs.append(blk.block_output)\nattn_outputs = torch.stack(attn_outputs)\nmlp_outputs = torch.stack(mlp_outputs)\nblock_outputs = torch.stack(block_outputs)\nprint(attn_outputs.shape, mlp_outputs.shape, block_outputs.shape)\n# %%\ntoken_ids = encoded_input['input_ids']\ntoken_texts = [tokenizer.decode([token_id]) for token_id in token_ids[0]]\n\n# %%\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\nimport numpy as np\n\ndef plot_one_ax(token_texts, rgb, ax, fig, title, num_lines=5):\n    # Define the colors\n    # fill nan with 0\n    rgb = np.nan_to_num(rgb)\n    colors = [mcolors.rgb2hex(rgb[i]) for i in range(len(token_texts))]\n\n    # Split the sentence into words\n    words = token_texts\n\n\n    y_pos = 0.9\n    x_pos = 0.0\n    max_word_length = max(len(word) for word in words)\n    count = 0\n    for word, color in zip(words, colors):\n        if word == '&lt;|begin_of_text|&gt;':\n            word = '&lt;SoT&gt;'\n            y_pos -= 0.05\n            x_pos = 0.0\n\n\n        text_color = 'black' if sum(mcolors.hex2color(color)) &gt; 1.3 else 'white'  # Choose text color based on background color\n        # text_color = 'black'\n        txt = ax.text(x_pos, y_pos, word, color=text_color, fontsize=12, bbox=dict(facecolor=color, alpha=0.8, edgecolor='none', pad=2))\n        txt_width = txt.get_window_extent().width / (fig.dpi * fig.get_size_inches()[0])  # Calculate the width of the text in inches\n\n        x_pos += txt_width * 1.1 + 0.01  # Adjust the spacing between words\n\n        if x_pos &gt; 0.97:\n            y_pos -= 0.2\n            x_pos = 0.0\n            count += 1\n            if count &gt;= num_lines:\n                break\n        # break\n\n    # Remove the axis ticks and spines\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n\n    ax.set_title(title, fontsize=14)\n\n# %%\nfrom ncut_pytorch import NCUT, rgb_from_tsne_3d\n\nfor i_layer in range(12):\n\n    attn_eig, _ = NCUT(num_eig=20, device=\"cuda:0\").fit_transform(\n        attn_outputs[i_layer].reshape(-1, attn_outputs[i_layer].shape[-1])\n    )\n    _, attn_rgb = rgb_from_tsne_3d(attn_eig, device=\"cuda:0\", seed=42)\n    mlp_eig, _ = NCUT(num_eig=20, device=\"cuda:0\").fit_transform(\n        mlp_outputs[i_layer].reshape(-1, mlp_outputs[i_layer].shape[-1])\n    )\n    _, mlp_rgb = rgb_from_tsne_3d(mlp_eig, device=\"cuda:0\", seed=42)\n    block_eig, _ = NCUT(num_eig=20, device=\"cuda:0\").fit_transform(\n        block_outputs[i_layer].reshape(-1, block_outputs[i_layer].shape[-1])\n    )\n    _, block_rgb = rgb_from_tsne_3d(block_eig, device=\"cuda:0\", seed=42)\n\n    fig, axs = plt.subplots(3, 1, figsize=(10, 5))\n    plot_one_ax(token_texts, attn_rgb.numpy(), axs[0], fig, \"attention layer output\")\n    plot_one_ax(token_texts, mlp_rgb.numpy(), axs[1], fig, \"MLP layer output\")\n    plot_one_ax(token_texts, block_rgb.numpy(), axs[2], fig, \"sum of residual stream\")\n\n    plt.suptitle(f\"GPT-2 layer {i_layer} NCUT spectral-tSNE\", fontsize=16)\n    plt.tight_layout()\n    # plt.show()\n\n    save_dir = \"/workspace/output/gallery/gpt2\"\n    import os\n    os.makedirs(save_dir, exist_ok=True)\n    plt.savefig(f\"{save_dir}/gpt2_layer_{i_layer}.jpg\", bbox_inches=\"tight\")\n    plt.close()\n\n# %%\n</code></pre>"},{"location":"gallery_llama3/","title":"Gallery llama3","text":"Click to expand full code  <pre><code>class Llama3(torch.nn.Module):\n</code></pre> <pre><code># %%\nfrom llama import Llama\nfrom typing import List, Optional\nimport torch\nimport os\n#%%\nos.environ[\"RANK\"] = \"0\"\nos.environ[\"WORLD_SIZE\"] = \"1\"\nos.environ[\"MASTER_ADDR\"] = \"localhost\"\nos.environ[\"MASTER_PORT\"] = \"12355\"\ntorch.distributed.init_process_group(\"nccl\")\n\n\nclass Llama3:\n    def __init__(self):\n        ckpt_dir = \"/data/Meta-Llama-3-8B\"\n        tokenizer_path = \"/data/Meta-Llama-3-8B/tokenizer.model\"\n        max_batch_size = 4\n        max_seq_len: int = 2048\n\n        self.generator = Llama.build(\n            ckpt_dir=ckpt_dir,\n            tokenizer_path=tokenizer_path,\n            max_seq_len=max_seq_len,\n            max_batch_size=max_batch_size,\n        )\n\n    @torch.no_grad()\n    def forward(self, prompts: List[str]):\n\n        for prompt in prompts:\n            out = self.generator.text_completion(prompts=[prompt], max_gen_len=1)\n\n# %%\nllama = Llama3()\n# %%\ndef new_forward(\n    self,\n    x: torch.Tensor,\n    start_pos: int,\n    freqs_cis: torch.Tensor,\n    mask: Optional[torch.Tensor],\n):\n    self.saved_attn = self.attention(self.attention_norm(x), start_pos, freqs_cis, mask)\n    h = x + self.saved_attn\n    self.saved_ffn = self.feed_forward(self.ffn_norm(h))\n    out = h + self.saved_ffn\n    self.saved_output = out\n    return out\n# %%\nsetattr(llama.generator.model.layers[0].__class__, \"forward\", new_forward)\n# %%\nlines = \"\"\"\n1. The majestic giraffe, with its towering height and distinctive long neck, roams the savannas of Africa. These gentle giants use their elongated tongues to pluck leaves from the tallest trees, making them well-adapted to their environment. Their unique coat patterns, much like human fingerprints, are unique to each individual.\n2. Penguins, the tuxedoed birds of the Antarctic, are expert swimmers and divers. These flightless seabirds rely on their dense, waterproof feathers and streamlined bodies to propel through icy waters in search of fish, krill, and other marine life. Their huddled colonies and amusing waddles make them a favorite among wildlife enthusiasts.\n3. The mighty African elephant, the largest land mammal, is revered for its intelligence and strong family bonds. These gentle giants use their versatile trunks for various tasks, from drinking and feeding to communicating and greeting one another. Their massive ears and wrinkled skin make them an iconic symbol of the African wilderness.\n4. The colorful and flamboyant peacock, native to Asia, is known for its stunning iridescent plumage. During mating season, the males fan out their magnificent train of feathers, adorned with intricate eye-like patterns, in an elaborate courtship display to attract potential mates, making them a true spectacle of nature.\n5. The sleek and powerful cheetah, the fastest land animal, is built for speed and agility. With its distinctive black tear-like markings and slender body, this feline predator can reach top speeds of up to 70 mph during short bursts, allowing it to chase down its prey with remarkable precision.\n6. The playful and intelligent dolphin, a highly social marine mammal, is known for its friendly demeanor and impressive acrobatic abilities. These aquatic creatures use echolocation to navigate and hunt, and their complex communication systems have long fascinated researchers studying their intricate social structures and cognitive abilities.\n7. The majestic bald eagle, the national emblem of the United States, soars high above with its distinctive white head and tail feathers. These powerful raptors are skilled hunters, swooping down from great heights to catch fish and other prey with their sharp talons, making them an iconic symbol of strength and freedom.\n8. The industrious beaver, nature's skilled engineers, are known for their remarkable ability to construct dams and lodges using their sharp incisors and webbed feet. These semiaquatic rodents play a crucial role in shaping their aquatic ecosystems, creating habitats for numerous other species while demonstrating their ingenuity and perseverance.\n9. The vibrant and enchanting hummingbird, one of the smallest bird species, is a true marvel of nature. With their rapidly flapping wings and ability to hover in mid-air, these tiny feathered creatures are expert pollinators, flitting from flower to flower in search of nectar and playing a vital role in plant reproduction.\n10. The majestic polar bear, the apex predator of the Arctic, is perfectly adapted to its icy environment. With its thick insulating fur and specialized paws for gripping the ice, this powerful carnivore relies on its exceptional hunting skills and keen senses to locate and capture seals, its primary prey, in the harsh Arctic landscape.\n\"\"\"\n\n# %%\nllama.forward([lines])\n# %%\nattn_outputs, ffn_outputs, block_outputs = [], [], []\nfor i, blk in enumerate(llama.generator.model.layers):\n    attn_outputs.append(blk.saved_attn)\n    ffn_outputs.append(blk.saved_ffn)\n    block_outputs.append(blk.saved_output)\nattn_outputs = torch.stack(attn_outputs).float().detach().cpu()\nffn_outputs = torch.stack(ffn_outputs).float().detach().cpu()\nblock_outputs = torch.stack(block_outputs).float().detach().cpu()\nprint(attn_outputs.shape, ffn_outputs.shape, block_outputs.shape)\n# %%\nprompt_tokens = llama.generator.tokenizer.encode(lines, bos=True, eos=False)\ntoken_texts = [llama.generator.tokenizer.decode([x]) for x in prompt_tokens]\n# %%\n\n# %%\ntorch.save((attn_outputs, ffn_outputs, block_outputs, token_texts), \"/tmp/llama3_features.pt\")\nexit(0)\n\n# %%\nimport torch\nattn_outputs, ffn_outputs, block_outputs, token_texts = torch.load(\"/tmp/llama3_features.pt\")\nfrom ncut_pytorch import NCUT, rgb_from_tsne_3d\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\nimport numpy as np\n\ndef plot_one_ax(token_texts, rgb, ax, fig, title, num_lines=5):\n    # Define the colors\n    # fill nan with 0\n    rgb = np.nan_to_num(rgb)\n    colors = [mcolors.rgb2hex(rgb[i]) for i in range(len(token_texts))]\n\n    # Split the sentence into words\n    words = token_texts\n\n\n    y_pos = 0.9\n    x_pos = 0.0\n    max_word_length = max(len(word) for word in words)\n    count = 0\n    for word, color in zip(words, colors):\n        if word == '&lt;|begin_of_text|&gt;':\n            word = '&lt;SoT&gt;'\n            y_pos -= 0.05\n            x_pos = 0.0\n\n\n        text_color = 'black' if sum(mcolors.hex2color(color)) &gt; 1.3 else 'white'  # Choose text color based on background color\n        # text_color = 'black'\n        txt = ax.text(x_pos, y_pos, word, color=text_color, fontsize=12, bbox=dict(facecolor=color, alpha=0.8, edgecolor='none', pad=2))\n        txt_width = txt.get_window_extent().width / (fig.dpi * fig.get_size_inches()[0])  # Calculate the width of the text in inches\n\n        x_pos += txt_width * 1.1 + 0.01  # Adjust the spacing between words\n\n        if x_pos &gt; 0.97:\n            y_pos -= 0.2\n            x_pos = 0.0\n            count += 1\n            if count &gt;= num_lines:\n                break\n        # break\n\n    # Remove the axis ticks and spines\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n\n    ax.set_title(title, fontsize=14)\n\n\nfor i_layer in range(12):\n\n    attn_eig, _ = NCUT(num_eig=20).fit_transform(\n        attn_outputs[i_layer].reshape(-1, attn_outputs[i_layer].shape[-1])\n    )\n    _, attn_rgb = rgb_from_tsne_3d(attn_eig, seed=42)\n    mlp_eig, _ = NCUT(num_eig=20).fit_transform(\n        ffn_outputs[i_layer].reshape(-1, ffn_outputs[i_layer].shape[-1])\n    )\n    _, mlp_rgb = rgb_from_tsne_3d(mlp_eig, seed=42)\n    block_eig, _ = NCUT(num_eig=20).fit_transform(\n        block_outputs[i_layer].reshape(-1, block_outputs[i_layer].shape[-1])\n    )\n    _, block_rgb = rgb_from_tsne_3d(block_eig, seed=42)\n\n    fig, axs = plt.subplots(3, 1, figsize=(10, 5))\n    plot_one_ax(token_texts, attn_rgb.numpy(), axs[0], fig, \"attention layer output\")\n    plot_one_ax(token_texts, mlp_rgb.numpy(), axs[1], fig, \"MLP layer output\")\n    plot_one_ax(token_texts, block_rgb.numpy(), axs[2], fig, \"sum of residual stream\")\n\n    plt.suptitle(f\"Llama3 layer {i_layer} NCUT spectral-tSNE\", fontsize=16)\n    plt.tight_layout()\n    # plt.show()\n\n    save_dir = \"/workspace/output/gallery/llama3\"\n    import os\n    os.makedirs(save_dir, exist_ok=True)\n    plt.savefig(f\"{save_dir}/llama3_layer_{i_layer}.jpg\", bbox_inches=\"tight\")\n    plt.close()\n</code></pre>"},{"location":"gallery_mae/","title":"Gallery mae","text":"Click to expand full code  <pre><code>class MAE(torch.nn.Module):\n</code></pre> <pre><code># %%\nfrom einops import rearrange\nimport torch\nfrom PIL import Image\nimport torchvision.transforms as transforms\nfrom torch import nn\nimport numpy as np\n\nimport timm\n\n# %%\nclass MAE(timm.models.vision_transformer.VisionTransformer):\n    def __init__(self, **kwargs):\n        super(MAE, self).__init__(**kwargs)\n\n        sd = torch.hub.load_state_dict_from_url(\n            \"https://dl.fbaipublicfiles.com/mae/pretrain/mae_pretrain_vit_base.pth\"\n        )\n\n        checkpoint_model = sd[\"model\"]\n        state_dict = self.state_dict()\n        for k in [\"head.weight\", \"head.bias\"]:\n            if (\n                k in checkpoint_model\n                and checkpoint_model[k].shape != state_dict[k].shape\n            ):\n                print(f\"Removing key {k} from pretrained checkpoint\")\n                del checkpoint_model[k]\n\n        # load pre-trained model\n        msg = self.load_state_dict(checkpoint_model, strict=False)\n        print(msg)\n\n        self.requires_grad_(False)\n        self.eval()\n        self.cuda()\n\n        def forward(self, x):\n            self.saved_attn_node = self.ls1(self.attn(self.norm1(x)))\n            x = x + self.saved_attn_node.clone()\n            self.saved_mlp_node = self.ls2(self.mlp(self.norm2(x)))\n            x = x + self.saved_mlp_node.clone()\n            self.saved_block_output = x.clone()\n            return x\n\n        setattr(self.blocks[0].__class__, \"forward\", forward)\n\n    def forward(self, x):\n        out = super().forward(x)\n        attn_nodes = [block.saved_attn_node for block in self.blocks]\n        mlp_nodes = [block.saved_mlp_node for block in self.blocks]\n        block_outputs = [block.saved_block_output for block in self.blocks]\n        attn_nodes = torch.stack(attn_nodes, dim=0)\n        mlp_nodes = torch.stack(mlp_nodes, dim=0)\n        block_outputs = torch.stack(block_outputs, dim=0)\n        return attn_nodes, mlp_nodes, block_outputs\n\ndef image_mae_feature(\n    images, resolution=(224, 224),\n):\n    if isinstance(images, list):\n        assert isinstance(images[0], Image.Image), \"Input must be a list of PIL images.\"\n    else:\n        assert isinstance(images, Image.Image), \"Input must be a PIL image.\"\n        images = [images]\n\n    transform = transforms.Compose(\n        [\n            transforms.Resize(resolution),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n        ]\n    )\n\n    feat_extractor = MAE()\n\n    attn_outputs, mlp_outputs, block_outputs = [], [], []\n    for i, image in enumerate(images):\n        torch_image = transform(image)\n        # feat = feat_extractor(torch_image.unsqueeze(0).cuda()).cpu()\n        attn_output, mlp_output, block_output = feat_extractor(\n            torch_image.unsqueeze(0).cuda()\n        )\n        # feats.append(feat)\n        attn_outputs.append(attn_output.cpu())\n        mlp_outputs.append(mlp_output.cpu())\n        block_outputs.append(block_output.cpu())\n    attn_outputs = torch.cat(attn_outputs, dim=1)\n    mlp_outputs = torch.cat(mlp_outputs, dim=1)\n    block_outputs = torch.cat(block_outputs, dim=1)\n\n    # feats = torch.cat(feats, dim=1)\n    # feats = rearrange(feats, \"l b c h w -&gt; l b h w c\")\n    return attn_outputs, mlp_outputs, block_outputs\n\n\n# %%\nfrom torchvision.datasets import ImageFolder\n\ndataset = ImageFolder(\"/data/coco/\")\nprint(\"number of images in the dataset:\", len(dataset))\n# %%\nimages = [dataset[i][0] for i in range(20)]\nattn_outputs, mlp_outputs, block_outputs = image_mae_feature(images)\n# %%\nprint(attn_outputs.shape, mlp_outputs.shape, block_outputs.shape)\n# %%\n# remove 1 cls token\ndef reshape_output(outputs):\n    from einops import rearrange\n    outputs = rearrange(outputs[:, :, 1:, :], \"l b (h w) c -&gt; l b h w c\", h=14, w=14)\n    return outputs\nattn_outputs = reshape_output(attn_outputs)\nmlp_outputs = reshape_output(mlp_outputs)\nblock_outputs = reshape_output(block_outputs)\nprint(attn_outputs.shape, mlp_outputs.shape, block_outputs.shape)\n# %%\nnum_nodes = np.prod(attn_outputs.shape[1:4])\n\n\n# %%\nfrom ncut_pytorch import NCUT, rgb_from_tsne_3d\n\nfor i_layer in range(12):\n\n    attn_eig, _ = NCUT(num_eig=100, device=\"cuda:0\").fit_transform(\n        attn_outputs[i_layer].reshape(-1, attn_outputs[i_layer].shape[-1])\n    )\n    _, attn_rgb = rgb_from_tsne_3d(attn_eig, device=\"cuda:0\", seed=42)\n    attn_rgb = attn_rgb.reshape(attn_outputs[i_layer].shape[:3] + (3,))\n    mlp_eig, _ = NCUT(num_eig=100, device=\"cuda:0\").fit_transform(\n        mlp_outputs[i_layer].reshape(-1, mlp_outputs[i_layer].shape[-1])\n    )\n    _, mlp_rgb = rgb_from_tsne_3d(mlp_eig, device=\"cuda:0\", seed=42)\n    mlp_rgb = mlp_rgb.reshape(mlp_outputs[i_layer].shape[:3] + (3,))\n    block_eig, _ = NCUT(num_eig=100, device=\"cuda:0\").fit_transform(\n        block_outputs[i_layer].reshape(-1, block_outputs[i_layer].shape[-1])\n    )\n    _, block_rgb = rgb_from_tsne_3d(block_eig, device=\"cuda:0\", seed=42)\n    block_rgb = block_rgb.reshape(block_outputs[i_layer].shape[:3] + (3,))\n\n    from matplotlib import pyplot as plt\n\n    fig, axs = plt.subplots(4, 10, figsize=(10, 5))\n    for ax in axs.flatten():\n        ax.axis(\"off\")\n    for i_col in range(10):\n        axs[0, i_col].imshow(images[i_col])\n        axs[1, i_col].imshow(attn_rgb[i_col])\n        axs[2, i_col].imshow(mlp_rgb[i_col])\n        axs[3, i_col].imshow(block_rgb[i_col])\n\n    axs[1, 0].set_title(\"attention layer output\", ha=\"left\")\n    axs[2, 0].set_title(\"MLP layer output\", ha=\"left\")\n    axs[3, 0].set_title(\"sum of residual stream\", ha=\"left\")\n\n    plt.suptitle(f\"MAE layer {i_layer} NCUT spectral-tSNE\", fontsize=16)\n    # plt.show()\n    save_dir = \"/workspace/output/gallery/mae\"\n    import os\n    os.makedirs(save_dir, exist_ok=True)\n    plt.savefig(f\"{save_dir}/mae_layer_{i_layer}.jpg\", bbox_inches=\"tight\")\n    plt.close()\n\nexit(0)\n\n# %%\n</code></pre>"},{"location":"gallery_sam/","title":"Gallery sam","text":"Click to expand full code  <pre><code>class SAM(torch.nn.Module):\n</code></pre> <pre><code># %%\nfrom einops import rearrange\nimport torch\nfrom PIL import Image\nimport torchvision.transforms as transforms\nfrom torch import nn\nimport numpy as np\n\n\nclass SAM(torch.nn.Module):\n    def __init__(self, checkpoint=\"/data/sam_model/sam_vit_b_01ec64.pth\", **kwargs):\n        super().__init__(**kwargs)\n        from segment_anything import sam_model_registry, SamPredictor\n        from segment_anything.modeling.sam import Sam\n\n        sam: Sam = sam_model_registry[\"vit_b\"](checkpoint=checkpoint)\n\n        from segment_anything.modeling.image_encoder import (\n            window_partition,\n            window_unpartition,\n        )\n\n        def new_block_forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n            shortcut = x\n            x = self.norm1(x)\n            # Window partition\n            if self.window_size &gt; 0:\n                H, W = x.shape[1], x.shape[2]\n                x, pad_hw = window_partition(x, self.window_size)\n\n            x = self.attn(x)\n            # Reverse window partition\n            if self.window_size &gt; 0:\n                x = window_unpartition(x, self.window_size, pad_hw, (H, W))\n            self.attn_output = x.clone()\n\n            x = shortcut + x\n            mlp_outout = self.mlp(self.norm2(x))\n            self.mlp_output = mlp_outout.clone()\n            x = x + mlp_outout\n            self.block_output = x.clone()\n\n            return x\n\n        setattr(sam.image_encoder.blocks[0].__class__, \"forward\", new_block_forward)\n\n        self.image_encoder = sam.image_encoder\n        self.image_encoder.eval()\n        self.image_encoder = self.image_encoder.cuda()\n\n    @torch.no_grad()\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        with torch.no_grad():\n            x = torch.nn.functional.interpolate(x, size=(1024, 1024), mode=\"bilinear\")\n        out = self.image_encoder(x)\n\n        attn_outputs, mlp_outputs, block_outputs = [], [], []\n        for i, blk in enumerate(self.image_encoder.blocks):\n            attn_outputs.append(blk.attn_output)\n            mlp_outputs.append(blk.mlp_output)\n            block_outputs.append(blk.block_output)\n            # print(f\"block {i} attn_output shape: {blk.attn_output.shape}\")\n            # print(f\"block {i} mlp_output shape: {blk.mlp_output.shape}\")\n            # print(f\"block {i} block_output shape: {blk.block_output.shape}\")\n        attn_outputs = torch.stack(attn_outputs)\n        mlp_outputs = torch.stack(mlp_outputs)\n        block_outputs = torch.stack(block_outputs)\n        return attn_outputs, mlp_outputs, block_outputs\n\n\ndef image_sam_feature(\n    images, resolution=(1024, 1024), checkpoint=\"/data/sam_model/sam_vit_b_01ec64.pth\"\n):\n    if isinstance(images, list):\n        assert isinstance(images[0], Image.Image), \"Input must be a list of PIL images.\"\n    else:\n        assert isinstance(images, Image.Image), \"Input must be a PIL image.\"\n        images = [images]\n\n    transform = transforms.Compose(\n        [\n            transforms.Resize(resolution),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n        ]\n    )\n\n    feat_extractor = SAM(checkpoint=checkpoint)\n\n    attn_outputs, mlp_outputs, block_outputs = [], [], []\n    for i, image in enumerate(images):\n        torch_image = transform(image)\n        # feat = feat_extractor(torch_image.unsqueeze(0).cuda()).cpu()\n        attn_output, mlp_output, block_output = feat_extractor(\n            torch_image.unsqueeze(0).cuda()\n        )\n        # feats.append(feat)\n        attn_outputs.append(attn_output.cpu())\n        mlp_outputs.append(mlp_output.cpu())\n        block_outputs.append(block_output.cpu())\n    attn_outputs = torch.cat(attn_outputs, dim=1)\n    mlp_outputs = torch.cat(mlp_outputs, dim=1)\n    block_outputs = torch.cat(block_outputs, dim=1)\n\n    # feats = torch.cat(feats, dim=1)\n    # feats = rearrange(feats, \"l b c h w -&gt; l b h w c\")\n    return attn_outputs, mlp_outputs, block_outputs\n\n\n# %%\nfrom torchvision.datasets import ImageFolder\n\ndataset = ImageFolder(\"/data/coco/\")\nprint(\"number of images in the dataset:\", len(dataset))\n# %%\nimages = [dataset[i][0] for i in range(20)]\nattn_outputs, mlp_outputs, block_outputs = image_sam_feature(images)\n# %%\nprint(attn_outputs.shape, mlp_outputs.shape, block_outputs.shape)\n# %%\nnum_nodes = np.prod(attn_outputs.shape[1:4])\n\n\n# %%\nfrom ncut_pytorch import NCUT, rgb_from_tsne_3d\n\ni_layer = 9\n\nfor i_layer in range(12):\n\n    attn_eig, _ = NCUT(num_eig=100, device=\"cuda:0\").fit_transform(\n        attn_outputs[i_layer].reshape(-1, attn_outputs[i_layer].shape[-1])\n    )\n    _, attn_rgb = rgb_from_tsne_3d(attn_eig, device=\"cuda:0\")\n    attn_rgb = attn_rgb.reshape(attn_outputs[i_layer].shape[:3] + (3,))\n    mlp_eig, _ = NCUT(num_eig=100, device=\"cuda:0\").fit_transform(\n        mlp_outputs[i_layer].reshape(-1, mlp_outputs[i_layer].shape[-1])\n    )\n    _, mlp_rgb = rgb_from_tsne_3d(mlp_eig, device=\"cuda:0\")\n    mlp_rgb = mlp_rgb.reshape(mlp_outputs[i_layer].shape[:3] + (3,))\n    block_eig, _ = NCUT(num_eig=100, device=\"cuda:0\").fit_transform(\n        block_outputs[i_layer].reshape(-1, block_outputs[i_layer].shape[-1])\n    )\n    _, block_rgb = rgb_from_tsne_3d(block_eig, device=\"cuda:0\")\n    block_rgb = block_rgb.reshape(block_outputs[i_layer].shape[:3] + (3,))\n\n    from matplotlib import pyplot as plt\n\n    fig, axs = plt.subplots(4, 10, figsize=(10, 5))\n    for ax in axs.flatten():\n        ax.axis(\"off\")\n    for i_col in range(10):\n        axs[0, i_col].imshow(images[i_col])\n        axs[1, i_col].imshow(attn_rgb[i_col])\n        axs[2, i_col].imshow(mlp_rgb[i_col])\n        axs[3, i_col].imshow(block_rgb[i_col])\n\n    axs[1, 0].set_title(\"attention layer output\", ha=\"left\")\n    axs[2, 0].set_title(\"MLP layer output\", ha=\"left\")\n    axs[3, 0].set_title(\"sum of residual stream\", ha=\"left\")\n\n    plt.suptitle(f\"SAM layer {i_layer} NCUT spectral-tSNE\", fontsize=16)\n    # plt.show()\n    save_dir = \"/workspace/output/gallery/sam\"\n    import os\n    os.makedirs(save_dir, exist_ok=True)\n    plt.savefig(f\"{save_dir}/sam_layer_{i_layer}.jpg\", bbox_inches=\"tight\")\n    plt.close()\n\nexit(0)\n# %%\n</code></pre>"},{"location":"gallery_sam2_video/","title":"Gallery sam2 video","text":"Click to expand full code  <pre><code>class SAM2(torch.nn.Module):\n</code></pre> <pre><code># %%\nimport os\nfrom einops import repeat, rearrange\nimport numpy as np\nimport torch\nfrom PIL import Image\nimport requests\nimport torch\nfrom PIL import Image\nimport torchvision.transforms as transforms\nfrom torch import nn\n\n\nN_FRAMES = 600\n\n\nclass SAM2(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n\n        from sam2.build_sam import build_sam2\n        from sam2.sam2_image_predictor import SAM2ImagePredictor\n\n        sam2_checkpoint = \"/data/sam_model/sam2_hiera_large.pt\"\n        # model_cfg = \"/data/sam_model/sam2_hiera_b+.yaml\"\n        model_cfg = \"sam2_hiera_l\"\n\n        device = 'cuda:0'\n        sam2_model = build_sam2(model_cfg, sam2_checkpoint, device=device)\n\n        image_encoder = sam2_model.image_encoder\n        image_encoder.eval()\n\n        self.image_encoder = image_encoder\n\n    @torch.no_grad()\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        output = self.image_encoder(x)\n        features = output['vision_features']\n        features = rearrange(features, 'b c h w -&gt; b h w c')\n        return features\n\n# %%\n\n\ndef transform_images(frames, size=(1024, 1024)):\n    resized = []\n    length = len(frames)\n    for i in range(length):\n        frame = frames[i]\n        # image = Image.fromarray((frame * 255).astype(np.uint8))\n        image = Image.fromarray(frame)\n        image = image.resize(size, Image.LANCZOS)\n        image = np.array(image) / 255.0\n        resized.append(np.array(image))\n    frames = np.stack(resized, axis=0)\n    frames = frames.transpose(0, 3, 1, 2)  # (N, H, W, C) -&gt; (N, C, H, W)\n    frames = torch.tensor(frames, dtype=torch.float32)\n    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\n    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n    frames = (frames - mean) / std\n    return frames\n\n\ndef read_video(video_path: str) -&gt; torch.Tensor:\n    try:\n        from decord import VideoReader\n    except ImportError:\n        raise ImportError(\"Please install the decord library: pip install decord\")\n\n    vr = VideoReader(video_path)\n    print(f\"Total frames: {len(vr)}\")\n    # frames = vr.get_batch(range(len(vr))).asnumpy()\n    lenth = len(vr)\n    lenth = N_FRAMES if lenth &gt; N_FRAMES else lenth\n    frames = vr.get_batch(np.arange(lenth)).asnumpy()\n    # if less than N_FRAMES frames, repeat the last frame\n    if lenth &lt; N_FRAMES:\n        last_frame = frames[-1]\n        for i in range(N_FRAMES - lenth):\n            frames = np.append(frames, last_frame.reshape(1, *last_frame.shape), axis=0)\n    return frames\n\n\n@torch.no_grad()\ndef video_sam_feature(video_path, checkpoint=\"/data/sam_model/sam_vit_b_01ec64.pth\"):\n    frames = read_video(video_path)\n\n    feat_extractor = SAM2()\n\n    features = []\n    for i in range(frames.shape[0]):\n        frame = frames[i]\n        frame = transform_images([frame])\n        feature = feat_extractor(frame.cuda())\n        feature = torch.nn.functional.normalize(feature, dim=-1)\n        features.append(feature.cpu())\n    features = torch.cat(features, dim=0)\n    return features\n\n\n# %%\nvideo_paths = [\n    \"/workspace/ego4d_dog_264.mp4\",\n    \"/workspace/ego4d_tractor_264.mp4\",\n    \"/workspace/ego4d_drum_264.mp4\",\n]\n# %%\nfeatures = []\nfor video_path in video_paths:\n    features.append(video_sam_feature(video_path))\nfeatures = torch.cat(features, dim=0)\n# %%\nprint(features.shape)\n# %%\nnum_nodes = np.prod(features.shape[:-1])\nprint(\"Number of nodes:\", num_nodes)\n\n# %%\nfrom ncut_pytorch import NCUT, rgb_from_tsne_3d\n\neigvectors, eigenvalues = NCUT(\n    num_eig=100, num_sample=30000, device=\"cuda:0\", normalize_features=False,\n).fit_transform(features.reshape(-1, features.shape[-1]))\n# %%\n_, rgb = rgb_from_tsne_3d(eigvectors, num_sample=50000, perplexity=500, knn=10, device=\"cuda:0\")\n\n\n# %%\nimage_rgb = rgb.reshape(*features.shape[:-1], 3)\n\nimport matplotlib.pyplot as plt\n\n\nframes1 = read_video(video_paths[0])\nframes2 = read_video(video_paths[1])\nframes3 = read_video(video_paths[2])\n\nsave_dir = \"/tmp/ncut_video_sam2/\"\nimport shutil\nshutil.rmtree(save_dir, ignore_errors=True)\nimport os\nos.makedirs(save_dir, exist_ok=True)\n\ndef resize_image(image, size=(540, 540)):\n    image = Image.fromarray(image)\n    image = image.resize(size, Image.LANCZOS)\n    image = np.array(image)\n    return image\n\nfor i_frame in range(0, N_FRAMES):\n    fig, axes = plt.subplots(2, 3, figsize=(10, 7))\n    for ax in axes.flatten():\n        ax.axis(\"off\")\n\n    offsets = [0, N_FRAMES, 2 * N_FRAMES]\n    for i, frames in enumerate([frames1, frames2, frames3]):\n        axes[0, i].imshow(resize_image(frames[i_frame]))\n        offset = offsets[i]\n        np_image = image_rgb[i_frame+offset].cpu().numpy()\n        np_image = (np_image * 255).astype(np.uint8)\n        axes[1, i].imshow(resize_image(np_image))\n\n    plt.tight_layout()\n    plt.savefig(f\"{save_dir}/frame_{i_frame:04d}.png\")\n    # plt.show()\n    plt.close()\n\n# %%\n# make video\nsave_dir = \"/nfscc/tmp/ncut_video_sam2/\"\n\ndef make_video_from_images(image_dir, video_path):\n    import cv2\n    import os\n\n    images = sorted(os.listdir(image_dir))\n    frame = cv2.imread(os.path.join(image_dir, images[0]))\n    height, width, layers = frame.shape\n\n    video = cv2.VideoWriter(video_path, cv2.VideoWriter_fourcc(*\"mp4v\"), 30, (width, height))\n\n    for image in images:\n        video.write(cv2.imread(os.path.join(image_dir, image)))\n\n    cv2.destroyAllWindows()\n    video.release()\n\nmake_video_from_images(save_dir, \"/workspace/output/ncut_video_sam2.mp4\")\n# %%\n</code></pre> <p>This video example use image model without temporal dimension</p> <ol> <li> <p>extract image feature for every frame, independently</p> </li> <li> <p>concatenate all the image features and compute NCUT</p> </li> </ol>"},{"location":"gallery_sam_video/","title":"Gallery sam video","text":"Click to expand full code  <pre><code>class SAM(torch.nn.Module):\n</code></pre> <pre><code># %%\nfrom einops import rearrange\nimport torch\nfrom PIL import Image\nimport torchvision.transforms as transforms\nfrom torch import nn\nimport numpy as np\n\nN_FRAMES = 600\n\n\ndef transform_images(frames, size=(1024, 1024)):\n    resized = []\n    length = len(frames)\n    for i in range(length):\n        frame = frames[i]\n        # image = Image.fromarray((frame * 255).astype(np.uint8))\n        image = Image.fromarray(frame)\n        image = image.resize(size, Image.Resampling.LANCZOS)\n        image = np.array(image) / 255.0\n        resized.append(np.array(image))\n    frames = np.stack(resized, axis=0)\n    frames = frames.transpose(0, 3, 1, 2)  # (N, H, W, C) -&gt; (N, C, H, W)\n    frames = torch.tensor(frames, dtype=torch.float32)\n    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\n    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n    frames = (frames - mean) / std\n    return frames\n\n\ndef read_video(video_path: str) -&gt; torch.Tensor:\n    try:\n        from decord import VideoReader\n    except ImportError:\n        raise ImportError(\"Please install the decord library: pip install decord\")\n\n    vr = VideoReader(video_path)\n    print(f\"Total frames: {len(vr)}\")\n    # frames = vr.get_batch(range(len(vr))).asnumpy()\n    lenth = len(vr)\n    lenth = N_FRAMES if lenth &gt; N_FRAMES else lenth\n    frames = vr.get_batch(np.arange(lenth)).asnumpy()\n    # if less than N_FRAMES frames, repeat the last frame\n    if lenth &lt; N_FRAMES:\n        last_frame = frames[-1]\n        for i in range(N_FRAMES - lenth):\n            frames = np.append(frames, last_frame.reshape(1, *last_frame.shape), axis=0)\n    return frames\n\nfrom ncut_pytorch.backbone import load_model\n\n\n@torch.no_grad()\ndef video_sam_feature(video_path):\n    frames = read_video(video_path)\n\n    feat_extractor = load_model('SAM(sam_vit_b)')\n    feat_extractor = feat_extractor.eval().cuda()\n\n    block_outputs = []\n    for i in range(frames.shape[0]):\n        frame = frames[i]\n        frame = transform_images([frame])\n        block_output = feat_extractor(frame.cuda())['block']\n        block_outputs.append(block_output[-1].cpu())\n    block_outputs = torch.stack(block_outputs)\n    return block_outputs\n\n\n# %%\nvideo_paths = [\n    \"/workspace/ego4d_dog_264.mp4\",\n    \"/workspace/ego4d_tractor_264.mp4\",\n    \"/workspace/ego4d_drum_264.mp4\",\n]\n# %%\nfeatures = []\nfor video_path in video_paths:\n    features.append(video_sam_feature(video_path))\nfeatures = torch.cat(features, dim=0)\n# %%\nprint(features.shape)\n# %%\nfeatures = features.squeeze(1)\n# %%\nnum_nodes = np.prod(features.shape[:-1])\nprint(\"Number of nodes:\", num_nodes)\n\n# %%\nfrom ncut_pytorch import NCUT, rgb_from_tsne_3d\n\neigvectors, eigenvalues = NCUT(\n    num_eig=100, num_sample=10000, device=\"cuda:0\"\n).fit_transform(features.reshape(-1, features.shape[-1]))\n# %%\n_, rgb = rgb_from_tsne_3d(eigvectors, num_sample=1000, perplexity=500, knn=10, device=\"cuda:0\")\n\n\n# %%\nimage_rgb = rgb.reshape(*features.shape[:-1], 3).squeeze(1)\n\nimport matplotlib.pyplot as plt\n\n\nframes1 = read_video(video_paths[0])\nframes2 = read_video(video_paths[1])\nframes3 = read_video(video_paths[2])\n\nsave_dir = \"/tmp/ncut_video_sam/\"\nimport shutil\nshutil.rmtree(save_dir, ignore_errors=True)\nimport os\nos.makedirs(save_dir, exist_ok=True)\n\ndef resize_image(image, size=(540, 540)):\n    image = Image.fromarray(image)\n    image = image.resize(size, Image.ANTIALIAS)\n    image = np.array(image)\n    return image\n\nfor i_frame in range(0, N_FRAMES):\n    fig, axes = plt.subplots(2, 3, figsize=(10, 7))\n    for ax in axes.flatten():\n        ax.axis(\"off\")\n\n    offsets = [0, N_FRAMES, 2 * N_FRAMES]\n    for i, frames in enumerate([frames1, frames2, frames3]):\n        axes[0, i].imshow(resize_image(frames[i_frame]))\n        offset = offsets[i]\n        np_image = image_rgb[i_frame+offset].cpu().numpy()\n        np_image = (np_image * 255).astype(np.uint8)\n        axes[1, i].imshow(resize_image(np_image))\n\n    plt.tight_layout()\n    plt.savefig(f\"{save_dir}/frame_{i_frame:04d}.png\")\n    # plt.show()\n    plt.close()\n\n# %%\n# make video\nsave_dir = \"/tmp/ncut_video_sam/\"\n\ndef make_video_from_images(image_dir, video_path):\n    import cv2\n    import os\n\n    images = sorted(os.listdir(image_dir))\n    frame = cv2.imread(os.path.join(image_dir, images[0]))\n    height, width, layers = frame.shape\n\n    video = cv2.VideoWriter(video_path, cv2.VideoWriter_fourcc(*\"mp4v\"), 30, (width, height))\n\n    for image in images:\n        video.write(cv2.imread(os.path.join(image_dir, image)))\n\n    cv2.destroyAllWindows()\n    video.release()\n\nmake_video_from_images(save_dir, \"/workspace/output/ncut_video_sam.mp4\")\n# %%\n</code></pre> <p>This video example use features from Segment Anything Model:</p> <ol> <li> <p>for every frame, extract image features, shape <code>[B, T, H, W, C]</code></p> </li> <li> <p>concatenate all the frames and pixels, shape <code>[B*T*H*W, C]</code></p> </li> <li> <p>compute 100 NCUT eigenvectors, shape <code>[B*T*H*W, 100]</code></p> </li> <li> <p>use spectral-tSNE to reduce 100 eigenvectors to 3D, shape <code>[B*T*H*W, 3]</code></p> </li> <li> <p>plot the 3D as RGB</p> </li> </ol>"},{"location":"gradient_of_ncut/","title":"Gradient of NCUT","text":""},{"location":"gradient_of_ncut/#accessing-the-gradient-of-ncut-with-functional-api","title":"Accessing the Gradient of NCUT with Functional API","text":"<p>In our PyTorch implementation of NCUT, gradient is handled by PyTorch autograd.</p> <p>This example use NCUT with Nystrom approximation, and access gradient of eigenvectors.</p> <pre><code>from ncut_pytorch import NCUT\nimport torch\n\nfeatures = torch.randn(10000, 768)\nfeatures.requires_grad = True\neigvectors, eigvalues = NCUT(num_eig=50, num_sample=1000).fit_transform(features)\nloss = eigvectors.sum()\nloss.backward()\ngrad = features.grad\nprint(grad.shape)\n# torch.Size([10000, 768])\n</code></pre> <p>This example use NCUT without Nystrom approximation, and access gradient of eigenvectors.</p> <pre><code>from ncut_pytorch import ncut, affinity_from_features  # use functional API\nimport torch\n\nfeatures = torch.randn(100, 768)\nfeatures.requires_grad = True\naffinity = affinity_from_features(features)\neigvectors, eigvalues = ncut(affinity, num_eig=50)\nloss = eigvectors.sum()\nloss.backward()\ngrad = features.grad\nprint(grad.shape)\n# torch.Size([100, 768])\n</code></pre>"},{"location":"how_ncut_works/","title":"How NCUT works","text":"<p> PROCEDURE How NCUT Works</p> <p> 1. Feature Extraction: extract feature for every image frame or text token. 2. NCUT: compute k NCUT eigenvectors, each node is a latent-pixel or text token.  3. spectral-tSNE: reduce k eigenvectors to 3D, plot as RGB image.   </p> <p>This Guide covers:</p> <ul> <li> <p>How NCUT works, in math and coding example</p> </li> <li> <p>Scale up NCUT with Nystrom approximation</p> </li> <li> <p>spectral-tSNE: Plot NCUT eigenvectors as RGB image</p> </li> </ul>"},{"location":"how_ncut_works/#normalized-cuts-and-spectral-clustering","title":"Normalized Cuts and Spectral Clustering","text":"<p>Spectral clustering, or Normalized Cuts, clusters data based on the eigenvectors (spectrum) of a similarity matrix derived from the data. The Normalized Cuts algorithm aims to partition a graph into subgraphs while minimizing the graph cut value.</p> <p>Image taken from Spectral Clustering: Step-by-step derivation of the spectral clustering algorithm including an implementation in Python</p>"},{"location":"how_ncut_works/#the-basic-idea","title":"The Basic Idea","text":"<p>Spectral clustering works by embedding the data points \\(F \\in \\mathbb{R}^{N \\times 768}\\) into a lower-dimensional space using the eigenvectors of a Laplacian matrix derived from the data's similarity graph \\(W \\in \\mathbb{R}^{N \\times N}\\). The data is then clustered in this new space embedded by \\(k\\) eigenvectors \\(\\mathbf{x} \\in \\mathbb{R}^{N \\times k}\\).</p>"},{"location":"how_ncut_works/#the-graph-laplacian","title":"The Graph Laplacian","text":"<p>Given a set of data points, spectral clustering first constructs a similarity graph. Each node represents a data point, and edges represent the similarity between data points. The similarity graph can be represented by an adjacency matrix \\( W \\), where each element \\( W_{ij} \\) represents the similarity between data points \\( i \\) and \\( j \\).</p> <p>Take cosine distance for example, let \\(F \\in \\mathbb{R}^{N \\times 768}\\) be the input features (from a backbone image model), \\(N\\) is number of pixels, \\(768\\) is feature dimension. Feature vectors \\(f_i, f_j \\in \\mathbb{R}^{768}\\), the cosine similarity between \\(f_i\\) and \\(f_j\\) is defined as:</p> \\[W_{ij} = \\text{cosine}(f_i, f_j) = \\frac{f_i \\cdot f_j}{|f_i| |f_j|}\\] <p>where \\(|f|\\) denotes the Euclidean norm of a vector \\(f\\).</p> <p>In matrix form, this can be written as:</p> \\[W = \\text{cosine}(F, F) = \\frac{F F^\\top}{\\text{diag}(F F^\\top)^{1/2} , \\text{diag}(F F^\\top)^{1/2}}\\] <p>where \\(F F^\\top \\in \\mathbb{R}^{N \\times N}\\) is the matrix of pairwise dot products between the feature vectors, and \\(\\text{diag}(\\cdot)\\) extracts the diagonal elements of a square matrix. The resulting matrix \\(W = \\text{cosine}(F, F)\\) is an \\(\\mathbb{R}^{N \\times N}\\) matrix, where each element \\((i, j)\\) represents the cosine similarity between the \\(i\\)-th and \\(j\\)-th feature vectors.</p> <p>The degree matrix \\( D \\) is a diagonal matrix where each element \\( D_{ii} \\) is the sum of the similarities of node \\( i \\) to all other nodes.</p> \\[ D_{ii} = \\sum_{j=0}^{N} W_{ij} \\] <p>The unnormalized graph Laplacian is:</p> \\[ L = D - W \\] <p>The normalized graph Laplacian has two common forms:</p> <ol> <li>Symmetric normalized Laplacian:</li> </ol> \\[ L_{\\text{sym}} = D^{-1/2} L D^{-1/2} = I - D^{-1/2} W D^{-1/2} \\] <ol> <li>Random walk normalized Laplacian:</li> </ol> \\[ L_{\\text{rw}} = D^{-1} L = I - D^{-1} W \\]"},{"location":"how_ncut_works/#normalized-cuts","title":"Normalized Cuts","text":"<p>Normalized Cuts (Ncut) is a method for partitioning a graph into disjoint subsets, aiming to minimize the total edge weight between the subsets relative to the total edge weight within each subset. The Ncut criterion is particularly useful for ensuring balanced partitioning, which prevents trivial solutions where one cluster might be significantly smaller than the other.</p> <p>The normalized cut criterion is defined as:</p> \\[ \\text{Ncut}(A, B) = \\left(\\frac{\\text{cut}(A, B)}{\\text{assoc}(A, V)}\\right) + \\left(\\frac{\\text{cut}(A, B)}{\\text{assoc}(B, V)}\\right) \\] <p>Where:</p> <ul> <li> <p>\\( A \\) and \\( B \\) are two disjoint subsets (clusters) of the graph.</p> </li> <li> <p>\\( \\text{cut}(A, B) \\) is the sum of the weights of the edges between the subsets \\( A \\) and \\( B \\):</p> </li> </ul> \\[ \\text{cut}(A, B) = \\sum_{i \\in A, j \\in B} W_{ij} \\] <ul> <li>\\( \\text{assoc}(A, V) \\) is the sum of the weights of all edges attached to nodes in subset \\( A \\) (similar definition for \\( B \\)):</li> </ul> \\[ \\text{assoc}(A, V) = \\sum_{i \\in A, j \\in V} W_{ij} \\] <p>The goal is to find a partition that minimizes the Ncut value, which balances minimizing the inter-cluster connection (cut) with maintaining strong intra-cluster connections (association).</p>"},{"location":"how_ncut_works/#solving-the-normalized-cut-using-eigenvectors","title":"Solving the Normalized Cut Using Eigenvectors","text":"<p>To solve the Ncut problem, we reformulate it as a problem of finding the eigenvectors of a normalized graph Laplacian. Here\u2019s how this works:</p>"},{"location":"how_ncut_works/#mathematical-formulation","title":"Mathematical Formulation","text":"<p>Let \\( \\mathbf{x} \\) be an indicator vector such that:</p> \\[ x_i =  \\begin{cases}  1 &amp; \\text{if node } i \\in A \\\\ -1 &amp; \\text{if node } i \\in B  \\end{cases} \\] <p>The cut value \\( \\text{cut}(A, B) \\) can be rewritten in terms of \\( \\mathbf{x} \\) as:</p> \\[ \\text{cut}(A, B) = \\frac{1}{4} \\sum_{i,j} W_{ij} (x_i - x_j)^2 = \\mathbf{x}^\\top L \\mathbf{x} \\] <p>Where \\( L = D - W \\) is the unnormalized graph Laplacian.</p> <p>The association \\( \\text{assoc}(A, V) \\) is given by:</p> \\[ \\text{assoc}(A, V) = \\mathbf{x}^\\top D \\mathbf{1}_A \\] <p>Here, \\( \\mathbf{1}_A \\) is a vector that is 1 for elements in \\( A \\) and 0 elsewhere.</p> <p>The Ncut problem can thus be rewritten as:</p> \\[ \\text{Ncut}(A, B) = \\frac{\\mathbf{x}^\\top L \\mathbf{x}}{\\mathbf{x}^\\top D \\mathbf{x}} \\] <p>Minimizing this directly is NP-hard. However, it can be relaxed into a generalized eigenvalue problem, because the above eq is close to the Rayleigh quotient which is minimized by solving eigenvectors. By relaxing the constraint that \\( x_i \\) takes only discrete values (1 or -1), we allow \\( \\mathbf{x} \\) to take real values, and the problem becomes finding the eigenvector corresponding to the second smallest eigenvalue of the generalized eigenvalue problem:</p> \\[ L \\mathbf{y} = \\lambda D \\mathbf{y} \\] <p>To make it a simple eigenvalue solving, that can be solved by most eigenvector solvers.  Move \\(D\\) to the left side of the equation. this is equivalent to finding the eigenvectors of the normalized Laplacian:</p> \\[ D^{-1/2} L D^{-1/2} \\mathbf{y} = \\lambda \\mathbf{y} \\]"},{"location":"how_ncut_works/#proof-of-minimization","title":"Proof of Minimization","text":"<p>The eigenvector approach is derived from the relaxation of the original NP-hard problem. By solving the generalized eigenvalue problem:</p> \\[ L \\mathbf{y} = \\lambda D \\mathbf{y} \\] <p>We are effectively minimizing the Rayleigh quotient:</p> \\[ \\frac{\\mathbf{y}^\\top L \\mathbf{y}}{\\mathbf{y}^\\top D \\mathbf{y}} = \\frac{\\mathbf{y}^\\top D^{-1/2} L D^{-1/2} \\mathbf{y}}{\\mathbf{y}^\\top  \\mathbf{y}} \\] <p>The Rayleigh quotient directly reflecting the Ncut objective. The solution of Rayleigh quotient is eigenvectors</p> \\[ \\text{Ncut}(A, B) = \\frac{\\mathbf{x}^\\top L \\mathbf{x}}{\\mathbf{x}^\\top D \\mathbf{x}} = \\frac{\\mathbf{x}^\\top D^{-1/2} L D^{-1/2} \\mathbf{x}}{\\mathbf{x}^\\top \\mathbf{x}} \\] <p>thus, solving eigenvector \\(\\mathbf{y}\\) of \\(D^{-1/2} L D^{-1/2} = \\lambda \\mathbf{y}\\) is equal to solving the graph cut solution \\(\\mathbf{x}\\):</p> \\[      \\mathbf{x} = \\mathbf{y}  \\]"},{"location":"how_ncut_works/#laplacian-vs-affinity","title":"Laplacian vs Affinity","text":"<p>Normalized Cuts can also be solved by eigenvectors of \\(D^{-1/2} W D^{-1/2} \\), instead of \\(D^{-1/2} L D^{-1/2} \\), where \\(L = D - W\\). They have the same eigenvectors:</p> \\[\\begin{aligned} D^{-1/2} (D - W) D^{-1/2} &amp;= Y \\Lambda Y^T \\\\ I - D^{-1/2} W D^{-1/2} &amp;= Y \\Lambda Y^T \\\\ D^{-1/2} W D^{-1/2} &amp;= Y (I - \\Lambda) Y^T \\end{aligned}\\]"},{"location":"how_ncut_works/#eigenvectors-and-clustering","title":"Eigenvectors and Clustering","text":"<p>The eigenvector corresponding to the second smallest eigenvalue (also called the Fiedler vector) provides a real-valued solution to the relaxed Ncut problem. Sorting the nodes based on the values in this eigenvector and choosing a threshold to split them yields an approximate solution to the Ncut problem.</p> <p>Each subsequent eigenvector can be used to further partition the graph into subclusters. The process is as follows:</p> <ol> <li>Compute the symmetric normalized Laplacian \\( L_{\\text{sym}} \\).</li> <li>Compute the eigenvectors \\( \\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_k \\) corresponding to the smallest \\( k \\) eigenvalues.</li> <li>Use \\( \\mathbf{x}_2 \\) (the Fiedler vector) to divide the graph into two clusters.</li> <li>For multi-way clustering, use the next eigenvectors \\( \\mathbf{x}_3, \\mathbf{x}_4, \\dots \\) to further hierarchically subdivide these clusters.</li> </ol> <p>Video: Heatmap is cosine distance of eigenvectors, w.r.t the mouse pixel (blue point). Reduce `n_eig` hierarchical grow the object heatmap try it at \ud83e\udd17HuggingFace Demo (switch to tab \"PlayGround\")"},{"location":"how_ncut_works/#example-eigenvector-visualization","title":"Example: Eigenvector Visualization","text":"<p>Let's visualize how the eigenvectors divide the graph into clusters using our toy example.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\nfrom scipy.linalg import eigh\nfrom sklearn.metrics.pairwise import rbf_kernel\nfrom sklearn.cluster import KMeans\n\n# Generate synthetic data with 4 clusters\nX, y = make_blobs(n_samples=400, centers=4, cluster_std=0.60, random_state=0)\n\n# Compute the similarity matrix using RBF (Gaussian) kernel\nsigma = 1.0\nW = rbf_kernel(X, gamma=1/(2*sigma**2))\n\n# Degree matrix\nD = np.diag(np.sum(W, axis=1))\n\n# Unnormalized Laplacian\nL = D - W\n\n# Normalized Laplacian (symmetric)\nD_inv_sqrt = np.linalg.inv(np.sqrt(D))\nL_sym = np.dot(np.dot(D_inv_sqrt, L), D_inv_sqrt)\n\n# Compute the eigenvalues and eigenvectors of the normalized Laplacian\neigvals, eigvecs = eigh(L_sym)\n\n# Plot the first few eigenvectors\nk = 4  # number of clusters\nfor i in range(1, k+1):\n    plt.figure()\n    plt.scatter(X[:, 0], X[:, 1], c=eigvecs[:, i-1], s=50, cmap='coolwarm')\n    plt.title(f\"Eigenvector {i} - Dividing the Graph\")\n    plt.colorbar()\n    plt.show()\n\n# Use the first k eigenvectors to form a matrix U\nU = eigvecs[:, :k]\n\n# Normalize U\nU_norm = U / np.linalg.norm(U, axis=1, keepdims=True)\n\n# Perform k-means clustering on U_norm\nkmeans = KMeans(n_clusters=k, random_state=0)\nlabels = kmeans.fit_predict(U_norm)\n\n# Plot the clusters\nplt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='viridis')\nplt.title(\"Spectral Clustering: Clusters Found\")\nplt.show()\n</code></pre> <ul> <li> <p>The 1-st eigenvector generally corresponds to the global structure (often related to the connected components of the graph).</p> </li> <li> <p>The 2-nd eigenvector splits the graph into 2 clusters.</p> </li> <li> <p>Subsequent eigenvectors further hierarchically splits the data into more detailed sub-clusters.</p> </li> </ul>"},{"location":"how_ncut_works/#a-good-eigenvector-solver","title":"A Good Eigenvector Solver","text":"<p>Solving the full eigenvector \\(\\mathbf{x} \\in \\mathbb{R}^{N \\times N}\\) is computational expensive, methods has been developed to solve top k eigenvectors \\(\\mathbf{x} \\in \\mathbb{R}^{N \\times k}\\) in linearly complexity scaling. In particular, we use svd_lowrank.</p> <ul> <li> <p>Reference: </p> <p>Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding structure with randomness: probabilistic algorithms for constructing approximate matrix decompositions, 2009.</p> </li> </ul>"},{"location":"how_ncut_works/#summary","title":"Summary","text":"<p>The Normalized Cuts algorithm aims to partition a graph into subgraphs while minimizing the graph cut value. It embeds the data points into a lower-dimensional space using the eigenvectors of a Laplacian matrix derived from the data's similarity graph.</p> <p>Eigenvectors are:</p> <p>1) soft-cluster assignments, eigenvector in [-1, 1], -1 or 1 are extreme assignments, 0 means assigned to neither cluster. </p> <p>2) \\(n\\) eigenvectors can hierarchically divide into \\(2^n\\) sub-graphs.</p>"},{"location":"how_ncut_works/#nystrom-like-approximation","title":"Nystrom-like Approximation","text":"<p>The Nystrom-like approximation is our new technique designed to address the computational challenges of solving large-scale graph cuts. </p> <p>An intuition: Nystrom approximation solve the large-scale problem on a small sub-sampled set, then propagate the solution from the small set to the large set.</p> <p>The approach involves three main steps: (1) sub-sampling a subset of nodes, (2) computing the Normalized Cut (Ncut) on the sampled nodes, and (3) propagating the eigenvectors from the sampled nodes to the unsampled nodes.</p>"},{"location":"how_ncut_works/#sub-sampling-the-affinity-matrix","title":"Sub-sampling the Affinity Matrix","text":"<p>The affinity matrix \\( W \\) is partitioned as follows:</p> \\[ W = \\begin{pmatrix} A &amp; B \\\\ B^\\top &amp; C \\end{pmatrix} \\] <p>where:</p> <ul> <li> <p>\\( A \\in \\mathbb{R}^{n \\times n} \\) is the submatrix of weights among the sampled nodes,</p> </li> <li> <p>\\( B \\in \\mathbb{R}^{(N-n) \\times n} \\) is the weights between the sampled nodes and the remaining unsampled nodes,</p> </li> <li> <p>\\( C \\in \\mathbb{R}^{(N-n) \\times (N-n)} \\) is the weights between the unsampled nodes.</p> </li> </ul> <p>Computation loads: \\( C \\) is a very large matrix that is computationally expensive to handle. \\( B \\) is also expensive to store and compute once sampled nodes \\( n \\) gets large, although \\( n \\) (the number of sampled nodes) is much smaller than \\( N \\) (the total number of nodes), a better quality of  Nystrom approximation requires \\( n \\) to be larger, thus makes \\( B \\) a large matrix.</p>"},{"location":"how_ncut_works/#a-good-sampling-strategy-fps","title":"A Good Sampling Strategy: FPS","text":"<p>An effective sampling strategy is crucial for obtaining a good approximation of the Ncut. Farthest Point Sampling (FPS) is used because it ensures that the sampled nodes are well-distributed across the graph.  We use a tree-based QuickFPS algorithm developed in </p> <ul> <li> <p>Reference: </p> <p>QuickFPS: Architecture and Algorithm Co-Design for Farthest Point Sampling in Large-Scale Point Cloud, Han, Meng and Wang, Liang and Xiao, Limin and Zhang, Hao and Zhang, Chenhao and Xu, Xiangrong and Zhu, Jianfeng, 2023</p> </li> </ul> <p>As a side note for speed, for high-dimensional input model features \\(F \\in \\mathbb{R}^{N \\times 768}\\) where \\(768\\) is feature dimension, FPS can be computationally expensive for large \\(768\\). To mitigate this, Principal Component Analysis (PCA) is used to reduce the dimensionality of the features to \\(F' \\in \\mathbb{R}^{N \\times 8}\\). PCA is only applied in the FPS sampling step, but not to the affinity graph \\(W\\).</p>"},{"location":"how_ncut_works/#accounting-for-indirect-connections","title":"Accounting for Indirect Connections","text":"<p>Unsampled nodes can act as bridges between clusters, and ignoring these connections could lead to inaccurate cluster assignments. To address this, the original Nystrom method [2] approximates the eigenvectors by solving the following matrix \\( S \\):</p> \\[ S = A + A^{-1/2} B B^\\top A^{-1/2} \\] <p>Here, the term \\( A^{-1/2} B B^\\top A^{-1/2} \\in \\mathbb{R}^{n \\times n} \\) accounts for indirect connections between the sampled nodes by considering the influence of the unsampled nodes.</p> <p>In our method, we solve for eigenvectors on the matrix \\( S \\) given by:</p> \\[ S = A + \\left({D_{\\text{r}}}^{-1} B\\right) \\left(B {D_{\\text{c}}}^{-1}\\right)^\\top \\] <p>where \\( D_{\\text{r}} \\) and \\( D_{\\text{c}} \\) are the row and column sums of matrix \\( B \\), respectively. </p> <p>the term \\( \\left({D_{\\text{r}}}^{-1} B\\right) \\left(B {D_{\\text{c}}}^{-1}\\right)^\\top \\in \\mathbb{R}^{n \\times n}\\) is the indirect random walk probabilities.</p>"},{"location":"how_ncut_works/#example-case-for-indirect-connections","title":"Example Case for Indirect Connections","text":"<p>This example shows when indirect connection is important. In the bottom example, the connected components of the graph is altered, thus will lead to incorrect approximation. After adding indirect connection, the graph become correctly connected.</p>"},{"location":"how_ncut_works/#reduce-computation-loads-for-indirect-connections","title":"Reduce Computation Loads for Indirect Connections","text":"<p>Given that \\( B \\) has dimensions \\( (N-n) \\times n \\), \\(N\\) is large, directly storing and computing with \\( B \\) can be prohibitively expensive. To overcome this, we reduce the number of unsampled nodes by applying PCA. If the feature size is \\( 768 \\), let \\( F_A \\in \\mathbb{R}^{n \\times 768} \\) be the feature matrix for the sampled nodes, and \\( F_B \\in \\mathbb{R}^{(N-n) \\times 768} \\) be the feature matrix for the unsampled nodes. After applying PCA, \\(m\\) is the PCA-ed dimension, the reduced feature matrix \\( F_B' \\in \\mathbb{R}^{m \\times 768} \\) represents the unsampled nodes, where \\( m \\ll (N-n) \\). Thus, \\( B' = d(F_A, F_{B'}) \\) becomes a matrix of size \\( \\mathbb{R}^{m \\times n} \\), significantly reduce computation loads and memory usage.</p>"},{"location":"how_ncut_works/#k-nearest-neighbors-knn-propagation","title":"K-Nearest Neighbors (KNN) Propagation","text":"<p>After computing the eigenvectors \\( \\mathbf{X}' \\in \\mathbb{R}^{n \\times C} \\) on the sub-sampled graph \\( S = A + \\left({D_{\\text{r}}}^{-1} B'\\right) \\left(B' {D_{\\text{c}}}^{-1}\\right)^\\top \\in \\mathbb{R}^{n \\times n} \\) using the Ncut formulation, the next step is to propagate these eigenvectors to the full graph. Let \\( \\mathbf{\\tilde{X}} \\in \\mathbb{R}^{N \\times C} \\) be the approximated eigenvectors for the full graph, where \\( N \\) is the total number of nodes. The eigenvector approximation \\( \\mathbf{\\tilde{X}}_i \\) for each node \\( i \\leq N \\) in the full graph is obtained by averaging the eigenvectors \\( \\mathbf{X}'_k \\) of the top K-nearest neighbors from the subgraph:</p> \\[ \\mathcal{K}_i = \\text{KNN}(\\mathbf{A}_{*i}; n, K) = argmax_{k \\leq n} \\sum_{k=1}^{K} \\mathbf{B}_{ki} \\] \\[ \\mathbf{\\tilde{X}}_i = \\frac{1}{\\sum_{k \\in \\mathcal{K}_i} \\mathbf{B}_{ki}} \\sum_{k \\in \\mathcal{K}_i} \\mathbf{B}_{ki} \\mathbf{X}'_k  \\] <p>Here, \\( \\text{KNN}(\\mathbf{A}_{*i}; n, K) \\) denotes the set of K-nearest neighbors from the full-graph node \\( i \\leq N \\) to the sub-graph nodes \\( k \\leq n \\). In other words, the eigenvectors of un-sampled nodes are assigned by weighted averaging top KNN eigenvectors from sampled nodes.</p> <p>The propagation step is only when \\(B \\in \\mathbb{R}^{n \\times (N-n)} \\) is computed (but not stored), recall that the full \\( B \\) is too large to store, our implementation divide the propagation into chunks, where each \\(q\\) size chunk \\(B_i \\in \\mathbb{R}^{n \\times (q)} \\) is feasible to store, each chunk \\(B_i\\) is discarded after the propagation. In practice, the KNN propagation is the major speed bottle-neck for NCUT on large-scale graph, but KNN propagation is easy to parallelize on GPU. </p> <pre><code># GPU speeds up the KNN propagation\neigvectors, eigvalues = NCUT(num_eig=100, device='cuda:0').fit_transform(data)\n</code></pre>"},{"location":"how_ncut_works/#visualizing-ncut-eigenvectors","title":"Visualizing NCUT Eigenvectors","text":"<p>The input image features \\( F \\in \\mathbb{R}^{N \\times 768} \\) are embedded into eigenvectors \\( \\mathbf{x} \\in \\mathbb{R}^{N \\times k} \\), where \\( N \\) is the number of pixels, \\( 768 \\) is the feature dimension, and \\( k \\) is the number of eigenvectors. We use t-SNE or UMAP on the eigenvectors for visualization. </p> <p>3D t-SNE is computed on eigenvectors \\( \\mathbf{x} \\in \\mathbb{R}^{N \\times k} \\), resulting in a color matrix \\( \\mathbb{R}^{N \\times 3} \\). RGB values are assigned to each pixel based on its 3D t-SNE embedding.</p> <p>Alternatively, in the old days before t-SNE comes out, K-means clustering are applied to the eigenvectors. Compares t-SNE with K-means, t-SNE has two advantages: (1) continuous color gradients instead of discrete clusters, (2) t-SNE color reflects distances (e.g., distance between purple vs. red &lt; green vs. red). K-means colors are arbitrary.</p>"},{"location":"how_ncut_works/#nystrom-approximation-for-t-sne-and-umap","title":"Nystrom Approximation for t-SNE and UMAP","text":"<p>t-SNE and UMAP can be slow for large-scale graphs. We applied the same Nystrom-like approximation: (1) FPS sub-samples a subset of nodes, (2) t-SNE is computed on the sampled nodes, and (3) KNN propagates the colors from sampled to unsampled nodes.</p> <p>For extra speed-up, we found FPS sub-sampling (euclidean) on eigenvectors \\( \\mathbf{x} \\in \\mathbb{R}^{N \\times k} \\) more effective than on the original features \\( F \\in \\mathbb{R}^{N \\times 768} \\), because the distribution of eigenvectors. Thus, the Nystrom approximation for t-SNE requires fewer samples to perform well (we recommend 300 samples for t-SNE, 10,000 for NCUT). Note that small sampling works well for t-SNE only on eigenvectors but not on the original features.  t-SNE with small sampling is fast on CPU, then the expensive KNN propagation step computes in GPU.</p> <pre><code>X_3d, rgb = rgb_from_tsne_3d(eigvectors, num_samples=300, device='cuda:0')\n</code></pre> <p>paper in prep, Yang 2024</p> <p>[1] AlignedCut: Visual Concepts Discovery on Brain-Guided Universal Feature Space, Huzheng Yang, James Gee*, Jianbo Shi*, 2024</p> <p>[2] Spectral Grouping Using the Nystrom Method, Charless Fowlkes, Serge Belongie, Fan Chung, and Jitendra Malik, 2004</p> <p>[3] Normalized Cuts and Image Segmentation, Jianbo Shi and Jitendra Malik, 2000 </p>"},{"location":"how_to_get_better_segmentation/","title":"How to Get Better Segmentation from NCUT","text":""},{"location":"how_to_get_better_segmentation/#pick-number-of-eigenvectors","title":"Pick Number of Eigenvectors","text":"<p>More eigenvectors give more details on the segmentation, less eigenvectors is more robust.</p> <p>In NCUT, by math design (see How NCUT Works), i-th eigenvector give a optimal graph cut that divides the graph into 2^(i-1) clusters. E.g. <code>eigenvectors[:, 1]</code> divides the graph into 2 clusters, <code>eigenvectors[:, 1:4]</code> divides the graph into 8 clusters.</p> <p>To answer the question ``How many eigenvectors do I need?'', one need to consider the complexity of the graph (how many images, what's the backbone model), and the goal (e.g., whole-body or body parts). </p> <p>A general rule-of-thumb is:</p> <ol> <li> <p>Less eigenvectors for robust results, more eigenvectors can be noisy</p> </li> <li> <p>More eigenvectors for larger set of images, less eigenvectors for fewer images</p> </li> <li> <p>More eigenvectors for objects parts, less eigenvectors for whole-object</p> </li> <li> <p>Try recursive NCUT for large set of images</p> </li> </ol> <p>Here's an example grid search of how many eigenvectors to include:</p>"},{"location":"how_to_get_better_segmentation/#clean-up-the-affinity-matrix","title":"Clean up the Affinity Matrix","text":"<p>When an graph has noisy connections, e.g. random weak connections across any pair of nodes, one could apply thresholding to get rid of this type of noise.</p> <p>The <code>0 &lt; affinity_focal_gamma &lt;= 1</code> parameter is controlling the smooth thresholding, each element <code>A_ij</code> in affinity is transformed as:</p> <pre><code>A_ij = 1 - cos_similarity(node_i, node_j)\nA_ij = exp(-(A_ij / affinity_focal_gamma))\n</code></pre> <p>In practice, it's recommended to reduce the value of <code>affinity_focal_gamma</code> (default 0.3) until the segmentation is shredded too much. </p>"},{"location":"how_to_get_better_segmentation/#increase-sampling-size","title":"Increase Sampling Size","text":"<p>It's recommended to use as large sample size as it fits into memory (default <code>num_sample=3000</code>).</p> <p>Nystrom approximation made it possible to compute on large-scale graph (see How NCUT Works). A decent sampling size of Nystrom approximation is critical for a good approximation.  In general, as the graph gets larger, the sampling size need to be increased. In fact, the increased need for larger sampling size is due to the increased complexity of the graph but not more number of nodes. E.g., 100 images of different cats will be more complex than 100 views of the same cats, hence larger sample size is recommended. Thank's to svd_lowrank, the time complexity of NCUT scales linearly w.r.t. sample size, the bottleneck is memory scales quadratically. </p> <p>On a 16GB GPU, <code>num_sample=30000</code> fits into memory. On a 12GB GPU, <code>num_sample=20000</code> fits into memory.</p> <pre><code>\neigvecs, eigvals = NCUT(num_samples=30000).fit_transform(inp)\n    </code></pre>"},{"location":"how_to_get_better_segmentation/#t-sne-and-umap-parameters","title":"t-SNE and UMAP parameters","text":"<p>For visualization purpose, t-SNE or UMAP is applied on the NCUT eigenvectors. Our Nystrom approximation is also applied to t-SNE/UMAP. It's possible to in crease the quality of t-SNE/UMAP by tweaking the parameters:</p> <ul> <li> <p>increase the sample size of Nystrom approximation, <code>num_samples</code></p> </li> <li> <p>increase <code>perplexity</code> for t-SNE, <code>n_neighbors</code> for UMAP</p> </li> </ul> <pre><code>\n# fast, for small-scale\nX_3d, rgb = rgb_from_tsne_3d(eigvecs, num_samples=300, perplexity=150)\nX_3d, rgb = rgb_from_umap_3d(eigvecs, num_samples=300, n_neighbors=150, min_dist=0.1)\n\n# balanced speed and quality, for medium-scale\nX_3d, rgb = rgb_from_tsne_3d(eigvecs, num_samples=1000, perplexity=250)\nX_3d, rgb = rgb_from_umap_3d(eigvecs, num_samples=1000, n_neighbors=250, min_dist=0.1)\n\n# extreme quality, much slower\nX_3d, rgb = rgb_from_tsne_3d(eigvecs, num_samples=10000, perplexity=500)\nX_3d, rgb = rgb_from_umap_3d(eigvecs, num_samples=10000, n_neighbors=500, min_dist=0.1)\n    </code></pre>"},{"location":"how_to_get_better_segmentation/#rotate-the-rgb-cube","title":"Rotate the RGB Cube","text":"<p>Human perception is not uniform on the RGB color space -- green vs. yellow is less perceptually different than red vs. blue. Therefore, it's a good idea to rotate the RGB cube and try a different color. In the following example, all images has the same euclidean distance matrix, but perceptually they could tell different story. Please see Coloring for the code to rotate RGB cube.</p> <p>If a perceptually uniform colormap is preferred, unfortunately we don't find a perfect perceptually uniform colormap in 3D, but there are great perceptually uniform colormaps in 2D (see pycolormap-2d). However, the downside is that 2D colormap don't use the full color space, and 2D t-SNE/UMAP can be underwhelming.</p> <p>Please see Tutorial - Coloring for a full comparison of coloring methods:</p> Pros Cons t-SNE(3D) make fuller use of the color space slow for large samples UMAP(3D) fast for large samples holes in the color space; slow for small samples UMAP(sphere) can be plotted in 2D&amp;3D do not use the full color space t-SNE(2D) can be plotted in 2D, use perceptually uniform colormap do not use the full color space UMAP(2D) can be plotted in 2D, use perceptually uniform colormap do not use the full color space"},{"location":"how_to_get_better_segmentation/#recursive-ncut","title":"Recursive NCUT","text":"<p>NCUT can be applied recursively, the eigenvectors from previous iteration is the input for the next iteration NCUT. </p> <pre><code>\n# Type A: cosine, more amplification\neigenvectors1, eigenvalues1 = NCUT(num_eig=100, affinity_focal_gamma=0.3).fit_transform(input_feats)\neigenvectors2, eigenvalues2 = NCUT(num_eig=50, affinity_focal_gamma=0.3, distance='cosine', normalize_features=True).fit_transform(eigenvectors1)\neigenvectors3, eigenvalues3 = NCUT(num_eig=20, affinity_focal_gamma=0.3, distance='cosine', normalize_features=True).fit_transform(eigenvectors2)\n\n# Type B: euclidean, less amplification, match t-SNE\neigenvectors1, eigenvalues1 = NCUT(num_eig=100, affinity_focal_gamma=0.3).fit_transform(input_feats)\neigenvectors2, eigenvalues2 = NCUT(num_eig=50, affinity_focal_gamma=0.3, distance='euclidean', normalize_features=False).fit_transform(eigenvectors1)\n    </code></pre> <p>Recursive NCUT amplifies small object parts because:</p> <ol> <li>Eigenvectors with small eigenvalues are amplified -- we didn't take eigenvalue but only took eigenvectors in each iteration. This could be beneficial if we picked a good eigenvector number, if too much eigenvectors are picked, the noise will gets amplified. Less or no recursion is better for whole-object segmentation.</li> <li>Affinity is amplified in each iteration. When used recursive NCUT with <code>affinity_focal_gamma &lt; 1</code>, each iteration will clean up the affinity graph and amplify the strong connections.</li> </ol> <p>Recursive NCUT with affinity_focal_gamma=0.5 </p> <p>Recursive NCUT with affinity_focal_gamma=1.0 </p>"},{"location":"how_to_get_better_segmentation/#play-ground","title":"Play Ground","text":"<p>Please visit our \ud83e\udd17HuggingFace Demo. Play around models and parameters.</p> <p> </p> <p></p>"},{"location":"local_demo/","title":"Local demo","text":"<p>This demo is hosted at UPenn, password is:      158.130.50.41     Copied! open demo in new tab</p>"},{"location":"memory_usage/","title":"How to Optimize Memory Usage","text":"<p>In the case of GPU memory runs out, please consider the following:</p>"},{"location":"memory_usage/#lower-the-sampling-size","title":"Lower the Sampling Size","text":"<p>The sub-sampled affinity graph \\(A \\in \\mathbb{R}^{n \\times n}\\) is stored and computed on GPU (see How NCUT Works), \\(n\\) is the <code>num_samples</code> parameter. \\(A\\) could get large and consume a lot of GPU memory. It's recommended to use as large sampling size as it fits into memory. Please consider lowering the <code>num_samples</code> parameter if out of memory.</p> <pre><code>eigvectors, eigvalues = NCUT(num_eig=20, num_samples=10000).fit_transform(data)\n</code></pre>"},{"location":"memory_usage/#use-cpu-to-store-gpu-to-compute","title":"Use CPU to Store, GPU to Compute","text":"<p>For a simple and easy usage of NCUT, the input can be on neither cpu and gpu. To save more GPU memory, please save the input at CPU memory, and use <code>NCUT(device='cuda:0')</code>, NCUT will move the critical computation from CPU to GPU.</p> <pre><code>data = torch.rand(1000, 256, device='cpu')\neigvectors, eigvalues = NCUT(num_eig=20, device='cuda:0').fit_transform(data)\n</code></pre> <p>Computation that moved to GPU (see How NCUT Works): </p> <ol> <li> <p>cosine similarity to compute the sub-sampled graph affinity</p> </li> <li> <p>eigen-decomposition on the sub-sampled graph</p> </li> <li> <p>KNN propagation, each chunk is moved to GPU sequentially</p> </li> </ol> <p>Computation that stays in CPU:</p> <ol> <li> <p>normalization of input features (optional, fast)</p> </li> <li> <p>Farthest Point Sampling (QuickFPS, fast)</p> </li> <li> <p>Post-hoc orthogonalization of eigenvectors (optional, slow) </p> </li> </ol>"},{"location":"mixing_data/","title":"Tutorial 4 - Mixing Data","text":""},{"location":"mixing_data/#tutorial-mixing-images-of-different-resolution","title":"Tutorial: Mixing Images of Different Resolution","text":"<p>Mixing different resolution is straight-forward when using NCUT, just flatten the nodes and concatenate them.</p>"},{"location":"mixing_data/#feature-extraction-from-two-resolutions","title":"Feature Extraction from Two Resolutions","text":"Click to expand full code  <pre><code>dataset = torchvision.datasets.VOCSegmentation(...)\ndef feature_extractor(...):  # DiNOv2\n</code></pre> <pre><code>import torchvision\nimport torch\nfrom torchvision import transforms\nfrom PIL import Image\nimport numpy as np\n\ndataset_voc = torchvision.datasets.VOCSegmentation(\n    \"/data/pascal_voc/\",\n    year=\"2012\",\n    download=True,\n    image_set=\"val\",\n)\nprint(\"number of images in the dataset:\", len(dataset_voc))\n\nfrom einops import rearrange\nimport torch\nfrom PIL import Image\nimport torchvision.transforms as transforms\nfrom torch import nn\nimport numpy as np\n\n\ndef feature_extractor(images, resolution=(448, 448), layer=11):\n    if isinstance(images, list):\n        assert isinstance(images[0], Image.Image), \"Input must be a list of PIL images.\"\n    else:\n        assert isinstance(images, Image.Image), \"Input must be a PIL image.\"\n        images = [images]\n\n    transform = transforms.Compose(\n        [\n            transforms.Resize(resolution),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n        ]\n    )\n\n    # extract DINOv2 last layer features from the image\n    class DiNOv2Feature(torch.nn.Module):\n        def __init__(self, ver=\"dinov2_vitb14_reg\", layer=11):\n            super().__init__()\n            self.dinov2 = torch.hub.load(\"facebookresearch/dinov2\", ver)\n            self.dinov2.requires_grad_(False)\n            self.dinov2.eval()\n            self.dinov2 = self.dinov2.cuda()\n            self.layer = layer\n\n        def forward(self, x):\n            out = self.dinov2.get_intermediate_layers(x, reshape=True, n=np.arange(12))[\n                self.layer\n            ]\n            return out\n\n    feat_extractor = DiNOv2Feature(layer=layer)\n\n    feats = []\n    for i, image in enumerate(images):\n        torch_image = transform(image)\n        feat = feat_extractor(torch_image.unsqueeze(0).cuda()).cpu()\n        feat = feat.squeeze(0).permute(1, 2, 0)\n        feats.append(feat)\n    feats = torch.stack(feats).squeeze(0)\n    return feats\n</code></pre> <pre><code>images = [dataset_voc[i][0] for i in range(20)]\nfeats1 = feature_extractor(images, resolution=(224, 224), layer=9)\nnum_nodes1 = np.prod(feats1.shape[:3])\n\nfeats2 = feature_extractor(images, resolution=(448, 448), layer=9)\nnum_nodes2 = np.prod(feats2.shape[:3])\nmixed_feats = torch.cat(\n    [feats1.reshape(-1, feats1.shape[-1]), feats2.reshape(-1, feats2.shape[-1])], dim=0\n)\nprint(\"Mixed feature shape:\", mixed_feats.shape)\nprint(\"224x224 feature shape:\", feats1.shape, 'num_nodes:', num_nodes1)\nprint(\"448x448 feature shape:\", feats2.shape, 'num_nodes:', num_nodes2)\n\n# Mixed feature shape: torch.Size([25600, 768])\n# 224x224 feature shape: torch.Size([20, 16, 16, 768]) num_nodes: 5120\n# 448x448 feature shape: torch.Size([20, 32, 32, 768]) num_nodes: 20480\n</code></pre>"},{"location":"mixing_data/#compute-ncut-and-t-sne-coloring","title":"Compute NCUT and t-SNE coloring","text":"<pre><code>from ncut_pytorch import NCUT, rgb_from_tsne_3d\n\neigenvectors, eigenvalues = NCUT(\n    num_eig=50, num_sample=30000, knn=10, affinity_focal_gamma=0.5, device=\"cuda:0\"\n).fit_transform(mixed_feats)\n\nX_3d, rgb = rgb_from_tsne_3d(eigenvectors)\n</code></pre>"},{"location":"mixing_data/#plotting","title":"Plotting","text":"<pre><code>import matplotlib.pyplot as plt\n\nrgb1 = rgb[:num_nodes1].reshape(*feats1.shape[:3], 3)\nrgb2 = rgb[num_nodes1:].reshape(*feats2.shape[:3], 3)\nfig, axs = plt.subplots(3, 8, figsize=(10, 4))\nfor ax in axs.flatten():\n    ax.axis(\"off\")\nfor i in range(8):\n    axs[0, i].imshow(images[i])\n    axs[1, i].imshow(rgb1[i].cpu().numpy())\n    axs[2, i].imshow(rgb2[i].cpu().numpy())\naxs[1, 0].set_title(\"224x224 input\")\naxs[2, 0].set_title(\"448x448 input\")\nplt.suptitle(\"Mixed resolution input\", fontsize=16)\nplt.tight_layout()\nplt.show()\n</code></pre> The complete code for this tutorial <p>huzeyann/ncut_pytorch</p> <p></p>"},{"location":"parameters/","title":"NCUT Parameters","text":"<p>Please visit our \ud83e\udd17HuggingFace Demo. Play around models and parameters.</p> <p> </p> <p></p>"},{"location":"parameters/#num_eig","title":"num_eig","text":"<p>number of eigenvectors </p> <p>More eigenvectors give more details on the segmentation, less eigenvectors is more robust.</p> <p>In NCUT, by math design (see How NCUT Works), i-th eigenvector give a optimal graph cut that divides the graph into 2^(i-1) clusters. E.g. <code>eigenvectors[:, 1]</code> divides the graph into 2 clusters, <code>eigenvectors[:, 1:4]</code> divides the graph into 8 clusters.</p> <p>To answer the ultimate question ``How many eigenvectors should one use?'', one need to consider the complexity of the graph (how many images, what's the backbone model), and the goal (e.g., whole body vs body parts). Here's an example grid search of how many eigenvectors to include:</p>"},{"location":"parameters/#knn","title":"knn","text":"<p>propagation smoothness: higher knn means smoother propagation, higher knn value will give smoother eigenvector outputs.</p> <p>In Nystrom NCUT, KNN is used to propagate nystrom sampled nodes to not-sampled nodes, each not-sampled nodes only take the top K nearest neighbor to propagate eigenvectors.</p>"},{"location":"parameters/#affinity_focal_gamma","title":"affinity_focal_gamma","text":"<p>sharpness for affinity, range is (0, 1]. Each element A_ij in affinity is transformed as </p> <pre><code>A_ij = 1 - cos_similarity(node_i, node_j)\nA_ij = exp(-(A_ij / affinity_focal_gamma))\n</code></pre> <p>lower <code>affinity_focal_gamma</code> means sharper affinity. </p> <p>This transform is inspired by focal loss. In graph cut methods, disconnected edges are more important than connected edges, adding edge between two already connected clusters will not greatly alter the resulting eigenvectors, however, adding edge between two disconnected clusters will greatly alter the eigenvectors. Lower the <code>affinity_focal_gamma</code> value will make less-connected edges fade away, the resulting eigenvector will be sharper.</p>"},{"location":"parameters/#num_samples","title":"num_samples","text":"<p>Nystrom approximation sample size (see How NCUT Works). A decent sampling size of Nystrom approximation is critical for a good approximation. Complexity scaling is linear.</p>"},{"location":"speed_and_performance/","title":"Speed and Performance","text":"<p>Our new Nystrom NCUT time and space complexity scales linearly! w.r.t number of nodes. </p> N images (64x64 pixels per image) 1 10 100 1,000 10,000 GPU (RTX4090) time 0.1 sec 0.1 sec 0.4 sec 3.8 sec 38.9 sec CPU (i9-13900K) time 0.3 sec 0.7 sec 3.5 sec 42.3 sec 426.8 sec <pre><code>from ncut_pytorch import NCUT\nimport time\nimport torch\n\nn = 4096\n\nfor device in ['cuda:0', 'cpu']:\n    for n_data in [1*n, 10*n, 100*n, 1000*n, 10000*n]:\n\n        input_feats = torch.rand(int(n_data), 3)\n        input_feats = torch.nn.functional.normalize(input_feats, dim=1)\n\n        start = time.time()\n        eigenvectors, eigenvalues = NCUT(\n            num_eig=50,\n            num_sample=10000,\n            knn=10,\n            device=device,\n            make_orthogonal=False,\n            normalize_features=False,\n        ).fit_transform(input_feats)\n        end = time.time()\n        print(device, n_data, \"Nystr\u00f6m ncut time: {:.1f}\".format(end - start))\n</code></pre> <p>The limitation is that the quality of Nystrom approximation will degrades as sample ratio goes low, therefore we use FPS sampling for better quality (see How NCUT Works).</p>"},{"location":"speed_and_performance/#speed-up-tricks","title":"Speed-up Tricks","text":"<ul> <li>Start with <code>num_sample=10000</code> for NCUT, <code>num_sample=300</code> for t-SNE. For larger number and diverse set of images, if the quality gets low, please consider increase t-SNE <code>num_sample=1000</code> and increase NCUT <code>num_sample=25000</code>.</li> </ul> <pre><code>\n# fast, for small-scale\nX_3d, rgb = rgb_from_tsne_3d(eigvecs, num_samples=300, perplexity=150)\nX_3d, rgb = rgb_from_umap_3d(eigvecs, num_samples=300, n_neighbors=150, min_dist=0.1)\n\n# balanced speed and quality, for medium-scale\nX_3d, rgb = rgb_from_tsne_3d(eigvecs, num_samples=1000, perplexity=250)\nX_3d, rgb = rgb_from_umap_3d(eigvecs, num_samples=1000, n_neighbors=250, min_dist=0.1)\n\n# extreme quality, much slower\nX_3d, rgb = rgb_from_tsne_3d(eigvecs, num_samples=10000, perplexity=500)\nX_3d, rgb = rgb_from_umap_3d(eigvecs, num_samples=10000, n_neighbors=500, min_dist=0.1)\n    </code></pre> <ul> <li>decrease <code>num_sample</code> will linearly speed up NCUT and quadratically speed up t-SNE.</li> </ul>"},{"location":"trouble_shooting/","title":"Install Trouble Shooting","text":"<p><code>pip install ncut-pytorch</code></p>"},{"location":"trouble_shooting/#error-installing-from-pip","title":"Error installing from <code>pip</code>","text":"<p><code>fpsample</code> is a dependency for <code>ncut-pytorch</code>, if you running into the issue like this image, please try the following options.</p> <p>Option A:</p> <pre><code>sudo apt-get update &amp;&amp; sudo apt-get install build-essential cargo rustc -y</code></pre> <p>Option B:</p> <pre><code>conda install rust -c conda-forge</code></pre> <p>Option C:</p> <pre><code>curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh &amp;&amp; . \"$HOME/.cargo/env\"</code></pre> <p>Finally, if you still run into other errors when installing <code>fpsample</code>, please follow their instruction and build from source: https://github.com/leonardodalinky/fpsample</p>"},{"location":"tutorials/","title":"NCUT Tutorial","text":""},{"location":"tutorials/#tutorial-single-image","title":"Tutorial: Single Image","text":""},{"location":"tutorials/#feature-extraction","title":"Feature extraction","text":"Click to expand full code  <pre><code>def feature_extractor(images, resolution=(448, 448), layer=11):\n    ...\n    return feat  # (B, H, W, D)\n</code></pre> <pre><code>from einops import rearrange\nimport torch\nfrom PIL import Image\nimport torchvision.transforms as transforms\nfrom torch import nn\n\n\ndef feature_extractor(images, resolution=(448, 448), layer=11):\n\n    if not isinstance(images, list):\n        images = [images]\n\n    transform = transforms.Compose(\n        [\n            transforms.Resize(resolution),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n        ]\n    )\n\n    # extract DINOv2 last layer features from the image\n    class DiNOv2Feature(torch.nn.Module):\n        def __init__(self, ver=\"dinov2_vitb14_reg\", layer=11):\n            super().__init__()\n            self.dinov2 = torch.hub.load(\"facebookresearch/dinov2\", ver)\n            self.dinov2.requires_grad_(False)\n            self.dinov2.eval()\n            self.dinov2 = self.dinov2.cuda()\n            self.layer = layer\n\n        def forward(self, x):\n            out = self.dinov2.get_intermediate_layers(x, reshape=True, n=np.arange(12))[self.layer]\n            return out\n\n    feat_extractor = DiNOv2Feature(layer=layer)\n\n    feats = []\n    for i, image in enumerate(images):\n        torch_image = transform(image)\n        feat = feat_extractor(torch_image.unsqueeze(0).cuda()).cpu()\n        feat = feat.squeeze(0).permute(1, 2, 0)\n        feats.append(feat)\n    feats = torch.stack(feats)\n    return feats  # (B, H, W, D) \n</code></pre> <pre><code>import requests\nfrom PIL import Image\n\nurl = \"https://huzeyann.github.io/assets/img/prof_pic_old.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\nimage\n</code></pre> <pre><code>feat = feature_extractor(image, resolution=(448, 448), layer=9)\nfeat = feat.squeeze(0)\nprint(feat.shape)\n# (32, 32, 768)\n</code></pre>"},{"location":"tutorials/#compute-ncut","title":"Compute NCUT","text":"<pre><code>from ncut_pytorch import NCUT\n\nh, w, c = feat.shape  # (32, 32, 768)\nmodel = NCUT(num_eig=20)\neigenvectors, eigenvalues = model.fit_transform(feat.flatten(0, 1))\nprint(\"Eigenvectors shape:\", eigenvectors.shape)\n# Eigenvectors shape: torch.Size([1024, 20])\n</code></pre>"},{"location":"tutorials/#plotting-basic","title":"Plotting: Basic","text":"Click to expand full code  <pre><code>import matplotlib.pyplot as plt\n\nfig, axs = plt.subplots(3, 4, figsize=(13, 10))\n...\nplt.show()\n</code></pre> <pre><code># visualize top 9 eigenvectors, 3 eigenvectors per row\nimport matplotlib.pyplot as plt\nfrom ncut_pytorch import quantile_normalize\nfig, axs = plt.subplots(3, 4, figsize=(13, 10))\ni_eig = 0\nfor i_row in range(3):\n    for i_col in range(1, 4):\n        ax = axs[i_row, i_col]\n        ax.imshow(eigenvectors[:, i_eig].reshape(h, w), cmap=\"coolwarm\", vmin=-0.1, vmax=0.1)\n        ax.set_title(f\"lambda_{i_eig} = {eigenvalues[i_eig]:.3f}\")\n        ax.axis(\"off\")\n        i_eig += 1\nfor i_row in range(3):\n    ax = axs[i_row, 0]\n    start, end = i_row * 3, (i_row + 1) * 3\n    rgb = quantile_normalize(eigenvectors[:, start:end]).reshape(h, w, 3)\n    ax.imshow(rgb)\n    ax.set_title(f\"eigenvectors {start}-{end-1}\")\n    ax.axis(\"off\")\nplt.suptitle(\"Top 9 eigenvectors of Ncut DiNOv2 last layer features\")\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"tutorials/#plotting-advanced","title":"Plotting: Advanced","text":"Click to expand full code <pre><code>def plot_3d(X_3d, rgb, title):\n    ...\n</code></pre> <pre><code>import matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\ndef plot_3d(X_3d, rgb, title):\n    x, y, z = X_3d.T\n    fig = plt.figure(figsize=(10, 5))\n\n    # Add a subplot for the static image\n    ax1 = fig.add_subplot(121)\n    ax1.imshow(rgb.reshape(h, w, 3))\n    ax1.axis('off')  # Hide axes\n\n    # Add a subplot for the 3D scatter plot\n    ax = fig.add_subplot(122, projection='3d')\n    scat = ax.scatter(x, y, z, c=rgb, s=10)\n\n    # set ticks labels\n    ax.set_xlabel(\"Dimension #1\")\n    ax.set_ylabel(\"Dimension #2\")\n    ax.set_zlabel(\"Dimension #3\")\n\n    # set ticks, labels to none\n    x_ticks = ax.get_xticks()\n    y_ticks = ax.get_yticks()\n    z_ticks = ax.get_zticks()\n    labels = [\"\" for _ in range(len(x_ticks))]\n    ax.set_xticklabels(labels)\n    ax.set_yticklabels(labels)\n    ax.set_zticklabels(labels)\n\n    plt.suptitle(title)\n    plt.show()\n</code></pre> <pre><code>from ncut_pytorch import rgb_from_tsne_3d\n\nX_3d, rgb = rgb_from_tsne_3d(eigenvectors[:, :10], device=\"cpu\", perplexity=100)\nplot_3d(X_3d, rgb, \"spectral-tSNE of top 10 Ncut eigenvectors\")\n</code></pre> <pre><code>from ncut_pytorch import rgb_from_umap_sphere\n\nX_3d, rgb = rgb_from_umap_sphere(eigenvectors[:, :10], device=\"cpu\", n_neighbors=100, min_dist=0.1)\nplot_3d(X_3d, rgb, \"spectral-UMAP of top 10 Ncut eigenvectors\")\n</code></pre> The complete code for this tutorial <p>huzeyann/ncut_pytorch</p> <p></p>"},{"location":"tutorials/#tutorial-multiple-images","title":"Tutorial: Multiple ImagesThe complete code for this tutorial","text":"<p>Feature extraction</p>  Click to expand full code  <pre><code>dataset = torchvision.datasets.VOCSegmentation(...)\nfeat = feature_extractor(images, layer=9)\n# Feature shape for 100 images: torch.Size([100, 32, 32, 768])\n</code></pre> <pre><code>import torchvision\nimport torch\nfrom torchvision import transforms\nfrom PIL import Image\nimport numpy as np\n\ndataset_voc = torchvision.datasets.VOCSegmentation(\n    \"/data/pascal_voc/\",\n    year=\"2012\",\n    download=True,\n    image_set=\"val\",\n)\nprint(\"number of images in the dataset:\", len(dataset_voc))\n\nimage = dataset_voc[0][0]\nfeat = feature_extractor(image, resolution=(448, 448), layer=9)\nprint(\"Feature shape per-image:\", feat.shape)\nnum_nodes = feat.shape[0] * feat.shape[1]\nprint(\"Number of nodes per-image:\", num_nodes)\n\n# create a large-scale feature matrix\nimages = [dataset_voc[i][0] for i in range(100)]\nfeats = feature_extractor(images, resolution=(448, 448), layer=9)\nprint(\"Feature shape for 100 images:\", feats.shape)\nnum_nodes = np.prod(feats.shape[:3])\nprint(\"Number of nodes for 100 images:\", num_nodes)\n\n# Feature shape for 100 images: torch.Size([100, 32, 32, 768])\n# Number of nodes for 100 images: 102400\n</code></pre> <p>Compute NCUT</p> <pre><code>from ncut_pytorch import NCUT\n\ninput_feats = feats.flatten(0, 2)\neigenvectors, eigenvalues = NCUT(\n    num_eig=50, num_sample=30000, knn=10, affinity_focal_gamma=0.3, device='cpu'\n).fit_transform(input_feats)\n</code></pre> <p>Plotting</p>  Click to expand full code <pre><code>def plot_images(images, rgb, title):\n    ...\n</code></pre> <pre><code>import matplotlib.pyplot as plt\n\ndef plot_images(images, rgb, title):\n    fig, axs = plt.subplots(6, 8, figsize=(15, 10))\n    for i_row in range(0, 6, 2):\n        for i_col in range(8):\n            ax = axs[i_row, i_col]\n            image = images[i_row * 8 + i_col]\n            image = image.resize((224, 224), Image.BILINEAR)\n            ax.imshow(image)\n            ax.axis(\"off\")\n        for i_col in range(8):\n            ax = axs[i_row + 1, i_col]\n            ax.imshow(rgb[i_row * 8 + i_col])\n            ax.axis(\"off\")\n    plt.suptitle(title)\n    plt.show()    \n</code></pre> <pre><code>from ncut_pytorch import rgb_from_tsne_3d, rgb_from_umap_sphere\n\nX_3d, rgb = rgb_from_tsne_3d(eigenvectors[:, :50], perplexity=100)\nimage_rgb = rgb.reshape(feats.shape[:3] + (3,))\nplot_images(images, image_rgb, \"NCUT top 50 eigenvectors, t-SNE color, DiNOv2 layer9\")\n\nX_3d, rgb = rgb_from_umap_sphere(eigenvectors[:, :50], n_neighbors=100, min_dist=0.1)\nimage_rgb = rgb.reshape(feats.shape[:3] + (3,))\nplot_images(images, image_rgb, \"NCUT top 50 eigenvectors, UMAP color, DiNOv2 layer9\")\n</code></pre> <p>huzeyann/ncut_pytorch</p> <p></p>"},{"location":"tutorials/#tutorial-video","title":"Tutorial: VideoThe complete code for this tutorial","text":"<p>Feature extraction</p>  Click to expand full code <pre><code>def video_mae_feature(video_path, layer=11):\n    ...\n    return feature  # (t/2, (h*w), c)\n</code></pre> <pre><code>class VideoMAE(nn.Module):\n    def __init__(self, layer=11, **kwargs):\n        super().__init__()\n\n        try:\n            from transformers import VideoMAEForVideoClassification\n        except ImportError:\n            raise ImportError(\n                \"Please install the transformers library: pip install transformers\"\n            )\n\n        self.model = VideoMAEForVideoClassification.from_pretrained(\n            \"MCG-NJU/videomae-base-finetuned-kinetics\"\n        )\n        self.model.requires_grad_(False)\n        self.model.eval()\n\n        self.layer = layer\n\n    def forward(self, x):\n        assert x.dim() == 5\n        assert x.shape[1:] == (16, 3, 224, 224)  # frame, color channel, height, width\n\n        outputs = self.model(x, output_hidden_states=True, return_dict=True)\n        layer_idx = - (12 - self.layer)\n        return outputs.hidden_states[layer_idx] \n        # last_layer = outputs.hidden_states[-1]\n        # return last_layer\n\n\nfrom PIL import Image\nfrom torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\nimport numpy as np\n\n\ndef transform_images(frames, size=(224, 224)):\n    resized = []\n    length = len(frames)\n    for i in range(length):\n        frame = frames[i]\n        # image = Image.fromarray((frame * 255).astype(np.uint8))\n        image = Image.fromarray(frame)\n        image = image.resize(size, Image.ANTIALIAS)\n        image = np.array(image) / 255.0\n        resized.append(np.array(image))\n    frames = np.stack(resized, axis=0)\n    frames = frames.transpose(0, 3, 1, 2)  # (N, H, W, C) -&gt; (N, C, H, W)\n    frames = torch.tensor(frames, dtype=torch.float32)\n    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\n    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n    frames = (frames - mean) / std\n    return frames\n\n\ndef read_video(video_path: str) -&gt; torch.Tensor:\n    try:\n        from decord import VideoReader\n    except ImportError:\n        raise ImportError(\"Please install the decord library: pip install decord\")\n\n    vr = VideoReader(video_path)\n    print(f\"Total frames: {len(vr)}\")\n    # frames = vr.get_batch(range(len(vr))).asnumpy()\n    lenth = len(vr)\n    lenth = 1600 if lenth &gt; 1600 else lenth\n    frames = vr.get_batch(np.arange(lenth)).asnumpy()\n    # if less than 1600 frames, repeat the last frame\n    if lenth &lt; 1600:\n        last_frame = frames[-1]\n        for i in range(1600 - lenth):\n            frames = np.append(frames, last_frame.reshape(1, *last_frame.shape), axis=0)\n    # frames = np.array(frames)\n    frames = transform_images(frames)\n    return frames\n\n\ndef video_mae_feature(video_path, layer=11):\n    frames = read_video(video_path)\n    videomae = VideoMAE(layer=layer)\n    videomae = videomae.cuda()\n    frames = frames.cuda()\n    frames = rearrange(frames, \"(b t) c h w -&gt; b t c h w\", affinity_focal_gamma=16)\n    feats = videomae(frames)\n    return feats  # (t/2, (h*w), c)\n</code></pre> <pre><code>video_path = './tmp/video.mp4'\nfeatures = video_mae_feature(video_path, layer=11)\nprint(\"Features shape:\", features.shape)\n# Features shape: torch.Size([100, 1568, 768])\n</code></pre> <p>Compute NCUT</p> <pre><code>from ncut_pytorch import NCUT\n\nmodel = NCUT(num_eig=20)\neigenvectors, eigenvalues = model.fit_transform(features.flatten(0, 1))\nprint(\"Eigenvectors shape:\", eigenvectors.shape)\n# Eigenvectors shape: torch.Size([156800, 20])\n</code></pre> <p>Plotting</p> <pre><code>from ncut_pytorch import rgb_from_tsne_3d\n\nX_3d, rgb = rgb_from_tsne_3d(eigenvectors, perplexity=100)\n</code></pre>   Click to expland full code  <pre><code>def get_one_plot(i_frame, vr=vr, rgb=rgb):\n    ...\n    plt.show()\n</code></pre> <pre><code>import matplotlib.pyplot as plt\nfrom decord import VideoReader\n\nvr = VideoReader(video_path)\n\nimport matplotlib.gridspec as gridspec\ndef get_one_plot(i_frame, vr=vr, rgb=rgb):\n    fig = plt.figure(figsize=(10, 5))\n    gs = gridspec.GridSpec(1, 2, width_ratios=[1.8, 1])\n\n    ax0 = fig.add_subplot(gs[0, 0])\n    ax1 = fig.add_subplot(gs[0, 1])\n\n    frames = vr[i_frame]\n    ax0.imshow(frames.asnumpy())\n    ax0.set_title(f\"Frame {i_frame:04d}\")\n    ax0.axis(\"off\")\n\n    ax1.imshow(rgb[i_frame])\n    ax1.set_title(f\"Ncut(VideoMAE,layer11)\\n 3D spectral-tSNE, 20 eigenvectors\")\n    ax1.axis(\"off\")\n\n    plt.show()\n\nrgb = rgb.reshape(800, 14, 14, 3)\nrgb = repeat(rgb, 't h w c -&gt; t n h w c', n=2)\nrgb = rgb.reshape(1600, 14, 14, 3)\n\nfor i in range(100, 1600, 200):\n    get_one_plot(i)\n</code></pre>          Your browser does not support the video tag.      <p>huzeyann/ncut_pytorch</p> <p></p>"},{"location":"tutorials/#tutorial-language-model","title":"Tutorial: Language ModelThe complete code for this tutorial","text":"<p>Feature extraction</p> <pre><code>from transformers import GPT2Tokenizer, GPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = GPT2Model.from_pretrained('gpt2')\ntext = \"The majestic giraffe, with its towering height and distinctive long neck, roams the savannas of Africa. These gentle giants use their elongated tongues to pluck leaves from the tallest trees, making them well-adapted to their environment. Their unique coat patterns, much like human fingerprints, are unique to each individual.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input, output_hidden_states=True)\ntoken_texts = [tokenizer.decode([token_id]) for token_id in encoded_input['input_ids'][0]]\nfeatures = output.last_hidden_state.squeeze(0)\nprint(features.shape, len(token_texts))\n# torch.Size([66, 768]) 66\n</code></pre> <p>Compute NCUT</p> <pre><code>from ncut_pytorch import NCUT, rgb_from_tsne_3d\n\neigenvectors, eigenvalues = NCUT(num_eig=10).fit_transform(features)\nX_3d, rgb = rgb_from_tsne_3d(eigenvectors, perplexity=30)\nrgb = rgb.numpy()\nprint(\"rgb shape:\", rgb.shape)  #  (66, 3)\n</code></pre> <p>Plotting</p>  Click to expand full code <pre><code>import matplotlib.pyplot as plt\n...\nplt.show()\n</code></pre> <pre><code>import matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\nimport numpy as np\n\n# Create a figure and axis\nfig, ax = plt.subplots(figsize=(5, 2.5))\n\n# Define the colors\ncolors = [mcolors.rgb2hex(rgb[i]) for i in range(len(token_texts))]\n\n# Split the sentence into words\nwords = token_texts\n\n\ny_pos = 0.9\nx_pos = 0.0\nmax_word_length = max(len(word) for word in words)\nfor word, color in zip(words, colors):\n    if word == '&lt;|begin_of_text|&gt;':\n        word = '&lt;SoT&gt;'\n        y_pos -= 0.05\n        x_pos = 0.0\n\n\n    text_color = 'black' if sum(mcolors.hex2color(color)) &gt; 1.3 else 'white'  # Choose text color based on background color\n    # text_color = 'black'\n    txt = ax.text(x_pos, y_pos, word, color=text_color, fontsize=12, bbox=dict(facecolor=color, alpha=0.8, edgecolor='none', pad=2))\n    txt_width = txt.get_window_extent().width / (fig.dpi * fig.get_size_inches()[0])  # Calculate the width of the text in inches\n\n    x_pos += txt_width * 1.1 + 0.01  # Adjust the spacing between words\n\n    if x_pos &gt; 0.97:\n        y_pos -= 0.15\n        x_pos = 0.0\n    # break\n\n# Remove the axis ticks and spines\nax.set_xticks([])\nax.set_yticks([])\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.spines['bottom'].set_visible(False)\nax.spines['left'].set_visible(False)\n\nplt.title(\"GPT-2 last hidden state spectral-tSNE 3D(10eig)\")\nplt.tight_layout()\nplt.show()\n</code></pre> <p>huzeyann/ncut_pytorch</p> <p></p>"},{"location":"version/","title":"Version Information","text":"<p>Current version: 1.13.14</p> <p>This documentation was built for ncut_pytorch version 1.13.14.</p>"}]}