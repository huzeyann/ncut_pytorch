# How NCUT works

## Normalized Cuts and Spectral Clustering

Spectral clustering is a powerful technique for clustering data based on the eigenvectors (spectrum) of a similarity matrix derived from the data. The Normalized Cuts algorithm aims to partition a graph into subgraphs while minimizing the graph cut value.

<div style="text-align: center;">
<img src="../images/graph_cut.png" style="width:100%;">
</div>
Image taken from [Spectral Clustering: Step-by-step derivation of the spectral clustering algorithm including an implementation in Python](https://medium.com/@roiyeho/spectral-clustering-50aee862d300)


---

### The Basic Idea
Spectral clustering works by embedding the data points $F \in \mathbb{R}^{N \times D}$ into a lower-dimensional space using the eigenvectors of a Laplacian matrix derived from the data's similarity graph $W \in \mathbb{R}^{N \times N}$. The data is then clustered in this new space embedded by $k$ eigenvectors $\mathbf{x} \in \mathbb{R}^{N \times k}$.

<div style="text-align: center;">
<img src="../images/spectral_tsne_how.png" style="width:80%;">
</div>

### The Graph Laplacian
Given a set of data points, spectral clustering first constructs a similarity graph. Each node represents a data point, and edges represent the similarity between data points. The similarity graph can be represented by an adjacency matrix \( W \), where each element \( W_{ij} \) represents the similarity between data points \( i \) and \( j \).


Take cosine distance for example, let $F \in \mathbb{R}^{N \times D}$ be the input features (from a backbone image model), $N$ is number of pixels, $D$ is feature dimension. Feature vectors $f_i, f_j \in \mathbb{R}^D$, the cosine similarity between $f_i$ and $f_j$ is defined as:

$$W_{ij} = \text{cosine}(f_i, f_j) = \frac{f_i \cdot f_j}{|f_i| |f_j|}$$

where $|f|$ denotes the Euclidean norm of a vector $f$.

In matrix form, this can be written as:

$$W = \text{cosine}(F, F) = \frac{F F^\top}{\text{diag}(F F^\top)^{1/2} , \text{diag}(F F^\top)^{1/2}}$$

where $F F^\top \in \mathbb{R}^{N \times N}$ is the matrix of pairwise dot products between the feature vectors, and $\text{diag}(\cdot)$ extracts the diagonal elements of a square matrix.
The resulting matrix $W = \text{cosine}(F, F)$ is an $\mathbb{R}^{N \times N}$ matrix, where each element $(i, j)$ represents the cosine similarity between the $i$-th and $j$-th feature vectors.


The degree matrix \( D \) is a diagonal matrix where each element \( D_{ii} \) is the sum of the similarities of node \( i \) to all other nodes.

$$
D_{ii} = \sum_{j=0}^{N} W_{ij}
$$

The unnormalized graph Laplacian is defined as:

\[
L = D - W
\]

The normalized graph Laplacian has two common forms:

1. Symmetric normalized Laplacian:
 
\[
L_{\text{sym}} = D^{-1/2} L D^{-1/2} = I - D^{-1/2} W D^{-1/2}
\]

2. Random walk normalized Laplacian:

\[
L_{\text{rw}} = D^{-1} L = I - D^{-1} W
\]


### Normalized Cuts 
Normalized Cuts (Ncut) is a method for partitioning a graph into disjoint subsets, aiming to minimize the total edge weight between the subsets relative to the total edge weight within each subset. The Ncut criterion is particularly useful for ensuring balanced partitioning, which prevents trivial solutions where one cluster might be significantly smaller than the other.

<div style="text-align: center;">
<img src="../images/graph_cut.png" style="width:100%;">
</div>

The normalized cut criterion is defined as:

\[
\text{Ncut}(A, B) = \left(\frac{\text{cut}(A, B)}{\text{assoc}(A, V)}\right) + \left(\frac{\text{cut}(A, B)}{\text{assoc}(B, V)}\right)
\]

Where:

- \( A \) and \( B \) are two disjoint subsets (clusters) of the graph.

- \( \text{cut}(A, B) \) is the sum of the weights of the edges between the subsets \( A \) and \( B \):

\[
\text{cut}(A, B) = \sum_{i \in A, j \in B} W_{ij}
\]

- \( \text{assoc}(A, V) \) is the sum of the weights of all edges attached to nodes in subset \( A \) (similar definition for \( B \)):

\[
\text{assoc}(A, V) = \sum_{i \in A, j \in V} W_{ij}
\]

The goal is to find a partition that minimizes the Ncut value, which balances minimizing the inter-cluster connection (cut) with maintaining strong intra-cluster connections (association).


### Solving the Normalized Cut Using Eigenvectors

To solve the Ncut problem, we reformulate it as a problem of finding the eigenvectors of a normalized graph Laplacian. Hereâ€™s how this works:

#### Mathematical Formulation

Let \( \mathbf{x} \) be an indicator vector such that:

\[
x_i = 
\begin{cases} 
1 & \text{if node } i \in A \\
-1 & \text{if node } i \in B 
\end{cases}
\]

The cut value \( \text{cut}(A, B) \) can be rewritten in terms of \( \mathbf{x} \) as:

\[
\text{cut}(A, B) = \frac{1}{4} \sum_{i,j} W_{ij} (x_i - x_j)^2 = \mathbf{x}^\top L \mathbf{x}
\]

Where \( L = D - W \) is the unnormalized graph Laplacian.

The association \( \text{assoc}(A, V) \) is given by:

\[
\text{assoc}(A, V) = \mathbf{x}^\top D \mathbf{1}_A
\]

Here, \( \mathbf{1}_A \) is a vector that is 1 for elements in \( A \) and 0 elsewhere.

The Ncut problem can thus be rewritten as:

\[
\text{Ncut}(A, B) = \frac{\mathbf{x}^\top L \mathbf{x}}{\mathbf{x}^\top D \mathbf{x}}
\]

Minimizing this directly is NP-hard. However, it can be relaxed into a generalized eigenvalue problem. By relaxing the constraint that \( x_i \) takes only discrete values (1 or -1), we allow \( \mathbf{x} \) to take real values, and the problem becomes finding the eigenvector corresponding to the second smallest eigenvalue of the generalized eigenvalue problem:

\[
L \mathbf{y} = \lambda D \mathbf{y}
\]

To make it a simple eigenvalue solving, that can be solved by most eigenvector solvers. 
Move $D$ to the left side of the equation. this is equivalent to finding the eigenvectors of the normalized Laplacian:

\[
D^{-1/2} L D^{-1/2} \mathbf{y} = \lambda \mathbf{y}
\]


#### Proof of Minimization

The eigenvector approach is derived from the relaxation of the original NP-hard problem. By solving the generalized eigenvalue problem:

\[
L \mathbf{y} = \lambda D \mathbf{y}
\]

We are effectively minimizing the [Rayleigh quotient](https://en.wikipedia.org/wiki/Rayleigh_quotient):

\[
\frac{\mathbf{y}^\top L \mathbf{y}}{\mathbf{y}^\top D \mathbf{y}} = \frac{\mathbf{y}^\top D^{-1/2} L D^{-1/2} \mathbf{y}}{\mathbf{y}^\top  \mathbf{y}}
\]

The Rayleigh quotient directly reflecting the Ncut objective. The solution of Rayleigh quotient is eigenvectors

\[
\text{Ncut}(A, B) = \frac{\mathbf{x}^\top L \mathbf{x}}{\mathbf{x}^\top D \mathbf{x}} = \frac{\mathbf{x}^\top D^{-1/2} L D^{-1/2} \mathbf{x}}{\mathbf{x}^\top \mathbf{x}}
\]

thus, solving eigenvector $\mathbf{y}$ of $D^{-1/2} L D^{-1/2} = \lambda \mathbf{y}$ is equal to solving the graph cut solution $\mathbf{x}$:

\[ 
    \mathbf{x} = \mathbf{y} 
\]


#### Laplacian vs Affinity

Normalized Cuts can also be solved by eigenvectors of \(D^{-1/2} W D^{-1/2} \), instead of \(D^{-1/2} L D^{-1/2} \), where $L = D - W$. They have the same eigenvectors:

$$\begin{aligned}
D^{-1/2} (D - W) D^{-1/2} &= Y \Lambda Y^T \\
I - D^{-1/2} W D^{-1/2} &= Y \Lambda Y^T \\
D^{-1/2} W D^{-1/2} &= Y (I - \Lambda) Y^T
\end{aligned}$$

#### Eigenvectors and Clustering

The eigenvector corresponding to the second smallest eigenvalue (also called the Fiedler vector) provides a real-valued solution to the relaxed Ncut problem. Sorting the nodes based on the values in this eigenvector and choosing a threshold to split them yields an approximate solution to the Ncut problem.

Each subsequent eigenvector can be used to further partition the graph into subclusters. The process is as follows:

1. Compute the symmetric normalized Laplacian \( L_{\text{sym}} \).
2. Compute the eigenvectors \( \mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_k \) corresponding to the smallest \( k \) eigenvalues.
3. Use \( \mathbf{x}_2 \) (the Fiedler vector) to divide the graph into two clusters.
4. For multi-way clustering, use the next eigenvectors \( \mathbf{x}_3, \mathbf{x}_4, \dots \) to further hierarchically subdivide these clusters.


#### Example: Eigenvector Visualization

Let's visualize how the eigenvectors divide the graph into clusters using our toy example.

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from scipy.linalg import eigh
from sklearn.metrics.pairwise import rbf_kernel
from sklearn.cluster import KMeans

# Generate synthetic data with 4 clusters
X, y = make_blobs(n_samples=400, centers=4, cluster_std=0.60, random_state=0)

# Compute the similarity matrix using RBF (Gaussian) kernel
sigma = 1.0
W = rbf_kernel(X, gamma=1/(2*sigma**2))

# Degree matrix
D = np.diag(np.sum(W, axis=1))

# Unnormalized Laplacian
L = D - W

# Normalized Laplacian (symmetric)
D_inv_sqrt = np.linalg.inv(np.sqrt(D))
L_sym = np.dot(np.dot(D_inv_sqrt, L), D_inv_sqrt)

# Compute the eigenvalues and eigenvectors of the normalized Laplacian
eigvals, eigvecs = eigh(L_sym)

# Plot the first few eigenvectors
k = 4  # number of clusters
for i in range(1, k+1):
    plt.figure()
    plt.scatter(X[:, 0], X[:, 1], c=eigvecs[:, i-1], s=50, cmap='coolwarm')
    plt.title(f"Eigenvector {i} - Dividing the Graph")
    plt.colorbar()
    plt.show()

# Use the first k eigenvectors to form a matrix U
U = eigvecs[:, :k]

# Normalize U
U_norm = U / np.linalg.norm(U, axis=1, keepdims=True)

# Perform k-means clustering on U_norm
kmeans = KMeans(n_clusters=k, random_state=0)
labels = kmeans.fit_predict(U_norm)

# Plot the clusters
plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='viridis')
plt.title("Spectral Clustering: Clusters Found")
plt.show()
```

<div style="text-align: center;">
<img src="../images/dots_eig.jpg.png" style="width:100%;">
</div>

- The 1-st eigenvector generally corresponds to the global structure (often related to the connected components of the graph).

- The 2-nd eigenvector splits the graph into 2 clusters.

- Subsequent eigenvectors further hierarchically splits the data into more detailed sub-clusters.


<div style="text-align: center;">
<img src="../images/dots_kmeans.png" style="width:60%;">
</div>

#### A Good Eigenvector Solver

Solving the full eigenvector $\mathbf{x} \in \mathbb{R}^{N \times N}$ is computational expensive, methods has been developed to solve top k eigenvectors $\mathbf{x} \in \mathbb{R}^{N \times k}$ in linearly complexity scaling. In particular, we use [svd_lowrank](https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html).

- Reference: 

    Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding structure with randomness: probabilistic algorithms for constructing approximate matrix decompositions, 2009.

### Summary
The Normalized Cuts algorithm aims to partition a graph into subgraphs while minimizing the graph cut value. It embeds the data points into a lower-dimensional space using the eigenvectors of a Laplacian matrix derived from the data's similarity graph.

Eigenvectors are:

1) soft-cluster assignments, eigenvector in [-1, 1], -1 or 1 are extreme assignments, 0 means assigned to neither cluster. 

2) $n$ eigenvectors can hierarchically divide into $2^n$ sub-graphs.

------



## Nystrom-like Approximation

The Nystrom-like approximation is our new technique designed to address the computational challenges of solving large-scale graph cuts. 

**An intuition**: Nystrom approximation solve the large-scale problem on a small sub-sampled set, then propagate the solution from the small set to the large set.

<div style="text-align: center;">
<img src="../images/subsample.png" style="width:80%;">
</div>

The approach involves three main steps: (1) sub-sampling a subset of nodes, (2) computing the Normalized Cut (Ncut) on the sampled nodes, and (3) propagating the eigenvectors from the sampled nodes to the unsampled nodes.

### Sub-sampling the Affinity Matrix

The affinity matrix \( W \) is partitioned as follows:

\[
W = \begin{pmatrix}
A & B \\
B^\top & C
\end{pmatrix}
\]

where:

- \( A \in \mathbb{R}^{n \times n} \) is the submatrix of weights among the sampled nodes,

- \( B \in \mathbb{R}^{(N-n) \times n} \) is the weights between the sampled nodes and the remaining unsampled nodes,

- \( C \in \mathbb{R}^{(N-n) \times (N-n)} \) is the weights between the unsampled nodes.

**Computation loads**: \( C \) is a very large matrix that is computationally expensive to handle. \( B \) is also expensive to store and compute once sampled nodes \( n \) gets large, although \( n \) (the number of sampled nodes) is much smaller than \( N \) (the total number of nodes), a better quality of  Nystrom approximation requires \( n \) to be larger, thus makes \( B \) a large matrix.
 
### A Good Sampling Strategy: FPS

An effective sampling strategy is crucial for obtaining a good approximation of the Ncut. Farthest Point Sampling (FPS) is used because it ensures that the sampled nodes are well-distributed across the graph. 
We use a tree-based QuickFPS algorithm developed in 

- Reference: 

    QuickFPS: Architecture and Algorithm Co-Design for Farthest Point Sampling in Large-Scale Point Cloud, Han, Meng and Wang, Liang and Xiao, Limin and Zhang, Hao and Zhang, Chenhao and Xu, Xiangrong and Zhu, Jianfeng, 2023

As a side note for speed, for high-dimensional input model features $F \in \mathbb{R}^{N \times D}$ where $D$ is feature dimension, FPS can be computationally expensive for large $D$. To mitigate this, Principal Component Analysis (PCA) is used to reduce the dimensionality of the features to $F' \in \mathbb{R}^{N \times 5}$. PCA is only applied in the FPS sampling step, but not to the affinity graph $W$.

### Accounting for Indirect Connections

Unsampled nodes can act as bridges between clusters, and ignoring these connections could lead to inaccurate cluster assignments. To address this, the original Nystrom method [2] approximates the eigenvectors by solving the following matrix \( S \):

\[
S = A + A^{-1/2} B B^\top A^{-1/2}
\]

Here, the term \( A^{-1/2} B B^\top A^{-1/2} \in \mathbb{R}^{n \times n} \) accounts for indirect connections between the sampled nodes by considering the influence of the unsampled nodes.

In our method, we solve for eigenvectors on the matrix \( S \) given by:

\[
S = A + \left({D_{\text{r}}}^{-1} B\right) \left(B {D_{\text{c}}}^{-1}\right)^\top
\]

where \( D_{\text{r}} \) and \( D_{\text{c}} \) are the row and column sums of matrix \( B \), respectively. 

the term \( \left({D_{\text{r}}}^{-1} B\right) \left(B {D_{\text{c}}}^{-1}\right)^\top \in \mathbb{R}^{n \times n}\) is the indirect random walk probabilities.

#### Example Case for Indirect Connections

<div style="text-align: center;">
<img src="../images/indirect_vs_17.png" style="width:80%;">
</div>


<div style="text-align: center;">
<img src="../images/indirect_vs_12.png" style="width:80%;">
</div>

This example shows when indirect connection is important. In the bottom example, the connected components of the graph is altered, thus will lead to incorrect approximation. After adding indirect connection, the graph become correctly connected.

#### Reduce Computation Loads for Indirect Connections

Given that \( B \) has dimensions \( (N-n) \times n \), $N$ is large, directly storing and computing with \( B \) can be prohibitively expensive. To overcome this, we reduce the number of unsampled nodes by applying PCA. If the feature size is \( D \), let \( F_A \in \mathbb{R}^{n \times D} \) be the feature matrix for the sampled nodes, and \( F_B \in \mathbb{R}^{(N-n) \times D} \) be the feature matrix for the unsampled nodes. After applying PCA, $m$ is the PCA-ed dimension, the reduced feature matrix \( F_B' \in \mathbb{R}^{m \times D} \) represents the unsampled nodes, where \( m \ll (N-n) \). Thus, \( B' = d(F_A, F_{B'}) \) becomes a matrix of size \( \mathbb{R}^{m \times n} \), significantly reduce computation loads and memory usage.


### K-Nearest Neighbors (KNN) Propagation

<div style="text-align: center;">
<img src="../images/knn_propagate.png" style="width:80%;">
</div>

After computing the eigenvectors \( \mathbf{X}' \in \mathbb{R}^{n \times C} \) on the sub-sampled graph \( S = A + \left({D_{\text{r}}}^{-1} B'\right) \left(B' {D_{\text{c}}}^{-1}\right)^\top \in \mathbb{R}^{n \times n} \) using the Ncut formulation, the next step is to propagate these eigenvectors to the full graph. Let \( \mathbf{\tilde{X}} \in \mathbb{R}^{N \times C} \) be the approximated eigenvectors for the full graph, where \( N \) is the total number of nodes. The eigenvector approximation \( \mathbf{\tilde{X}}_i \) for each node \( i \leq N \) in the full graph is obtained by averaging the eigenvectors \( \mathbf{X}'_k \) of the top K-nearest neighbors from the subgraph:

\[
\mathcal{K}_i = \text{KNN}(\mathbf{A}_{*i}; n, K) = argmax_{k \leq n} \sum_{k=1}^{K} \mathbf{B}_{ki}
\]

\[
\mathbf{\tilde{X}}_i = \frac{1}{\sum_{k \in \mathcal{K}_i} \mathbf{B}_{ki}} \sum_{k \in \mathcal{K}_i} \mathbf{B}_{ki} \mathbf{X}'_k 
\]

Here, \( \text{KNN}(\mathbf{A}_{*i}; n, K) \) denotes the set of K-nearest neighbors from the full-graph node \( i \leq N \) to the sub-graph nodes \( k \leq n \). In other words, the eigenvectors of un-sampled nodes are assigned by weighted averaging top KNN eigenvectors from sampled nodes.

The propagation step is only when \(B \in \mathbb{R}^{n \times (N-n)} \) is computed (but not stored), recall that the full \( B \) is too large to store, our implementation divide the propagation into chunks, where each $q$ size chunk \(B_i \in \mathbb{R}^{n \times (q)} \) is feasible to store, each chunk \(B_i\) is discarded after the propagation. In practice, the KNN propagation is the major speed bottle-neck for NCUT on large-scale graph, but KNN propagation is easy to parallelize on GPU. 

```py
# GPU speeds up the KNN propagation
eigvectors, eigvalues = NCUT(num_eig=100, device='cuda:0').fit_transform(data)
```

---

## Visualizing NCUT Eigenvectors

<div style="text-align: center;">
<img src="../images/spectral_tsne_how.png" style="width:80%;">
</div>

The input image features \( F \in \mathbb{R}^{N \times D} \) are embedded into eigenvectors \( \mathbf{x} \in \mathbb{R}^{N \times k} \), where \( N \) is the number of pixels, \( D \) is the feature dimension, and \( k \) is the number of eigenvectors. We use t-SNE or UMAP on the eigenvectors for visualization. 

3D t-SNE is computed on eigenvectors \( \mathbf{x} \in \mathbb{R}^{N \times k} \), resulting in a color matrix \( \mathbb{R}^{N \times 3} \). RGB values are assigned to each pixel based on its 3D t-SNE embedding.

<div style="text-align: center;">
<img src="../images/raw_eigenvectors.png" style="width:80%;">
</div>

Alternatively, in the old days before t-SNE comes out, K-means clustering are applied to the eigenvectors. Compares t-SNE with K-means, t-SNE has two advantages: (1) continuous color gradients instead of discrete clusters, (2) t-SNE color reflects distances (e.g., distance between purple vs. red < green vs. red). K-means colors are arbitrary.

<div style="text-align: center;">
<img src="../images/tsne_vs_kmeans.png" style="width:80%;">
</div>


### Nystrom Approximation for t-SNE and UMAP

t-SNE and UMAP can be slow for large-scale graphs. We applied the same Nystrom-like approximation: (1) FPS sub-samples a subset of nodes, (2) t-SNE is computed on the sampled nodes, and (3) KNN propagates the colors from sampled to unsampled nodes.

For extra speed-up, we found FPS sub-sampling (euclidean) on eigenvectors \( \mathbf{x} \in \mathbb{R}^{N \times k} \) more effective than on the original features \( F \in \mathbb{R}^{N \times D} \), because the distribution of eigenvectors. Thus, the Nystrom approximation for t-SNE requires fewer samples to perform well (we recommend 300 samples for t-SNE, 10,000 for NCUT). Note that small sampling works well for t-SNE only on eigenvectors but not on the original features. 
t-SNE with small sampling is fast on CPU, then the expensive KNN propagation step computes in GPU.

```py
X_3d, rgb = rgb_from_tsne_3d(eigvectors, num_samples=300, device='cuda:0')
```

---



> paper in prep, Yang 2024
>
> [1] AlignedCut: Visual Concepts Discovery on Brain-Guided Universal Feature Space, Huzheng Yang, James Gee\*, Jianbo Shi\*, 2024
>
> [2] Spectral Grouping Using the Nystrom Method, Charless Fowlkes, Serge Belongie, Fan Chung, and Jitendra Malik, 2004
> 
> [3] Normalized Cuts and Image Segmentation, Jianbo Shi and Jitendra Malik, 2000
> 